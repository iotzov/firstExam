@article{Friederici2011,
abstract = {Language processing is a trait of human species. The knowledge about its neurobiological basis has been increased considerably over the past decades. Different brain regions in the left and right hemisphere have been identified to support particular language functions. Networks involving the temporal cortex and the inferior frontal cortex with a clear left lateralization were shown to support syntactic processes, whereas less lateralized temporo-frontal networks subserve semantic processes. These networks have been substantiated both by functional as well as by structural connectivity data. Electrophysiological measures indicate that within these networks syntactic processes of local structure building precede the assignment of grammatical and semantic relations in a sentence. Suprasegmental prosodic information overtly available in the acoustic language input is processed predominantly in a temporo-frontal network in the right hemisphere associated with a clear electrophysiological marker. Studies with patients suffering from lesions in the corpus callosum reveal that the posterior portion of this structure plays a crucial role in the interaction of syntactic and prosodic information during language processing.},
author = {Friederici, A. D.},
doi = {10.1152/physrev.00006.2011},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Friederici - 2011 - The Brain Basis of Language Processing From Structure to Function.pdf:pdf},
isbn = {1522-1210 (Electronic)$\backslash$r0031-9333 (Linking)},
issn = {0031-9333},
journal = {Physiological Reviews},
mendeley-groups = {First Exam Citations/Ch4},
number = {4},
pages = {1357--1392},
pmid = {22013214},
title = {{The Brain Basis of Language Processing: From Structure to Function}},
url = {http://physrev.physiology.org/cgi/doi/10.1152/physrev.00006.2011},
volume = {91},
year = {2011}
}
@article{Rangaswamy2002,
abstract = {Background: In this study, the magnitude and spatial distribution of beta power in the resting electroencephalogram (EEG) were examined to address the possibility of an excitation-inhibition imbalance in the central nervous system of alcoholics. Methods: Log transformed absolute power in the Beta 1 (12.5-16 Hz), Beta 2 (16.5-20 Hz), and Beta 3 (20.5-28 Hz) bands in the eyes-closed EEG of 307 alcohol-dependent subjects and 307 unaffected age- and gender-matched control subjects were compared using a multivariate repeated measures design. Effect of gender, age, and drinking variables was examined separately. Results: Increased Beta 1 (12.5-16 Hz) and Beta 2 (16.5-20 Hz) absolute power was observed in alcohol-dependent subjects at all loci over the scalp. The increase was most prominent in the central region. Increased Beta 3 (20.5-28 Hz) power was frontal in the alcoholics. Age and clinical variables did not influence the increase. Male alcoholics had significantly higher beta power in all three bands. In female alcoholics the increase did not reach statistical significance. Conclusions: Beta power in all three bands of resting EEG is elevated in alcoholics. This feature is more prominent in male alcoholics. The increased beta power in the resting EEG may be an electrophysiological index of the imbalance in the excitation-inhibition homeostasis in the cortex. {\textcopyright} 2002 Society of Biological Psychiatry.},
author = {Rangaswamy, Madhavi and Porjesz, Bernice and Chorlian, David B. and Wang, Kongming and Jones, Kevin A. and Bauer, Lance O and Rohrbaugh, John and O'Connor, Sean J and Kuperman, Samuel and Reich, Theodore and Begleiter, Henri},
doi = {10.1016/S0006-3223(02)01362-8},
file = {:C$\backslash$:/Users/ivan/Desktop/rangaswamy2002.pdf:pdf},
isbn = {0145-6008},
issn = {00063223},
journal = {Biological Psychiatry},
keywords = {Absolute Power,Alcoholism,EEG,Theta},
mendeley-groups = {First Exam Citations/Ch4},
month = {oct},
number = {8},
pages = {831--842},
pmid = {12711923},
title = {{Beta power in the EEG of alcoholics}},
url = {http://doi.wiley.com/10.1097/01.ALC.0000060523.95470.8F http://linkinghub.elsevier.com/retrieve/pii/S0006322302013628},
volume = {52},
year = {2002}
}
@article{Michel1992,
abstract = {FFT dipole approximation and 3-dimensional dipole modelling were used to determine the locations of the equivalent dipole model sources of the delta, theta, alpha, beta-1 and beta-2 frequency bands in 13 normal subjects during resting. From each subject, 2 successive data sets were analysed, each consisting of 10 epochs of 2 sec randomly collected during 30 min. ANOVAs showed that over subjects, the source locations of EEG frequency bands differed significantly in the vertical and antero-posterior dimensions. Results of data set 2 confirmed those of data set 1. The source of delta was deepest and most anterior, theta more posterior and less deep, alpha most posterior and highest on the vertical dimension, beta-1 deeper and slightly more anterior than alpha, and beta-2 again more anterior and deeper than beta-1. Thus, the depth of source location was not linearly related to temporal frequency. The sources of all 5 bands were oriented in the sagittal direction; delta mean fields had steeper gradients anteriorly, alpha and beta-1 posteriorly. The power map for any frequency was well described by a single phase angle. The results indicate that the different EEG frequency bands during a given EEG epoch are generated by neural populations in different brain locations. {\textcopyright} 1992.},
author = {Michel, C. M. and Lehmann, D. and Henggeler, B. and Brandeis, D.},
doi = {10.1016/0013-4694(92)90180-P},
file = {:C$\backslash$:/Users/ivan/Desktop/michel1992.pdf:pdf},
issn = {00134694},
journal = {Electroencephalography and Clinical Neurophysiology},
keywords = {Alpha EEG source localization,Delta, theta, alpha, beta EEG source localization,EEG phase angles,EEG source localization in the frequency domain,FFT dipole approximation,Spontaneous EEG sources},
mendeley-groups = {First Exam Citations/Ch4},
number = {1},
pages = {38--44},
pmid = {1370142},
title = {{Localization of the sources of EEG delta, theta, alpha and beta frequency bands using the FFT dipole approximation}},
volume = {82},
year = {1992}
}
@article{Arnal2012,
abstract = {Many theories of perception are anchored in the central notion that the brain continuously updates an internal model of the world to infer the probable causes of sensory events. In this framework, the brain needs not only to predict the causes of sensory input, but also when they are most likely to happen. In this article, we review the neurophysiological bases of sensory predictions of "what' (predictive coding) and 'when' (predictive timing), with an emphasis on low-level oscillatory mechanisms. We argue that neural rhythms offer distinct and adapted computational solutions to predicting 'what' is going to happen in the sensory environment and 'when'. {\textcopyright} 2012 Elsevier Ltd.},
author = {Arnal, Luc H. and Giraud, Anne Lise},
doi = {10.1016/j.tics.2012.05.003},
file = {:C$\backslash$:/Users/ivan/Desktop/Arnal cortical oscillations.pdf:pdf},
isbn = {1364-6613},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
mendeley-groups = {First Exam Citations/Ch4},
number = {7},
pages = {390--398},
pmid = {22682813},
publisher = {Elsevier Ltd},
title = {{Cortical oscillations and sensory predictions}},
url = {http://dx.doi.org/10.1016/j.tics.2012.05.003},
volume = {16},
year = {2012}
}
@article{Wang2010,
abstract = {In our multisensory environment our sensory systems are continuously receiving information that is often interrelated and must be integrated. Recent work in animals and humans has demonstrated that input to one sensory modality can reset the phase of ambient cortical oscillatory activity in another. The periodic fluctuations in neuronal excitability reflected in these oscillations can thereby be aligned to forthcoming anticipated sensory input. In the auditory domain, the example par excellence is speech, because of its inherently rhythmic structure. In contrast, fluctuations of oscillatory phase in the visual system are argued to reflect periodic sampling of the environment. Thus rhythmic structure is imposed on, rather than extracted from, the visual sensory input. Given this distinction, we suggest that cross-modal phase reset subserves separate functions in the auditory and visual systems. We propose a modality-dependent role for cross-modal input in temporal prediction whereby an auditory event signals the visual system to look now, but a visual event signals the auditory system that it needs to hear what is coming.This article is part of a Special Issue entitled {\textless}Human Auditory Neuroimaging{\textgreater}.{\textcopyright} 2013 Elsevier B.V.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Wang, Xiao-Jing},
doi = {10.1152/physrev.00035.2008},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/ivan/Desktop/nihms225455.pdf:pdf},
isbn = {1097-4199 (Electronic)$\backslash$r0896-6273 (Linking)},
issn = {0031-9333},
journal = {Physiological Reviews},
keywords = {*Electrophysiology,*Neurons,*Sensory Integration,15,1989,2001b,2007,2014,27,3-dione,3-dione: pharmacology,4093,4100,6-Cyano-7-nitroquinoxaline-2,AMPA,AMPA: antagonists {\&} inhibitors,AMPA: metabolism,Accuracy,Acoustic Stimulation,Acoustic Stimulation/*methods,Acoustic Stimulation: methods,Action Potentials,Action Potentials: physiology,Adult,Afferent Pathways,Afferent Pathways: physiology,Affine transformation,Algorithms,Analysis of Variance,Animals,Area 17,Area 18,Area CM,Arousal,Arousal: physiology,Association cortex,Asynchrony,Attention,Attention/physiology,Attention: physiology,Audio-visual,Audiovisual,Audition,Auditory,Auditory Cortex,Auditory Cortex/*physiology,Auditory Cortex: physiology,Auditory Perception,Auditory Perception/*physiology,Auditory Perception: physiology,Auditory cortex,Auditory: physiology,Automated,Automaticity,BOLD,Bayes Theorem,Beta Gamma,Beta Rhythm,Bicuculline methiodide,Bimodal,Bimodal neuron,Binding,Brain,Brain Injuries,Brain Injuries: pathology,Brain Mapping,Brain Mapping/methods,Brain Mapping: methods,Brain Physiology. Oscillations. Biological rhythms,Brain oscillations,Brain: cytology,Brain: physiology,Brain: physiopathology,Brain: radionuclide imaging,Cat,Cerebral Cortex,Cerebral Cortex: physiology,Cerebral cortex,Choice Behavior/physiology,Coloring Agents,Coloring Agents: analysis,Complications,Computation,Computer-Assisted,Connections,Connectivity,Conscious recollection,Consolidation,Contrast sensitivity,Cortex,Cortical Synchronization,Cortical Synchronization: physiology,Cross-frequency coupling,Cross-modal,Crossmodal,Cues,Current Source Density,Current source density (CSD),Delta rhythm,Detection,EEG,ERP,Electric Stimulation,Electric Stimulation: methods,Electricity,Electro-corticography (ECoG),Electrocorticography,Electrocorticography (ECoG),Electrode localization,Electrodes,Electroencephalography,Electroencephalography: methods,Electroencephalography: statistics {\&} numerical dat,Electrophysiology,Emission-Computed,Endogenous,Entorhinal Cortex,Entorhinal Cortex: physiology,Epilepsy,Epilepsy surgery,Epilepsy: pathology,Epilepsy: physiopathology,Event-related potential (ERP),Evoked Potentials,Evoked Potentials: physiology,Evoked response,Excitation,Excitatory Amino Acid Antagonists,Excitatory Amino Acid Antagonists: pharmacology,Exogenous,Extra striate visual cortex,FMRI,Face,Facial Expression,Fano factor,Feedback,Feedback-feedforward,Female,Ferret,Field potential,Functional Laterality,Functional Laterality: physiology,Functional imaging,Functional magnetic resonance imaging,Functional mapping,Gamma,Gamma rhythm,Gamma-aminobutyric acid,Gamma-band activity,Global optimisation,Glutamate,Hand/physiology,Hearing,Hippocampus,Hippocampus: anatomy {\&} histology,Hippocampus: physiology,Human,Human visual cortex,Humans,Illusions,Illusions: physiology,Illusions: psychology,Image Processing,Image co-registration,Imaging,Implanted,In vivo application accuracy,Information,Inhibition,Injections,Integrate-and-fire network,Integration,Interdependency,Intracranial,Intracranial EEG,Intraoperative imaging,Invasive EEG,Invasive human recordings,Inverse effectiveness,Laura source estimation,Least-Squares Analysis,Light,MEG,MRI,Macaca,Macaca fascicularis,Macaca fascicularis: physiology,Macaca mulatta,Magnetic Resonance Imaging,Magnetic Resonance Imaging: methods,Magnetic resonance spectroscopy,Magnocellular pathway,Male,Memory,Memory systems,Memory/physiology,Mental Processes,Mental Processes: physiology,Microelectrodes,Middle Aged,Mismatch negativity,Models,Monkey,Monte Carlo Method,Motion Perception,Motion Quartet,Motor,Multi-resolution search,Multimodal interaction,Multimodal registration,Multiplex,Multisensory,Multisensory integration,Multisensory perception,N-Methyl-D-Aspartate,N-Methyl-D-Aspartate: antagonists {\&} inh,N-Methyl-D-Aspartate: metabolism,Natural stimuli,Neocortex,Neocortex: anatomy {\&} histology,Neocortex: physiology,Nerve Net,Nerve Net: cytology,Nerve Net: physiology,Neural Pathways,Neural Pathways: physiology,Neural circuits,Neural code,Neuroanatomy,Neurological,Neurons,Neurons: chemistry,Neurons: physiology,Neurophysiology,Neuropsychology {\&} Neurology [2520],New World monkey,New York,Noise,Normal Distribution,Ocular,Ocular: physiology,Orientation,Orientation: physiology,Orienting,Oscillation,Oscillations,Oscillatory,Oscillatory activity,Oscillometry,Oxygen,Oxygen: blood,P3 latency,Parietal Lobe,Parietal Lobe: physiology,Parvocellular pathway,Pattern Recognition,Perception,Perception: physiology,Perceptual enhancement,Periodicity,Phase,Phase resetting,Photic Stimulation,Photic Stimulation: methods,Physical Stimulation,Physiological,Physiological: physiology,Plasticity,Poisson Distribution,Polysensory,Posterior auditory field,Power,Principles of integration,Proprioception,Psychological,Psychological Theory,Psychological: physiology,Psychomotor Performance,Psychomotor Performance: physiology,Psychophysics,Rats,Reaction Time,Reaction Time/*physiology,Reaction Time: physiology,Real-time electrophysiology,Receptors,Redundancy gain,Redundant signals effect (RSE),Reference Values,Regression Analysis,Research,Response time,Retrograde tracers,Reward,Robustness,SYSNEURO,Sensation,Sensation: physiology,Sensory,Sensory Receptor Cells,Sensory Receptor Cells: physiology,Sensory Thresholds,Sensory Thresholds: physiology,Sensory integration,Sensory processing,Signal Detection,Signal Processing,Simple reaction time,Single-trial analyses,Somatosensory,Somatosensory Cortex,Somatosensory Cortex/*cytology/*physiology,Somatosensory Cortex: anatomy {\&} histology,Somatosensory Cortex: blood supply,Somatosensory Cortex: physiology,Somatosensory/physiology,Sound Localization,Sound Localization: physiology,Spatial attention,Species Specificity,Speech,Speech Perception,Speech Perception: physiology,Speech perception,Statistical,Stereoelectroencephalography,Stereotactic electroencephalography (SEEG),Stereotaxy,Stochastic Processes,Structure,Subthreshold facilitation,Superadditivity,Superior colliculus,Superior temporal sulcus,Synchronization,Sysneuro,Tactile,Task Performance and Analysis,Temporal Lobe,Temporal Lobe: physiology,Thalamus,Theta,Time Factors,Time-frequency analysis,Tomography,Touch,Touch/*physiology,Touch: physiology,United States,V1,Valine,Valine: analogs {\&} derivatives,Valine: pharmacology,Vision,Visual,Visual Cortex,Visual Cortex: anatomy {\&} histology,Visual Cortex: physiology,Visual Fields,Visual Pathways,Visual Pathways: physiology,Visual Perception,Visual Perception/*physiology,Visual Perception: physiology,Visual learning,Visual perception,Visual-auditory,Visual/physiology,Visual: drug effects,Visual: physiology,Voice,WGA-HRP,abbreviation,accepted april 15,acop,additive model,all rights reserved,ambiguous perception,amplitude coupling,amplitude envelope correlations,and behaviors,and head,and to amplify the,april 11,area cm,areas,audiovisual,audition,auditory,auditory cortex,been used to help,behaving cat,behavioral,behavioral rhythm,beta,beta oscillations,bimodal,binding,bistable perception,brain rhythms,canonical,cat,characteristic frequency,chronization,cognitive,cognitive neuroscience,cognitive processing,colorectal tumor,computation,computer-aided drug design,connectivity,context,convergence,cortex,cortical,cross-frequency coupling,cross-modal,crossmodal,crossmodal {\'{a}} multimodal interplay,csd,current source density,cyclophilin,default,deficits,divisive normalization,driver and,ecog,eeg,eeg source analysis,electrocorticogram,electrocorticography,electroencephalography,electrophysiological studies,electrophysiology,empirical mode decomposition,en-,entrainment,erp,erps,event-related potential,evoked response,eye contact,eye movement,eyes-re-head,fMRI,fmri,free energy,free energy perturbation,frequency tuning,fries et al,frontal eye field,gamma,gamma band,gamma oscillations,gamma-b,gamma-band frequency,gaze shifts,gaze shifts are coordinated,ghazanfar,gray et al,growing fields in the,hancement,head-re-space,heading,hepatoma,hfo,high frequency oscillation,high frequency oscillations,high gamma,high-frequency,high-frequency activity,hiv,hsv,human,identify sensory and cognitive,immunotherapy,in individuals with schizophrenia,influence our perceptions,insula,interactions between sensory modalities,intracranial,intracranial eeg,intracranial recordings,intrinsic coupling modes,joost x,journal of neuroscience,language,large-scale neuronal networks,lateral belt,laura source estimation,local field,local field potential,local field potential (LFP),looming biases in monkey,macaque,macaque monkey,magnetoencephalography,maier and asif a,mcgurk,memory,microcircuit,mismatch negativity,monkey,motion,motor,motor readiness,movements of the eyes,mst,multimodal integration,multiscale interactions,multisensory,multisensory enhancement,multisensory integration,multisensory interactions,multiunit activity,muscle artifacts,natural sounds,neural model,neural networks,neuronal,neurosciences is the study,of attention,of behaviorally relevant stimuli,of multisensory,oncolytic therapy,one of the fastest,onset latency,optic flow,oscillations,oscillatory activity,p300,perception,phase,phase coding,phase coupling,phase resetting,phase syn-,phase-amplitude coupling,phase-resetting,posteromedial cortex,potential,power,precuing,predictive coding,press on behalf of,principle,psychology,published by oxford university,pulse,quartett,reading,reafference,received february 14,receiver operating characteristic (ROC),recordings have long,redundant signals effect,reference frame,reset {\'{a}} rodent {\'{a}},reverse transcriptase,revised april 14,rhythmic sampling,rse,self-motion,sensorimotor,sensory,sensory processing,single-neuron level,single-trial analyses,single-unit electrophysiology,social cognition,somatosensory,sound,sound-induced flash illusion,spatial,spatial attention,spatial orientation,speech,spike synchrony,spike-field coherence,stimulus detection,stimulus discrimination,stroboscopic alternative motion,super-additivity,superior colliculus,superior temporal,superior temporal sulcus,suppression,synchronization,synchrony,systems,systems neuroscience,target selec-,temporal measure,that rapidly reorient the,the guarantors of brain,the spatial rule,theta band,theta oscillations,they may affect shifts,through analyses of,time-frequency analysis,tion,tonotopy,touch,tpo,translation,vestibular,vision,visual,visual axis,visual cortex,visual discrimination,visual dominance,visual fixation,visual motion,{\ss} the author,{\'{a}} phase},
mendeley-groups = {First Exam Citations/Ch4},
month = {jul},
number = {3},
pages = {1195--1268},
pmid = {23856236},
title = {{Neurophysiological and Computational Principles of Cortical Rhythms in Cognition}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23177956{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/10944237{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pmc/articles/PMC16941/pdf/pq009777.pdf{\%}5Cnhttp://dx.doi.org/10.1016/j.neuroimage.2012.06.039{\%}5Cnhttp://www.pubmedcentral.nih.gov/arti http://www.physiology.org/doi/10.1152/physrev.00035.2008},
volume = {90},
year = {2010}
}
@article{Buzsaki2011,
author = {Buzs{\'{a}}ki, Gy{\"{o}}rgy and Draguhn, Andreas},
doi = {10.1126/science.1099745},
file = {:C$\backslash$:/Users/ivan/Desktop/buzsaki2004.pdf:pdf},
isbn = {1095-9203 (Electronic){\$}\backslash{\$}n0036-8075 (Linking)},
issn = {0036-8075},
journal = {American Association for the Advancement of Science},
mendeley-groups = {First Exam Citations/Ch4},
number = {5679},
pages = {1926--1929},
pmid = {15218136},
title = {{Neuronal Oscillations in Cortical Networks}},
url = {http://www.jstor.org/stable/3837193},
volume = {304},
year = {2011}
}
@article{Muresan2008,
abstract = {We present a method that estimates the strength of neuronal oscillations at the cellular level, relying on autocorrelation histograms computed on spike trains. The method delivers a number, termed oscillation score, that estimates the degree to which a neuron is oscillating in a given frequency band. Moreover, it can also reliably identify the oscillation frequency and strength in the given band, independently of the oscillation in other frequency bands, and thus it can handle superimposed oscillations on multiple scales (theta, alpha, beta, gamma, etc.). The method is relatively simple and fast. It can cope with a low number of spikes, converging exponentially fast with the number of spikes, to a stable estimation of the oscillation strength. It thus lends itself to the analysis of spike-sorted single-unit activity from electrophysiological recordings. We show that the method performs well on experimental data recorded from cat visual cortex and also compares favorably to other methods. In addition, we provide a measure, termed confidence score, that determines the stability of the oscillation score estimate over trials.},
author = {Muresan, R. C. and Jurjut, O. F. and Moca, V. V. and Singer, W. and Nikolic, D.},
doi = {10.1152/jn.00772.2007},
file = {:C$\backslash$:/Users/ivan/Desktop/jn.00772.2007.pdf:pdf},
isbn = {0022-3077 (Print)$\backslash$r0022-3077 (Linking)},
issn = {0022-3077},
journal = {Journal of Neurophysiology},
mendeley-groups = {First Exam Citations/Ch4},
number = {3},
pages = {1333--1353},
pmid = {18160427},
title = {{The Oscillation Score: An Efficient Method for Estimating Oscillation Strength in Neuronal Activity}},
url = {http://jn.physiology.org/cgi/doi/10.1152/jn.00772.2007},
volume = {99},
year = {2008}
}
@article{Silipo1999,
author = {Silipo, Rosaria and Greenberg, Steven and Arai, Takayuki},
file = {:C$\backslash$:/Users/ivan/Downloads/Eurospeech99.pdf:pdf},
journal = {Proceedings of Eurospeech},
mendeley-groups = {First Exam Citations/Ch4},
pages = {2687--2690},
title = {{Temporal constraints on speech intelligibility as deduced from exceedingly sparse spectral representations}},
year = {1999}
}
@article{Obleser2007,
abstract = {Speech processing in auditory cortex and beyond is a remarkable yet poorly understood faculty of the listening brain. Here we show that stop consonants, as the most transient constituents of speech, are sufficient to involve speech perception circuits in the human superior temporal cortex. Left anterolateral superior temporal cortex showed a stronger response in blood oxygenation leveldependent functional magnetic resonance imaging (fMRI) to intelligible consonantal bursts compared with incomprehensible control sounds matched for spectrotemporal complexity. Simultaneously, the left posterior superior temporal plane (including planum temporale [PT]) exhibited a noncategorical responsivity to complex stimulus acoustics across all trials, showing no preference for intelligible speech sounds. Multistage hierarchical processing of speech sounds is thus revealed with fMRI, providing evidence for a role of the PT in the fundamental stages of the acoustic analysis of complex sounds, including speech.},
author = {Obleser, Jonas and Zimmermann, Jonas and {Van Meter}, John and Rauschecker, Josef P.},
doi = {10.1093/cercor/bhl133},
file = {:C$\backslash$:/Users/ivan/Downloads/bhl133.pdf:pdf},
isbn = {1047-3211},
issn = {10473211},
journal = {Cerebral Cortex},
keywords = {Auditory cortex,Consonants,Hierarchical processing,Planum temporale,Speech,fMRI},
mendeley-groups = {First Exam Citations/Ch4},
number = {10},
pages = {2251--2257},
pmid = {17150986},
title = {{Multiple stages of auditory speech perception reflected in event-related fMRI}},
volume = {17},
year = {2007}
}
@article{Drullman1994,
abstract = {This paper describes two experiments on the effect of reduced spectral contrast on the speech-reception threshold (SRT) for sentences in a background of interfering sound. Signal processing is performed by smoothing the envelope of the squared short-time fast Fourier transform by a convolution with a Gaussian-shaped filter, and overlapping additions to reconstruct a continuous signal. In the first experiment the effect of reduced spectral contrast on the SRT for male speech is investigated and compared with previously obtained results for female speech [ter Keurs et al., J. Acoust. Soc. Am. 91, 2872-2880 (1992)]. Spectral energy is smeared over bandwidths of 1/8, 1/4, 1/3, 1/2, 1, 2, and 4 oct. The results show that, despite the differences in spectral pattern between male and female voices, the SRT in noise increases similarly for both voices for smearing bandwidths over 1/3 oct. In terms of the ripple density of the spectral envelope the results indicate that the range of lower spectral modulations, up to a limit of about 1.5 periods/oct, is sufficient for the intelligibility of speech in interfering sounds. In the second experiment the extent of the threshold difference between a speech masker and a noise masker is investigated for spectral smearing bandwidths of 1/2, 1, and 2 oct. The release from masking found for the speech masker relative to the (steady-state) noise masker decreases with spectral envelope smearing.},
author = {Drullman, Rob and Festen, Joost M. and Plomp, Reinier},
doi = {10.1121/1.408467},
file = {:C$\backslash$:/Users/ivan/Downloads/Drull94-envsmear.pdf:pdf},
isbn = {0001-4966},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
mendeley-groups = {First Exam Citations/Ch4},
number = {2},
pages = {1053--1064},
pmid = {8473608},
title = {{Effect of temporal envelope smearing on speech reception}},
url = {http://asa.scitation.org/doi/10.1121/1.408467},
volume = {95},
year = {1994}
}
@article{Cohen2017,
abstract = {It is said that we lose track of time -that " time flies " -when we are engrossed in a story. How does engagement with the story cause this distorted perception of time, and what are its neural correlates? People commit both time and attentional resources to an engaging stimulus. For narrative videos, attentional engagement can be represented as the level of similarity between the electroencephalographic responses of different viewers. Here we show that this measure of neural engagement predicted the duration of time that viewers were willing to commit to narrative videos. Contrary to popular wisdom, engagement did not distort the average perception of time duration. Rather, more similar brain responses resulted in a more uniform perception of time across viewers. These findings suggest that by capturing the attention of an audience, narrative videos bring both neural processing and the subjective perception of time into synchrony.},
author = {Cohen, Samantha S. and Henin, Simon and Parra, Lucas C.},
doi = {10.1038/s41598-017-04402-4},
file = {:home/ivan/Downloads/s41598-017-04402-4.pdf:pdf},
issn = {20452322},
journal = {Scientific Reports},
mendeley-groups = {First Exam Citations/Ch4},
number = {1},
pages = {1--10},
publisher = {Springer US},
title = {{Engaging narratives evoke similar neural activity and lead to similar time perception}},
volume = {7},
year = {2017}
}
@article{Petroni2017,
author = {Petroni, Agustin and Cohen, Samantha S. and Ai, Lei and Langer, Nicolas and Henin, Simon and Vanderwal, Tamara and Milham, Michael P. and Parra, Lucas C.},
doi = {10.1523/ENEURO.0244-17.2017},
file = {:home/ivan/Downloads/ENEURO.0244-17.2017.full.pdf:pdf},
issn = {2373-2822},
journal = {bioRxiv},
mendeley-groups = {First Exam Citations/Ch4},
title = {{Age and sex modulate the variability of neural responses to naturalistic videos}},
url = {http://www.biorxiv.org/content/early/2017/07/14/089060},
year = {2017}
}
@article{Ghitza2013,
abstract = {A RECENT COMMENTARY (OSCILLATORS AND SYLLABLES: a cautionary note. Cummins, 2012) questions the validity of a class of speech perception models inspired by the possible role of neuronal oscillations in decoding speech (e.g., Ghitza, 2011; Giraud and Poeppel, 2012). In arguing against the approach, Cummins raises a cautionary flag "from a phonetician's point of view." Here we respond to his arguments from an auditory processing viewpoint, referring to a phenomenological model of Ghitza (2011) taken as a representative of the criticized approach. We shall conclude by proposing the theta-syllable as an information unit defined by cortical function-an alternative to the conventional, ambiguously defined syllable. In the large context, the resulting discussion debate should be viewed as a subtext of acoustic and auditory phonetics vs. articulatory and motor theories of speech reception.},
author = {Ghitza, Oded},
doi = {10.3389/fpsyg.2013.00138},
file = {:home/ivan/Downloads/fpsyg-04-00138.pdf:pdf},
isbn = {1664-1078},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Cascaded neuronal oscillations,Everyday speech,Hierarchical window structure,Syllabic parsing,Synchronization},
mendeley-groups = {First Exam Citations/Ch4},
number = {MAR},
pages = {1--5},
pmid = {23519170},
title = {{The theta-syllable: A unit of speech information defined by cortical function}},
volume = {4},
year = {2013}
}
@article{Ghitza2009,
abstract = {This study was motivated by the prospective role played by brain rhythms in speech perception. The intelligibility - in terms of word error rate - of natural-sounding, synthetically generated sentences was measured using a paradigm that alters speech-energy rhythm over a range of frequencies. The material comprised 96 semantically unpredictable sentences, each approximately 2 s long (6-8 words per sentence), generated by a high-quality text-to-speech (TTS) synthesis engine. The TTS waveform was time-compressed by a factor of 3, creating a signal with a syllable rhythm three times faster than the original, and whose intelligibility is poor ({\textless}50{\%} words correct). A waveform with an artificial rhythm was produced by automatically segmenting the time-compressed waveform into consecutive 40-ms fragments, each followed by a silent interval. The parameters varied were the length of the silent interval (0-160 ms) and whether the lengths of silence were equal ('periodic') or not ('aperiodic'). The performance curve (word error rate as a function of mean duration of silence) was U-shaped. The lowest word error rate (i.e., highest intelligibility) occurred when the silence was 80 ms long and inserted periodically. This is also the condition for which word error rate increased when the silence was inserted aperiodically. These data are consistent with a model (TEMPO) in which low-frequency brain rhythms affect the ability to decode the speech signal. In TEMPO, optimum intelligibility is achieved when the syllable rhythm is within the range of the high theta-frequency brain rhythms (6-12 Hz), comparable to the rate at which segments and syllables are articulated in conversational speech.},
author = {Ghitza, Oded and Greenberg, Steven},
doi = {10.1159/000208934},
file = {:home/ivan/Downloads/ghitza2009.pdf:pdf},
isbn = {1423-0321},
issn = {00318388},
journal = {Phonetica},
mendeley-groups = {First Exam Citations/Ch4},
number = {1-2},
pages = {113--126},
pmid = {19390234},
title = {{On the possible role of brain rhythms in speech perception: Intelligibility of time-compressed speech with periodic and aperiodic insertions of silence}},
volume = {66},
year = {2009}
}
@article{Pellegrino2011,
abstract = {This article is a crosslinguistic investigation of the hypothesis that the average information rate conveyed during speech communication results from a trade-off between average information density and speech rate. The study, based on seven languages, shows a negative correlation between density and rate, indicating the existence of several encoding strategies. However, these strategies do not necessarily lead to a constant information rate. These results are further investigated in relation to the notion of syllabic complexity.},
author = {Pellegrino, Fran{\c{c}}ois and Coup{\'{e}}, Christophe and Marsico, Egidio},
doi = {10.1353/lan.2011.0057},
file = {:home/ivan/Desktop/project{\_}muse{\_}449938.pdf:pdf},
isbn = {0097-8507},
issn = {1535-0665},
journal = {Language},
keywords = {cross-lan-,information theory,speech communication,speech rate,working memory},
mendeley-groups = {First Exam Citations/Ch2},
number = {3},
pages = {539--558},
pmid = {21252317},
title = {{Across-Language Perspective on Speech Information Rate}},
url = {http://muse.jhu.edu/content/crossref/journals/language/v087/87.3.pellegrino.html},
volume = {87},
year = {2011}
}
@article{Greenberg2003,
abstract = {Temporal properties of the speech signal are of potentially great importance for understanding spoken language and may provide significant insight into the manner in which listeners process spoken language with so little apparent effort. It is the thesis of this study that durational properties of phonetic segments differentially reflect the amount of information contained within a syllable, and that syllable prominence is an indirect measure of linguistic entropy. The ability to understand spoken language appears to depend on a broad distribution of syllable duration, ranging between 50 and 400 ms (for American English), which is reflected in the modulation spectrum of the acoustic signal. The upper branch of the modulation spectrum (6-20 Hz) reflects unstressed syllables, while the lower branch ({\textless} 5 Hz) represents mostly heavily stressed syllables. Low-pass filtering the modulation spectrum reduces the intelligibility of spoken sentences in a manner consistent with the differential contribution of stressed and unstressed syllables to understanding spoken language. The origins of this phenomenon are investigated in terms of the durational properties of phonetic segments contained in a corpus of spontaneous American English telephone dialogues (SWITCHBOARD). Forty-five minutes of this material was manually annotated with respect to stress accent, and the relation between accent level and segmental duration examined. Statistical analysis indicates that much of the temporal variation observed at the syllabic and phonetic-segment levels can be accounted for in terms of two basic parameters: (1) stress-accent pattern and (2) position of the segment within the syllable. Segments are generally longest in heavily stressed syllables and shortest in syllables without stress. However, the magnitude of accent's impact on duration varies as a function of syllable position. Duration of the nucleus is heavily affected by stress-accent level - heavily stressed nuclei are, on average, twice as long as their unstressed counterparts, while the duration of the onset is also significantly sensitive to stress, but to a lesser degree. In contrast, stress has relatively little impact on coda duration. This pattern of durational variation suggests that the vocalic nucleus absorbs much of the impact of stress accent and potentially sets the register for interpreting the phonetic segments contained within the syllable. Moreover, the data imply that linguistic entropy is not uniformly distributed across the syllable - the onset and nucleus convey more information than the coda. {\textcopyright} 2003 Elsevier Ltd. All rights reserved.},
author = {Greenberg, Steven and Carvey, Hannah and Hitchcock, Leah and Chang, Shuangyu},
doi = {10.1016/j.wocn.2003.09.005},
file = {:home/ivan/Desktop/1-s2.0-S0095447003000603-main.pdf:pdf},
isbn = {0095-4470},
issn = {00954470},
journal = {Journal of Phonetics},
mendeley-groups = {First Exam Citations/Ch4},
number = {3-4},
pages = {465--485},
title = {{Temporal properties of spontaneous speech - A syllable-centric perspective}},
volume = {31},
year = {2003}
}
@article{Giraud2012,
abstract = {Neuronal oscillations are ubiquitous in the brain and may contribute to cognition in several ways: for example, by segregating information and organizing spike timing. Recent data show that delta, theta and gamma oscillations are specifically engaged by the multi-timescale, quasi-rhythmic properties of speech and can track its dynamics. We argue that they are foundational in speech and language processing, 'packaging' incoming information into units of the appropriate temporal granularity. Such stimulus-brain alignment arguably results from auditory and motor tuning throughout the evolution of speech and language and constitutes a natural model system allowing auditory research to make a unique contribution to the issue of how neural oscillatory activity affects human cognition.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Giraud, Anne-Lise and Poeppel, David},
doi = {10.1038/nn.3063},
eprint = {arXiv:1011.1669v3},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Giraud, Poeppel - 2012 - Cortical oscillations and speech processing emerging computational principles and operations.pdf:pdf},
isbn = {1546-1726 (Electronic)$\backslash$r1097-6256 (Linking)},
issn = {1546-1726},
journal = {Nature neuroscience},
keywords = {Acoustic Stimulation,Acoustic Stimulation: methods,Action Potentials,Action Potentials: physiology,Auditory Cortex,Auditory Cortex: physiology,Brain Mapping,Brain Mapping: methods,Brain Waves,Brain Waves: physiology,Computational Biology,Computational Biology: methods,Cortical Synchronization,Cortical Synchronization: physiology,Humans,Nerve Net,Nerve Net: physiology,Speech,Speech Perception,Speech Perception: physiology,Speech: physiology},
mendeley-groups = {First Exam - To Read,First Exam Citations/Ch4},
number = {4},
pages = {511--7},
pmid = {22426255},
publisher = {Nature Publishing Group},
title = {{Cortical oscillations and speech processing: emerging computational principles and operations.}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84859217287{\&}partnerID=tZOtx3y1},
volume = {15},
year = {2012}
}
@article{Ding2009,
abstract = {Natural sounds such as speech contain multiple levels and multiple types of temporal modulations. Because of nonlinearities of the auditory system, however, the neural response to multiple, simultaneous temporal modulations cannot be predicted from the neural responses to single modulations. Here we show the cortical neural representation of an auditory stimulus simultaneously frequency modulated (FM) at a high rate, f(FM) approximately 40 Hz, and amplitude modulation (AM) at a slow rate, f(AM) {\textless}15 Hz. Magnetoencephalography recordings show fast FM and slow AM stimulus features evoke two separate but not independent auditory steady-state responses (aSSR) at f(FM) and f(AM), respectively. The power, rather than phase locking, of the aSSR of both decreases with increasing stimulus f(AM). The aSSR at f(FM) is itself simultaneously amplitude modulated and phase modulated with fundamental frequency f(AM), showing that the slow stimulus AM is not only encoded in the neural response at f(AM) but also encoded in the instantaneous amplitude and phase of the neural response at f(FM). Both the amplitude modulation and phase modulation of the aSSR at f(FM) are most salient for low stimulus f(AM) but remain observable at the highest tested f(AM) (13.8 Hz). The instantaneous amplitude of the aSSR at f(FM) is successfully predicted by a model containing temporal integration on two time scales, approximately 25 and approximately 200 ms, followed by a static compression nonlinearity.},
author = {Ding, Nai and Simon, Jonathan Z},
doi = {10.1152/jn.00523.2009},
file = {:C$\backslash$:/Users/ivan/Downloads/jn.00523.2009.pdf:pdf},
isbn = {0022-3077},
issn = {0022-3077},
journal = {Journal of Neurophysiology},
mendeley-groups = {First Exam Citations/Ch4},
month = {nov},
number = {5},
pages = {2731--2743},
pmid = {19692508},
title = {{Neural Representations of Complex Temporal Modulations in the Human Auditory Cortex}},
url = {http://www.physiology.org/doi/10.1152/jn.00523.2009},
volume = {102},
year = {2009}
}
@article{Elliott2009,
abstract = {We systematically determined which spectrotemporal modulations in speech are necessary for comprehension by human listeners. Speech comprehension has been shown to be robust to spectral and temporal degradations, but the specific relevance of particular degradations is arguable due to the complexity of the joint spectral and temporal information in the speech signal. We applied a novel modulation filtering technique to recorded sentences to restrict acoustic information quantitatively and to obtain a joint spectrotemporal modulation transfer function for speech comprehension, the speech MTF. For American English, the speech MTF showed the criticality of low modulation frequencies in both time and frequency. Comprehension was significantly impaired when temporal modulations {\textless}12 Hz or spectral modulations {\textless}4 cycles/kHz were removed. More specifically, the MTF was bandpass in temporal modulations and low-pass in spectral modulations: temporal modulations from 1 to 7 Hz and spectral modulations {\textless}1 cycles/kHz were the most important. We evaluated the importance of spectrotemporal modulations for vocal gender identification and found a different region of interest: removing spectral modulations between 3 and 7 cycles/kHz significantly increases gender misidentifications of female speakers. The determination of the speech MTF furnishes an additional method for producing speech signals with reduced bandwidth but high intelligibility. Such compression could be used for audio applications such as file compression or noise removal and for clinical applications such as signal processing for cochlear implants.},
author = {Elliott, Taffeta M. and Theunissen, Fr{\'{e}}d{\'{e}}ric E.},
doi = {10.1371/journal.pcbi.1000302},
file = {:C$\backslash$:/Users/ivan/Downloads/journal.pcbi.1000302.PDF:PDF},
isbn = {1553-7358 (Electronic)$\backslash$r1553-734X (Linking)},
issn = {1553734X},
journal = {PLoS Computational Biology},
mendeley-groups = {First Exam Citations/Ch4},
number = {3},
pmid = {19266016},
title = {{The modulation transfer function for speech intelligibility}},
volume = {5},
year = {2009}
}
@article{Goswami2013,
author = {Goswami, Usha and Leong, Victoria},
doi = {10.1515/lp-2013-0004},
file = {:C$\backslash$:/Users/ivan/Downloads/[Laboratory Phonology] Speech rhythm and temporal structure Converging perspectives.pdf:pdf},
isbn = {1868-6354},
issn = {1868-6354},
journal = {Laboratory Phonology},
mendeley-groups = {First Exam Citations/Ch4},
number = {1},
pages = {67--92},
title = {{Speech rhythm and temporal structure: Converging perspectives?}},
url = {https://www.degruyter.com/view/j/lp.2013.4.issue-1/lp-2013-0004/lp-2013-0004.xml},
volume = {4},
year = {2013}
}
@article{Tzounopoulos2009,
abstract = {Mechanisms of plasticity have traditionally been ascribed to higher-order sensory processing areas such as the cortex, whereas early sensory processing centers have been considered largely hard-wired. In agreement with this view, the auditory brainstem has been viewed as a nonplastic site, important for preserving temporal information and minimizing transmission delays. However, recent groundbreaking results from animal models and human studies have revealed remarkable evidence for cellular and behavioral mechanisms for learning and memory in the auditory brainstem. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
author = {Tzounopoulos, Thanos and Kraus, Nina},
doi = {10.1016/j.neuron.2009.05.002},
file = {:C$\backslash$:/Users/ivan/Downloads/PIIS0896627309003535.pdf:pdf},
isbn = {0896-6273},
issn = {08966273},
journal = {Neuron},
mendeley-groups = {First Exam Citations/Ch4},
number = {4},
pages = {463--469},
pmid = {19477149},
publisher = {Elsevier Inc.},
title = {{Learning to Encode Timing: Mechanisms of Plasticity in the Auditory Brainstem}},
url = {http://dx.doi.org/10.1016/j.neuron.2009.05.002},
volume = {62},
year = {2009}
}
@article{Meyer2018,
abstract = {The negotiation of social order is intimately connected to the capacity to infer and track status relationships. Despite the foundational role of status in social cognition, we know little about how the brain constructs status from social interactions that display it. Although emerging cognitive neuroscience reveals that status judgments depend on the intraparietal sulcus, a brain region that supports the comparison of targets along a quantitative continuum, we present evidence that status judgments do not necessarily reduce to ranking targets along a quantitative continuum. The process of judging status also fits a social interdependence analysis. Consistent with third-party perceivers judging status by inferring whose goals are dictating the terms of the interaction and who is subordinating their desires to whom, status judgments were associated with increased recruitment of medial pFC and STS, brain regions implicated in mental state inference},
archivePrefix = {arXiv},
arxivId = {1511.04103},
author = {Meyer, Lars and Gumbert, Matthias},
doi = {10.1162/jocn_a_01236},
eprint = {1511.04103},
isbn = {9780192880512},
issn = {0898-929X},
journal = {Journal of Cognitive Neuroscience},
mendeley-groups = {First Exam Citations/Ch4},
month = {jan},
number = {3},
pages = {1--10},
pmid = {23647519},
title = {{Synchronization of Electrophysiological Responses with Speech Benefits Syntactic Information Processing}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/jocn{\_}a{\_}01236},
volume = {26},
year = {2018}
}
@article{Rauschecker2009,
abstract = {Speech and language are considered uniquely human abilities: animals have communication systems, but they do not match human linguistic skills in terms of recursive structure and combinatorial power. Yet, in evolution, spoken language must have emerged from neural mechanisms at least partially available in animals. In this paper, we will demonstrate how our understanding of speech perception, one important facet of language, has profited from findings and theory in nonhuman primate studies. Chief among these are physiological and anatomical studies showing that primate auditory cortex, across species, shows patterns of hierarchical structure, topographic mapping and streams of functional processing. We will identify roles for different cortical areas in the perceptual processing of speech and review functional imaging work in humans that bears on our understanding of how the brain decodes and monitors speech. A new model connects structures in the temporal, frontal and parietal lobes linking speech perception and production.},
author = {Rauschecker, Josef P. and Scott, Sophie K.},
doi = {10.1038/nn.2331},
file = {:C$\backslash$:/Users/ivan/Downloads/rauschecker2009.pdf:pdf},
isbn = {1546-1726 (Electronic)$\backslash$r1097-6256 (Linking)},
issn = {10976256},
journal = {Nature Neuroscience},
mendeley-groups = {First Exam Citations/Ch4},
number = {6},
pages = {718--724},
pmid = {19471271},
title = {{Maps and streams in the auditory cortex: Nonhuman primates illuminate human speech processing}},
volume = {12},
year = {2009}
}
@article{Sininger1993,
abstract = {The auditory brain stem response (ABR) is felt to be an objective technique for predicting hearing thresholds because a voluntary response is not required from the subject. However, determination of ABR threshold can be a subjective process. This article discusses a technique, termed Fsp, which adds objectivity to ABR threshold detection by creating a ratio of signal plus averaged background noise over an estimate of the averaged background noise for any given averaged ABR. Fsp values have an F distribution. Consequently, the confidence of true detection for a given ABR can be determined by comparing its calculated Fsp value to statistical tables. Using a technique such as Fsp not only adds objectivity to ABR threshold detection, but also optimizes test time by allowing the averaging process to stop as soon as the background noise has been reduced and the true neural potential can be judged to be present. The estimate of the background noise can be used as a weighting factor to reduce the influence of noisy segments during the averaging process as well. Using this technique, we have found ABR threshold to be within 5 or 6 dB of psychophysical threshold for like (click) stimuli and, in our pediatric clinic, ABR click thresholds are within 10 dB of puretone average for children with losses ranging from mild to profound.},
author = {Sininger, Yvonne S.},
doi = {10.1097/00003446-199302000-00004},
file = {:C$\backslash$:/Users/ivan/Downloads/sininger1993.pdf:pdf},
isbn = {0196-0202 (Print)$\backslash$r0196-0202 (Linking)},
issn = {0196-0202},
journal = {Ear and Hearing},
mendeley-groups = {First Exam Citations/Ch4},
number = {1},
pages = {23--30},
pmid = {8444334},
title = {{Auditory Brain Stem Response for Objective Measures of Hearing}},
url = {http://content.wkhealth.com/linkback/openurl?sid=WKPTLP:landingpage{\&}an=00003446-199302000-00004},
volume = {14},
year = {1993}
}
@misc{Bhattacharyya2017,
author = {Bhattacharyya, Neil},
booktitle = {Medscape},
mendeley-groups = {First Exam Citations/Ch4},
title = {{Auditory Brainstem Response Audiometry}},
url = {https://emedicine.medscape.com/article/836277-overview{\#}showall},
year = {2017}
}
@article{Parker2005,
abstract = {Recent electrophysiological investigations of the auditory system in primates along with functional neuroimaging studies of auditory perception in humans have suggested there are two pathways arising from the primary auditory cortex. In the primate brain, a 'ventral' pathway is thought to project anteriorly from the primary auditory cortex to prefrontal areas along the superior temporal gyrus while a separate 'dorsal' route connects these areas posteriorly via the inferior parietal lobe. We use diffusion MRI tractography, a noninvasive technique based on diffusion-weighted MRI, to investigate the possibility of a similar pattern of connectivity in the human brain for the first time. The dorsal pathway from Wernicke's area to Broca's area is shown to include the arcuate fasciculus and connectivity to Brodmann area 40, lateral superior temporal gyrus (LSTG), and lateral middle temporal gyrus. A ventral route between Wernicke's area and Broca's area is demonstrated that connects via the external capsule/uncinate fasciculus and the medial superior temporal gyrus. Ventral connections are also observed in the lateral superior and middle temporal gyri. The connections are stronger in the dominant hemisphere, in agreement with previous studies of functional lateralization of auditory-language processing. {\textcopyright} 2004 Elsevier Inc. All rights reserved.},
author = {Parker, Geoffrey J M and Luzzi, Simona and Alexander, Daniel C. and Wheeler-Kingshott, Claudia A M and Ciccarelli, Olga and {Lambon Ralph}, Matthew A.},
doi = {10.1016/j.neuroimage.2004.08.047},
file = {:C$\backslash$:/Users/ivan/Downloads/parker2005.pdf:pdf},
isbn = {1053-8119},
issn = {10538119},
journal = {NeuroImage},
keywords = {Diffusion-weighted imaging,Language,Lateralization,Tractography},
mendeley-groups = {First Exam Citations/Ch4},
number = {3},
pages = {656--666},
pmid = {15652301},
title = {{Lateralization of ventral and dorsal auditory-language pathways in the human brain}},
volume = {24},
year = {2005}
}
@misc{Hickok2000,
abstract = {The functional neuroanatomy of speech perception has been difficult to characterize. Part of the difficulty, we suggest, stems from the fact that the neural systems supporting 'speech perception' vary as a function of the task. Specifically, the set of cognitive and neural systems involved in performing traditional laboratory speech perception tasks, such as syllable discrimination or identification, only partially overlap those involved in speech perception as it occurs during natural language comprehension. In this review, we argue that cortical fields in the posterior-superior temporal lobe, bilaterally, constitute the primary substrate for constructing sound-based representations of speech, and that these sound-based representations interface with different supramodal systems in a task-dependent manner. Tasks that require access to the mental lexicon (i.e. accessing meaning-based representations) rely on auditory-to-meaning interface systems in the cortex in the vicinity of the left temporal-parietal-occipital junction. Tasks that require explicit access to speech segments rely on auditory-motor interface systems in the left frontal and parietal lobes. This auditory-motor interface system also appears to be recruited in phonological working memory. {\textcopyright} 2000 Elsevier Science Ltd.},
author = {Hickok, Gregory and Poeppel, David},
booktitle = {Trends in Cognitive Sciences},
doi = {10.1016/S1364-6613(00)01463-7},
isbn = {1364-6613},
issn = {13646613},
mendeley-groups = {First Exam Citations/Ch4},
number = {4},
pages = {131--138},
pmid = {10740277},
title = {{Towards a functional neuroanatomy of speech perception}},
volume = {4},
year = {2000}
}
@article{Hickok2004,
abstract = {Despite intensive work on language-brain relations, and a fairly impressive accumulation of knowledge over the last several decades, there has been little progress in developing large-scale models of the functional anatomy of language that integrate neuropsychological, neuroimaging, and psycholinguistic data. Drawing on relatively recent developments in the cortical organization of vision, and on data from a variety of sources, we propose a new framework for understanding aspects of the functional anatomy of language which moves towards remedying this situation. The framework posits that early cortical stages of speech perception involve auditory fields in the superior temporal gyrus bilaterally (although asymmetrically). This cortical processing system then diverges into two broad processing streams, a ventral stream, which is involved in mapping sound onto meaning, and a dorsal stream, which is involved in mapping sound onto articulatory-based representations. The ventral stream projects ventro-laterally toward inferior posterior temporal cortex (posterior middle temporal gyrus) which serves as an interface between sound-based representations of speech in the superior temporal gyrus (again bilaterally) and widely distributed conceptual representations. The dorsal stream projects dorso-posteriorly involving a region in the posterior Sylvian fissure at the parietal-temporal boundary (area Spt), and ultimately projecting to frontal regions. This network provides a mechanism for the development and maintenance of "parity" between auditory and motor representations of speech. Although the proposed dorsal stream represents a very tight connection between processes involved in speech perception and speech production, it does not appear to be a critical component of the speech perception process under normal (ecologically natural) listening conditions, that is, when speech input is mapped onto a conceptual representation. We also propose some degree of bi-directionality in both the dorsal and ventral pathways. We discuss some recent empirical tests of this framework that utilize a range of methods. We also show how damage to different components of this framework can account for the major symptom clusters of the fluent aphasias, and discuss some recent evidence concerning how sentence-level processing might be integrated into the framework. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Hickok, Gregory and Poeppel, David},
doi = {10.1016/j.cognition.2003.10.011},
file = {:home/ivan/Downloads/1-s2.0-S0010027703002282-main.pdf:pdf},
isbn = {0010-0277 (Print)$\backslash$n0010-0277 (Linking)},
issn = {00100277},
journal = {Cognition},
keywords = {Aphasia,Dorsal and ventral streams,Functional anatomy,Language,Speech perception,Speech production},
mendeley-groups = {First Exam Citations/Ch4},
number = {1-2},
pages = {67--99},
pmid = {15037127},
title = {{Dorsal and ventral streams: A framework for understanding aspects of the functional anatomy of language}},
volume = {92},
year = {2004}
}
@article{Zoefel2016,
abstract = {Phase entrainment of neural oscillations, the brain's adjustment to rhythmic stimulation, is a central component in recent theories of speech comprehension: the alignment between brain oscillations and speech sound improves speech intelligibility. However, phase entrainment to everyday speech sound could also be explained by oscillations passively following the low-level periodicities (e.g., in sound amplitude and spectral content) of auditory stimulation-and not by an adjustment to the speech rhythm per se. Recently, using novel speech/noise mixture stimuli, we have shown that behavioral performance can entrain to speech sound even when high-level features (including phonetic information) are not accompanied by fluctuations in sound amplitude and spectral content. In the present study, we report that neural phase entrainment might underlie our behavioral findings. We observed phase-locking between electroencephalogram (EEG) and speech sound in response not only to original (unprocessed) speech but also to our constructed "high-level" speech/noise mixture stimuli. Phase entrainment to original speech and speech/noise sound did not differ in the degree of entrainment, but rather in the actual phase difference between EEG signal and sound. Phase entrainment was not abolished when speech/noise stimuli were presented in reverse (which disrupts semantic processing), indicating that acoustic (rather than linguistic) high-level features play a major role in the observed neural entrainment. Our results provide further evidence for phase entrainment as a potential mechanism underlying speech processing and segmentation, and for the involvement of high-level processes in the adjustment to the rhythm of speech.},
author = {Zoefel, Benedikt and VanRullen, Rufin},
doi = {10.1016/j.neuroimage.2015.08.054},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zoefel, VanRullen - 2016 - EEG oscillations entrain their phase to high-level features of speech sound.pdf:pdf},
isbn = {1095-9572 (Electronic)
1053-8119 (Linking)},
issn = {10959572},
journal = {NeuroImage},
keywords = {Auditory,EEG,High-level,Intelligibility,Oscillation,Phase entrainment,Speech},
mendeley-groups = {First Exam Citations/Ch4},
pages = {16--23},
pmid = {26341026},
title = {{EEG oscillations entrain their phase to high-level features of speech sound}},
volume = {124},
year = {2016}
}
@article{Millman2015,
abstract = {The negotiation of social order is intimately connected to the capacity to infer and track status relationships. Despite the foundational role of status in social cognition, we know little about how the brain constructs status from social interactions that display it. Although emerging cognitive neuroscience reveals that status judgments depend on the intraparietal sulcus, a brain region that supports the comparison of targets along a quantitative continuum, we present evidence that status judgments do not necessarily reduce to ranking targets along a quantitative continuum. The process of judging status also fits a social interdependence analysis. Consistent with third-party perceivers judging status by inferring whose goals are dictating the terms of the interaction and who is subordinating their desires to whom, status judgments were associated with increased recruitment of medial pFC and STS, brain regions implicated in mental state inference},
archivePrefix = {arXiv},
arxivId = {1511.04103},
author = {Millman, Rebecca E. and Johnson, Sam R. and Prendergast, Garreth},
doi = {10.1162/jocn_a_00719},
eprint = {1511.04103},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Millman, Johnson, Prendergast - 2015 - The Role of Phase-locking to the Temporal Envelope of Speech in Auditory Perception and Speech In.pdf:pdf},
isbn = {9780192880512},
issn = {0898-929X},
journal = {Journal of Cognitive Neuroscience},
mendeley-groups = {First Exam Citations/Ch4},
month = {mar},
number = {3},
pages = {533--545},
pmid = {23647519},
title = {{The Role of Phase-locking to the Temporal Envelope of Speech in Auditory Perception and Speech Intelligibility}},
url = {http://www.mitpressjournals.org/doi/10.1162/jocn{\_}a{\_}00719},
volume = {27},
year = {2015}
}
@article{Zoefel2015,
abstract = {Constantly bombarded with input, the brain has the need to filter out relevant information while ignoring the irrelevant rest. A powerful tool may be represented by neural oscillations which entrain their high-excitability phase to important input while their low-excitability phase attenuates irrelevant information. Indeed, the alignment between brain oscillations and speech improves intelligibility and helps dissociating speakers during a "cocktail party". Although well-investigated, the contribution of low- and high-level processes to phase entrainment to speech sound has only recently begun to be understood. Here, we review those findings, and concentrate on three main results: (1) Phase entrainment to speech sound is modulated by attention or predictions, likely supported by top-down signals and indicating higher-level processes involved in the brain's adjustment to speech. (2) As phase entrainment to speech can be observed without systematic fluctuations in sound amplitude or spectral content, it does not only reflect a passive steady-state "ringing" of the cochlea, but entails a higher-level process. (3) The role of intelligibility for phase entrainment is debated. Recent results suggest that intelligibility modulates the behavioral consequences of entrainment, rather than directly affecting the strength of entrainment in auditory regions. We conclude that phase entrainment to speech reflects a sophisticated mechanism: several high-level processes interact to optimally align neural oscillations with predicted events of high relevance, even when they are hidden in a continuous stream of background noise.},
author = {Zoefel, Benedikt and VanRullen, Rufin},
doi = {10.3389/fnhum.2015.00651},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zoefel, VanRullen - 2015 - The Role of High-Level Processes for Oscillatory Phase Entrainment to Speech Sound.pdf:pdf},
issn = {1662-5161},
journal = {Frontiers in Human Neuroscience},
keywords = {EEG, oscillation, phase, entrainment, high-level,,amount of,auditory,cope with an enormous,eeg,entrainment,high-level,in virtually every situation,incoming information,intelligibility,of our life,only a fraction of,oscillation,phase,phase entrainment as a,s interpretation,speech,the brain has to,the scene,tool for input gating,which is essential for},
mendeley-groups = {First Exam Citations/Ch4},
number = {December},
pages = {1--12},
pmid = {26696863},
title = {{The Role of High-Level Processes for Oscillatory Phase Entrainment to Speech Sound}},
url = {http://journal.frontiersin.org/Article/10.3389/fnhum.2015.00651/abstract},
volume = {9},
year = {2015}
}
@article{Ding2017,
abstract = {Speech and music have structured rhythms. Here we discuss a major acoustic correlate of spoken and musical rhythms, the slow (0.25-32. Hz) temporal modulations in sound intensity and compare the modulation properties of speech and music. We analyze these modulations using over 25. h of speech and over 39. h of recordings of Western music. We show that the speech modulation spectrum is highly consistent across 9 languages (including languages with typologically different rhythmic characteristics). A different, but similarly consistent modulation spectrum is observed for music, including classical music played by single instruments of different types, symphonic, jazz, and rock. The temporal modulations of speech and music show broad but well-separated peaks around 5 and 2. Hz, respectively. These acoustically dominant time scales may be intrinsic features of speech and music, a possibility which should be investigated using more culturally diverse samples in each domain. Distinct modulation timescales for speech and music could facilitate their perceptual analysis and its neural processing.},
author = {Ding, Nai and Patel, Aniruddh D. and Chen, Lin and Butler, Henry and Luo, Cheng and Poeppel, David},
doi = {10.1016/j.neubiorev.2017.02.011},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding et al. - 2017 - Temporal modulations in speech and music.pdf:pdf},
isbn = {0149-7634},
issn = {01497634},
journal = {Neuroscience {\&} Biobehavioral Reviews},
keywords = {Modulation spectrum,Music,Rhythm,Speech,Temporal modulations},
mendeley-groups = {First Exam Citations/Ch4},
month = {oct},
pages = {181--187},
pmid = {28212857},
publisher = {Elsevier Ltd},
title = {{Temporal modulations in speech and music}},
url = {http://dx.doi.org/10.1016/j.neubiorev.2017.02.011 http://linkinghub.elsevier.com/retrieve/pii/S0149763416305668},
volume = {81},
year = {2017}
}
@article{Vanthornhout2017,
author = {Vanthornhout, J. and Decruy, L. and Wouters, J. and Simon, J.Z. and Francart, T.},
doi = {10.1101/246660},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vanthornhout et al. - 2017 - Speech intelligibility predicted from neural entrainment of the speech envelope.pdf:pdf},
journal = {Journal of the Association for Research in Otolaryngology},
mendeley-groups = {First Exam Citations/Ch4},
number = {637424},
title = {{Speech intelligibility predicted from neural entrainment of the speech envelope}},
year = {2017}
}
@article{Nourski2009,
abstract = {Speech comprehension relies on temporal cues contained in the speech envelope, and the auditory cortex has been implicated as playing a critical role in encoding this temporal information. We investigated auditory cortical responses to speech stimuli in subjects undergoing invasive electrophysiological monitoring for pharmacologically refractory epilepsy. Recordings were made from multicontact electrodes implanted in Heschl's gyrus (HG). Speech sentences, time compressed from 0.75 to 0.20 of natural speaking rate, elicited average evoked potentials (AEPs) and increases in event-related band power (ERBP) of cortical high-frequency (70-250 Hz) activity. Cortex of posteromedial HG, the presumed core of human auditory cortex, represented the envelope of speech stimuli in the AEP and ERBP. Envelope following in ERBP, but not in AEP, was evident in both language-dominant and -nondominant hemispheres for relatively high degrees of compression where speech was not comprehensible. Compared to posteromedial HG, responses from anterolateral HG-an auditory belt field-exhibited longer latencies, lower amplitudes, and little or no time locking to the speech envelope. The ability of the core auditory cortex to follow the temporal speech envelope over a wide range of speaking rates leads us to conclude that such capacity in itself is not a limiting factor for speech comprehension.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Nourski, K. V. and Reale, R. A. and Oya, H. and Kawasaki, H. and Kovach, C. K. and Chen, H. and Howard, M. A. and Brugge, J. F.},
doi = {10.1523/JNEUROSCI.3065-09.2009},
eprint = {NIHMS150003},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nourski et al. - 2009 - Temporal Envelope of Time-Compressed Speech Represented in the Human Auditory Cortex.pdf:pdf},
isbn = {1529-2401 (Electronic)$\backslash$r0270-6474 (Linking)},
issn = {0270-6474},
journal = {Journal of Neuroscience},
mendeley-groups = {First Exam Citations/Ch4},
number = {49},
pages = {15564--15574},
pmid = {20007480},
title = {{Temporal Envelope of Time-Compressed Speech Represented in the Human Auditory Cortex}},
url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.3065-09.2009},
volume = {29},
year = {2009}
}
@article{Jewett1970,
abstract = {Auditory potentials recorded from the vertex of humans by a modified averaging technique have very short latencies and are probably generated by brain stem structures located at a considerable distance from the recording point. The evoked waves, which shOW considerable detail and consistency within and across subjects, may be clinically useful in evaluating subcortical function.},
author = {Jewett, D. L. and Romano, M. N. and Williston, J. S.},
doi = {10.1126/science.167.3924.1517},
issn = {0036-8075},
journal = {Science},
mendeley-groups = {First Exam Citations/Ch4},
number = {3924},
pages = {1517--1518},
pmid = {5415287},
title = {{Human Auditory Evoked Potentials: Possible Brain Stem Components Detected on the Scalp}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.167.3924.1517},
volume = {167},
year = {1970}
}
@article{Young1979,
abstract = {This paper is concerned with the representation of the spectra of synthesized steady-state vowels in the temporal aspects of the discharges of auditory-nerve fibers. The results are based on a study of the responses of large numbers of single auditory-nerve fibers in anesthetized cats. By presenting the same set of stimuli to all the fibers encountered in each cat, we can directly estimate the population response to those stimuli. Period histograms of the responses of each unit to the vowels were constructed. The temporal response of a fiber to each harmonic component of the stimulus is taken to be the amplitude of the corresponding component in the Fourier transform of the unit's period histogram. At low sound levels, the temporal response to each stimulus component is maximal among units with CFs near the frequency of the component (i.e., near its place). Responses to formant components are larger than responses to other stimulus components. As sound level is increased, the responses to the formants, particularly the first formant, increase near their places and spread to adjacent regions, particularly toward higher CFs. Responses to nonformant components, exept for harmonics and intermodulation products of the formants (2F1,2F2,F1 + F2, etc), are suppressed; at the highest sound levels used (approximately 80 dB SPL), temporal responses occur almost exclusively at the first two or three formants and their harmonics and intermodulation products. We describe a simple calculation which combines rate, place, and temporal information to provide a good representation of the vowels' spectra, including a clear indication of at least the first two formant frequencies. This representation is stable with changes in sound level at least up to 80 dB SPL; its stability is in sharp contrast to the behavior of the representation of the vowels' spectra in terms of discharge rate which degenerates at stimulus levels within the conversational range.},
author = {Young, Eric D. and Sachs, Murray B.},
doi = {10.1121/1.383532},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
mendeley-groups = {First Exam Citations/Ch2},
number = {5},
pages = {1381--1403},
pmid = {500976},
title = {{Representation of steady-state vowels in the temporal aspects of the discharge patterns of populations of auditory-nerve fibers}},
url = {http://asa.scitation.org/doi/10.1121/1.383532},
volume = {66},
year = {1979}
}
@article{Jewett1971,
abstract = {Differential recording between closely spaced electrodes is useful for locating a neural generator since such a recording configuration can only detect near fields and is uninfluenced by the far fields of distant generators. Far field recordings (which imply that the generator is at a distance) offer advantages in that the position of the electrode is not critical for obtaining satisfactory recordings and in that potentials from widely spaced generators can be detected at a single electrode.Both of these advantages can be seen in the work presented here, where far field potentials evoked by auditory click stimuli are recorded from the scalps of humans. On the basis of some indirect evidence, it is possible to deduce the location of the generators of some of the waves. It is clear that far fields at least 10 cm. from their brain-stem generators can be recorded from humans and that electrical activity from any brain location within the human skull may be detectable at the scalp, given a satisfactory method of synchronizing the activity with the averager.},
author = {Jewett, Don L. and Williston, John S.},
doi = {10.1093/brain/94.4.681},
issn = {00068950},
journal = {Brain},
mendeley-groups = {First Exam Citations/Ch4},
number = {4},
pages = {681--696},
pmid = {5132966},
title = {{Auditory-evoked far fields averaged from the scalp of humans}},
volume = {94},
year = {1971}
}
@misc{pict,
author = {{Wikimedia Commons}},
mendeley-groups = {First Exam Citations},
publisher = {Wikimedia},
title = {{Spectrogram -iua-}},
url = {https://commons.wikimedia.org/wiki/File:Spectrogram{\_}-iua-.png},
year = {2005}
}
@article{Noll1967,
abstract = {The cepstrum , defined as the power spectrum of the logarithm of the power spectrum, has a strong peak corresponding to the pitch period of the voiced-speech segment being analyzed. Cepstra were calculated on a digital computer and were automatically plotted ... $\backslash$n},
author = {Noll, A Michael},
doi = {10.1121/1.1910339},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
mendeley-groups = {First Exam Citations},
number = {2},
pages = {293--309},
pmid = {6040805},
title = {{Cepstrum pitch determination}},
url = {http://scitation.aip.org/content/asa/journal/jasa/41/2/10.1121/1.1910339},
volume = {41},
year = {1967}
}
@article{Hickok2007,
abstract = {Despite decades of research, the functional neuroanatomy of speech processing has been difficult to characterize. A major impediment to progress may have been the failure to consider task effects when mapping speech-related processing systems. We outline a dual-stream model of speech processing that remedies this situation. In this model, a ventral stream processes speech signals for comprehension, and a dorsal stream maps acoustic speech signals to frontal lobe articulatory networks. The model assumes that the ventral stream is largely bilaterally organized  although there are important computational differences between the left- and right-hemisphere systems  and that the dorsal stream is strongly left-hemisphere dominant.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Hickok, Gregory and Poeppel, David},
doi = {10.1038/nrn2113},
eprint = {NIHMS150003},
file = {:home/ivan/Downloads/nrn2113.pdf:pdf},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch4},
number = {5},
pages = {393--402},
pmid = {17431404},
title = {{The cortical organization of speech processing}},
url = {http://www.nature.com/doifinder/10.1038/nrn2113},
volume = {8},
year = {2007}
}
@article{Davis1980,
abstract = {Several parametric representations of the acoustic signal were compared with regard to word recognition performance in a syllable-oriented continuous speech recognition system. The vocabulary included many phonetically similar monosyllabic words, therefore the emphasis was on the ability to retain phonetically significant acoustic information in the face of syntactic and duration variations. For each parameter set (based on a mel-frequency cepstrum, a linear frequency cepstrum, a linear prediction cepstrum, a linear prediction spectrum, or a set of reflection coefficients), word templates were generated using an efficient dynamic warping method, and test data were time registered with the templates. A set of ten mel-frequency cepstrum coefficients computed every 6.4 ms resulted in the best performance, namely 96.5 percent and 95.0 percent recognition with each of two speakers. The superior performance of the mel-frequency cepstrum coefficients may be attributed to the fact that they better represent the perceptually relevant aspects of the short-term speech spectrum.},
author = {Davis, S. and Mermelstein, Paul},
doi = {10.1109/TASSP.1980.1163420},
issn = {0096-3518},
journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
mendeley-groups = {First Exam Citations},
number = {4},
pages = {357--366},
pmid = {1163420},
title = {{Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences}},
url = {http://ieeexplore.ieee.org/document/1163420/},
volume = {28},
year = {1980}
}
@article{Riecke2017,
author = {Riecke, Lars and Formisano, Elia and Sorger, Bettina and Bakent, Deniz and Gaudrain, Etienne},
doi = {10.1101/233676},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Riecke et al. - 2017 - Neural Entrainment to Speech Modulates Speech Intelligibility.pdf:pdf},
issn = {09609822},
journal = {Current Biology},
mendeley-groups = {First Exam Citations/Ch4},
month = {dec},
pages = {1--9},
pmid = {29290557},
title = {{Neural Entrainment to Speech Modulates Speech Intelligibility}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0960982217315154},
year = {2017}
}
@book{Webster1992,
address = {New York, NY},
doi = {10.1007/978-1-4612-4416-5},
editor = {Webster, Douglas B. and Popper, Arthur N. and Fay, Richard R.},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 1992 - The Mammalian Auditory Pathway Neuroanatomy.pdf:pdf},
isbn = {978-0-387-97800-0},
mendeley-groups = {First Exam Citations/Ch4},
publisher = {Springer New York},
series = {Springer Handbook of Auditory Research},
title = {{The Mammalian Auditory Pathway: Neuroanatomy}},
volume = {1},
year = {1992}
}
@article{Poeppel2008,
abstract = {Speech perception consists of a set of computations that take continuously varying acoustic waveforms as input and generate discrete representations that make contact with the lexical representations stored in long-term memory as output. Because the perceptual objects that are recognized by the speech perception enter into subsequent linguistic computation, the format that is used for lexical representation and processing fundamentally constrains the speech perceptual processes. Consequently, theories of speech perception must, at some level, be tightly linked to theories of lexical representation. Minimally, speech perception must yield representations that smoothly and rapidly interface with stored lexical items. Adopting the perspective of Marr, we argue and provide neurobiological and psychophysical evidence for the following research programme. First, at the implementational level, speech perception is a multi-time resolution process, with perceptual analyses occurring concurrently on at least two time scales (approx. 20-80 ms, approx. 150-300 ms), commensurate with (sub)segmental and syllabic analyses, respectively. Second, at the algorithmic level, we suggest that perception proceeds on the basis of internal forward models, or uses an 'analysis-by-synthesis' approach. Third, at the computational level (in the sense of Marr), the theory of lexical representation that we adopt is principally informed by phonological research and assumes that words are represented in the mental lexicon in terms of sequences of discrete segments composed of distinctive features. One important goal of the research programme is to develop linking hypotheses between putative neurobiological primitives (e.g. temporal primitives) and those primitives derived from linguistic inquiry, to arrive ultimately at a biologically sensible and theoretically satisfying model of representation and computation in speech.},
author = {Poeppel, D. and Idsardi, W. J and van Wassenhove, V.},
doi = {10.1098/rstb.2007.2160},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Poeppel, Idsardi, van Wassenhove - 2008 - Speech perception at the interface of neurobiology and linguistics.pdf:pdf},
isbn = {0962-8436 (Print)$\backslash$r0962-8436 (Linking)},
issn = {0962-8436},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
keywords = {analysis-by-synthesis,distinctive features,forward model,multi-time resolution,predictive coding,temporal coding},
mendeley-groups = {First Exam Citations/Ch5},
number = {1493},
pages = {1071--1086},
pmid = {17890189},
title = {{Speech perception at the interface of neurobiology and linguistics}},
url = {http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2007.2160},
volume = {363},
year = {2008}
}
@misc{Galbraith1995,
abstract = {The human brain stem frequency-following response (FFR) registers phase-locked neural activity to cyclical auditory stimuli. We show that the FFR can be elicited by word stimuli, and when speech-evoked FFTs are reproduced as auditory stimuli they are heard as intelligible speech. Stimuli were 10 high- and 10 low-probability words drawn from normative verbal responses of university students. Horizontal and vertical dipole FFRs based on 1000 repetitions of each word were recorded from two different participants. Speech-evoked FFRs were evaluated by 80 listeners. The results showed significant effects of FFR participant, word probability, and whether or not words were presented with category cues. Depending on such subject and experimental variables, FFRs were correctly perceived from 5{\%} to 92{\%} of the time.},
archivePrefix = {arXiv},
arxivId = {1708.02002},
author = {Galbraith, Gary C. and Arbagey, Paul W. and Branski, Renee and Comerci, Nelson and Rector, Pollyanna M.},
booktitle = {NeuroReport},
doi = {10.1097/00001756-199511270-00021},
eprint = {1708.02002},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Galbraith et al. - 1995 - Intelligible speech encoded in the human brain stem frequency-following response.pdf:pdf},
isbn = {0959-4965},
issn = {0959-4965},
mendeley-groups = {First Exam Citations/Ch4},
number = {17},
pages = {2363--2367},
pmid = {8747154},
title = {{Intelligible speech encoded in the human brain stem frequency-following response}},
url = {http://content.wkhealth.com/linkback/openurl?sid=WKPTLP:landingpage{\&}an=00001756-199511270-00021},
volume = {6},
year = {1995}
}
@article{Skoe2010,
abstract = {This tutorial provides a comprehensive overview of the methodological approach to collecting and analyzing auditory brain stem responses to complex sounds (cABRs). cABRs provide a window into how behaviorally relevant sounds such as speech and music are processed in the brain. Because temporal and spectral characteristics of sounds are preserved in this subcortical response, cABRs can be used to assess specific impairments and enhancements in auditory processing. Notably, subcortical auditory function is neither passive nor hardwired but dynamically interacts with higher-level cognitive processes to refine how sounds are transcribed into neural code. This experience-dependent plasticity, which can occur on a number of time scales (e.g., life-long experience with speech or music, short-term auditory training, on-line auditory processing), helps shape sensory perception. Thus, by being an objective and noninvasive means for examining cognitive function and experience-dependent processes in sensory activity, cABRs have considerable utility in the study of populations where auditory function is of interest (e.g., auditory experts such as musicians, and persons with hearing loss, auditory processing, and language disorders). This tutorial is intended for clinicians and researchers seeking to integrate cABRs into their clinical or research programs.},
author = {Skoe, Erika and Kraus, Nina},
doi = {10.1097/AUD.0b013e3181cdb272},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Skoe, Kraus - 2010 - Auditory Brain Stem Response to Complex Sounds A Tutorial.pdf:pdf},
issn = {0196-0202},
journal = {Ear and Hearing},
keywords = {*Evoked Potentials,Acoustic Stimulation/*methods,Auditory,Auditory Pathways/*physiology,Brain Stem,Continuing,Education,Hearing Disorders/*diagnosis/*physiopathology,Humans,Language Disorders/diagnosis/physiopathology,Medical,Music,Neuronal Plasticity/physiology,Phonetics},
mendeley-groups = {First Exam Citations/Ch4},
number = {3},
pages = {302--324},
pmid = {20084007},
title = {{Auditory Brain Stem Response to Complex Sounds: A Tutorial}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20084007 http://content.wkhealth.com/linkback/openurl?sid=WKPTLP:landingpage{\&}an=00003446-201006000-00002},
volume = {31},
year = {2010}
}
@article{Choi2013,
abstract = {OBJECTIVES: It would be clinically valuable if an electrophysiological validation of hearing aid effectiveness in conveying speech information could be performed when a device is first provided to the individual after electroacoustic verification. This study evaluated envelope following responses (EFRs) elicited by English vowels in a steady state context and in natural sentences. It was the purpose of this study to determine whether EFRs could be detected rapidly enough to be clinically useful.$\backslash$n$\backslash$nDESIGN: EFRs were elicited using 5 vowels spanning the English vowel space, /i/, /$\epsilon$/, /{\ae}/, /(Equation is included in full-text article.)/, and /u/. These were presented either as concatenated steady state vowels (total duration 10.04 seconds) or in three 5-word sentences (total duration 11.77 seconds), where each vowel appeared once per sentence. Single-channel electroencephalogram was recorded from vertex (Cz) to the nape of the neck for 190 and 160 repetitions of the steady state vowels and sentences, respectively. The stimuli were presented at 70 dBA SPL. The fundamental frequency (f0) track from the stimuli was used with a Fourier analyzer to estimate the EFRs to each vowel. Noise amplitudes were also calculated at neighboring frequencies. Fifteen normal-hearing subjects who were 20 to 34 years of age participated in the experiment.$\backslash$n$\backslash$nRESULTS: In the analysis of steady state vowels, the mean response amplitude of /i/ was statistically the largest at 173 nV. The other 4 steady state vowels did not differ in mean response amplitude, which varied between 73 and 106 nV. In the analysis of vowels from the 3 sentences, the largest response amplitudes tended to be for /u/. Mean amplitudes for /u/ were 164, 111, and 140 nV for the words "booed," "food," and "Sue," respectively. The vowel /u/ produced statistically larger responses than /i/, /$\epsilon$/, and /(Equation is included in full-text article.)/ when grouped across words, whereas other vowels did not differ. Mean response amplitudes for the other vowel categories in the sentences varied between 82 and 105 nV. All subjects showed significant EFRs in response to the words "Bee's" and "booed," but only 9 subjects showed significant EFRs for "pet," "bed," and "Bob."$\backslash$n$\backslash$nCONCLUSIONS: The authors were readily able to detect significant EFRs elicited by vowels in a steady state context and from 3 natural sentences. These results are promising as an early step in developing a clinical tool for validating that vowel stimuli are at least partially encoded at the level of the auditory brainstem. Future research will require evaluation of the technique with aided listeners, where the natural sentences are expected to be treated as typical speech by hearing aid signal-processing algorithms.},
author = {Choi, Jong Min and Purcell, David W. and Coyne, Julie Anne M and Aiken, Steven J.},
doi = {10.1097/AUD.0b013e31828e4dad},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi et al. - 2013 - Envelope following responses elicited by english sentences.pdf:pdf},
isbn = {1538-4667 (Electronic)$\backslash$r0196-0202 (Linking)},
issn = {01960202},
journal = {Ear and Hearing},
mendeley-groups = {First Exam Citations/Ch4},
number = {5},
pages = {637--650},
pmid = {23575462},
title = {{Envelope following responses elicited by english sentences}},
volume = {34},
year = {2013}
}
@article{Power2012,
abstract = {Distinguishing between speakers and focusing attention on one speaker in multi-speaker environments is extremely important in everyday life. Exactly how the brain accomplishes this feat and, in particular, the precise temporal dynamics of this attentional deployment are as yet unknown. A long history of behavioral research using dichotic listening paradigms has debated whether selective attention to speech operates at an early stage of processing based on the physical characteristics of the stimulus or at a later stage during semantic processing. With its poor temporal resolution fMRI has contributed little to the debate, while EEG-ERP paradigms have been hampered by the need to average the EEG in response to discrete stimuli which are superimposed onto ongoing speech. This presents a number of problems, foremost among which is that early attention effects in the form of endogenously generated potentials can be so temporally broad as to mask later attention effects based on the higher level processing of the speech stream. Here we overcome this issue by utilizing the AESPA (auditory evoked spread spectrum analysis) method which allows us to extract temporally detailed responses to two concurrently presented speech streams in natural cocktail-party-like attentional conditions without the need for superimposed probes. We show attentional effects on exogenous stimulus processing in the 200-220 ms range in the left hemisphere. We discuss these effects within the context of research on auditory scene analysis and in terms of a flexible locus of attention that can be deployed at a particular processing stage depending on the task.},
author = {Power, Alan J. and Foxe, John J. and Forde, Emma Jane and Reilly, Richard B. and Lalor, Edmund C.},
doi = {10.1111/j.1460-9568.2012.08060.x},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Power et al. - 2012 - At what time is the cocktail party A late locus of selective attention to natural speech.pdf:pdf},
isbn = {1662-5145 (Electronic)$\backslash$r1662-5145 (Linking)},
issn = {0953816X},
journal = {European Journal of Neuroscience},
keywords = {AESPA,EEG,Exogenous processing,Multi-speaker environments},
mendeley-groups = {First Exam Citations/Ch4},
number = {9},
pages = {1497--1503},
pmid = {22462504},
title = {{At what time is the cocktail party? A late locus of selective attention to natural speech}},
volume = {35},
year = {2012}
}
@article{Fletcher1933,
abstract = {Loudness of sounds is defined as the magnitude of auditory sensations. Mathematical formulae are developed, and tables and graphs are presented to be used in calculating the loudness of continuous complex sounds. A discussion of the standard conditions in which the formulae hold, the types of apparatus used, the position of the observers, the frequencies and intensities of the sounds, etc., is included. (PsycINFO Database Record (c) 2009 APA, all rights reserved)},
archivePrefix = {arXiv},
arxivId = {129.255.231.83},
author = {Fletcher, H and Munson, W a},
doi = {10.1121/1.1915637},
eprint = {129.255.231.83},
issn = {0001-4966},
journal = {Journal of the Acoustical Society of America},
keywords = {Equal Loudness Curves,Loudness,Measurement,Perception,Volume},
mendeley-groups = {First Exam Citations/Ch3},
number = {1924},
pages = {82--108},
title = {{Loudness, its Definition, Measurement and Calculation}},
volume = {5},
year = {1933}
}
@book{Kandel2000,
abstract = {Important features to this edition include a new chapter - Genes and Behavior; a complete updating of development of the nervous system; the genetic basis of...},
author = {Kandel, E R and Schwartz, J H and Jessell, T M},
booktitle = {Neurology},
doi = {10.1036/0838577016},
isbn = {0838577016},
issn = {01956108},
mendeley-groups = {First Exam Citations/Ch3},
pages = {1414},
pmid = {11769631},
title = {{Principles of Neural Science}},
volume = {3},
year = {2000}
}
@article{Reichenbach2016,
abstract = {The auditory-brainstem response (ABR) to short and simple acoustical signals is an important clinical tool used to diagnose the integrity of the brainstem. The ABR is also employed to investigate the auditory brainstem in a multitude of tasks related to hearing, such as processing speech or selectively focusing on one speaker in a noisy environment. Such research measures the response of the brainstem to short speech signals such as vowels or words. Because the voltage signal of the ABR has a tiny amplitude, several hundred to a thousand repetitions of the acoustic signal are needed to obtain a reliable response. The large number of repetitions poses a challenge to assessing cognitive functions due to neural adaptation. Here we show that continuous, non-repetitive speech, lasting several minutes, may be employed to measure the ABR. Because the speech is not repeated during the experiment, the precise temporal form of the ABR cannot be determined. We show, however, that important structural features of the ABR can nevertheless be inferred. In particular, the brainstem responds at the fundamental frequency of the speech signal, and this response is modulated by the envelope of the voiced parts of speech. We accordingly introduce a novel measure that assesses the ABR as modulated by the speech envelope, at the fundamental frequency of speech and at the characteristic latency of the response. This measure has a high signal-to-noise ratio and can hence be employed effectively to measure the ABR to continuous speech. We use this novel measure to show that the auditory brainstem response is weaker to intelligible speech than to unintelligible, time-reversed speech. The methods presented here can be employed for further research on speech processing in the auditory brainstem and can lead to the development of future clinical diagnosis of brainstem function.},
author = {Reichenbach, Chagit S. and Braiman, Chananel and Schiff, Nicholas D. and Hudspeth, A. J. and Reichenbach, Tobias},
doi = {10.3389/fncom.2016.00047},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Reichenbach et al. - 2016 - The Auditory-Brainstem Response to Continuous, Non-repetitive Speech Is Modulated by the Speech Envelope and.pdf:pdf},
isbn = {1662-5188},
issn = {1662-5188},
journal = {Frontiers in Computational Neuroscience},
keywords = {abr,auditory brainstem response,auditory brainstem response (ABR), speech processi,fundamental frequency,speech envelope,speech processing},
mendeley-groups = {First Exam - To Read,First Exam Citations/Ch4},
number = {May},
pages = {1--11},
pmid = {27303286},
title = {{The Auditory-Brainstem Response to Continuous, Non-repetitive Speech Is Modulated by the Speech Envelope and Reflects Speech Processing}},
url = {http://journal.frontiersin.org/Article/10.3389/fncom.2016.00047/abstract},
volume = {10},
year = {2016}
}
@book{Schnupp2011,
abstract = {As I write these lines, a shiver is running down my back. Not that writing usually has that effect on me. But on this occasion, I am allowing myself a little moment of indulgence. As I am writing, I am also listening to one of my favorite pieces of music, the aria  Vorrei spiegarvi, oh dio!  composed by Mozart and masterfully performed by the soprano Kathleen Battle. A digital audio player sits in my pocket. It is smaller than a matchbox and outwardly serene; yet inside the little device is immensely busy, extracting 88,200 numerical values every second from computer fi les stored in its digital memory, which it converts into electrical currents. The currents, in turn, generate electric fi elds that incessantly push and tug ever so gently on a pair of delicate membranes in the ear buds of my in-ear headphones. And, voil {\`{a}} , there she is, Kathleen, hypnotizing me with her beautiful voice and dragging me through a brief but intense emotional journey that begins with a timid sadness, grows in intensity to plumb the depths of despair only to resolve into powerful and determined, almost uplifting defi ance.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Schnupp, Jan and Nelken, Israel and King, Andrew},
booktitle = {Auditory Neuroscience},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9780262113182},
issn = {1098-6596},
mendeley-groups = {First Exam Citations/Ch2},
pages = {1--50},
pmid = {25246403},
title = {{Auditory Neuroscience}},
year = {2011}
}
@book{Fastl2007,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and proteinprotein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD  2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fastl, Hugo and Zwicker, Eberhard},
booktitle = {Psychoacoustics: Facts and Models},
doi = {10.1007/978-3-540-68888-4},
eprint = {arXiv:1011.1669v3},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fastl, Zwicker - 2007 - Psychoacoustics Facts and models.pdf:pdf},
isbn = {3540231595},
issn = {1098-6596},
mendeley-groups = {First Exam Citations/Ch2},
pages = {1--463},
pmid = {25246403},
title = {{Psychoacoustics: Facts and models}},
year = {2007}
}
@misc{Mannell2008,
author = {Mannell, Robert},
booktitle = {Macquarie University},
mendeley-groups = {First Exam Citations/Ch2},
title = {{Speech Acoustics}},
url = {http://clas.mq.edu.au/speech/acoustics/waveforms/speech{\_}waveforms.html},
year = {2008}
}
@misc{Gutierrez-Osuna2017,
author = {Gutierrez-Osuna, Ricardo},
booktitle = {Texas A{\&}M},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gutierrez-Osuna - 2017 - Organization of Speech Sounds.pdf:pdf},
keywords = {organization of speech sounds},
mendeley-groups = {First Exam Citations/Ch2},
pages = {1--40},
title = {{Organization of Speech Sounds}},
year = {2017}
}
@article{Rabiner2007,
abstract = {chapter 6 LPC pag 73},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rabiner, Lawrence R. and Schafer, Ronald W.},
doi = {10.1561/2000000001},
eprint = {arXiv:1011.1669v3},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rabiner, Schafer - 2007 - Introduction to Digital Speech Processing.pdf:pdf},
isbn = {1601980701},
issn = {1932-8346},
journal = {Foundations and Trends{\textregistered} in Signal Processing},
mendeley-groups = {First Exam Citations/Ch2},
number = {12},
pages = {1--194},
pmid = {24439530},
title = {{Introduction to Digital Speech Processing}},
url = {http://www.nowpublishers.com/article/Details/SIG-001},
volume = {1},
year = {2007}
}
@article{Umeda1975,
abstract = {This is a summary report of the vowel duration data that have been accumulated over the past several years. The data corpus analyzed to derive temporal controls of vowels consists mainly of three different readings by three different speakers, each about 10 to 20 min in duration. The rules cover the temporal behavior of vowels under many phonological conditions. The conditions include stressed and unstressed positions, prepausal and nonprepausal positions, word-final and non-word-final conditions, and monosyllabic and polysyllabic words. The influence of following consonants is discussed as well. Included also are conditions other than phonological ones, such as the effect of the prominence of words on their vowels, and the speed of reading. The duration rules derived from the data are intended for use in our speech synthesis-by-rule system from printed text.},
author = {Umeda, Noriko},
doi = {10.1121/1.380688},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Umeda - 1975 - Vowel duration in american english.pdf:pdf},
issn = {0001-4966},
journal = {Journal of the Acoustical Society of America},
mendeley-groups = {First Exam Citations/Ch2},
number = {2},
pages = {434--445},
pmid = {1184837},
title = {{Vowel duration in american english}},
url = {http://scitation.aip.org/content/asa/journal/jasa/58/2/10.1121/1.380688},
volume = {58},
year = {1975}
}
@misc{Simons2017,
author = {Simons, Gary F and Fennig, Charles D},
booktitle = {Ethnologue},
mendeley-groups = {First Exam Citations/Ch1},
title = {{How many languages are there in the world?}},
url = {https://www.ethnologue.com/guides/how-many-languages},
year = {2017}
}
@article{Sereno2005,
abstract = {Despite the paucity of direct evidence about the origin of human language, the great intrinsic interest in this question has made it difficult for writers to resist speculating about it (Harnad et al., eds., 1976; Merlin, 1991; Deacon, 1997; Jablonski {\&} Aiello, eds., 1998; King, ed., 1999; Knight et al., eds., 2000). The following attempts to bring a fresh perspective on this old question, using an analogy with the origin of cellular coding systems and applying it to what we know about the evolution of vocal behavior in animals. In other places (Sereno, 1991b), I have argued that DNA and protein based life and language based human thought may have enough in common as the only two naturally occurring examples of a code-using system to make it useful to take an analogical look at one system in order to make predictions about the other. Rather than rehearsing those arguments, I will only visit two jumping off points reached while developing that analogy: the difference between origin and evolution, and the foundational role of an intermediate string of symbol representation segments with properties partway between symbol and meaning.},
author = {Sereno, M. I.},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sereno - 2005 - Language origins without the semantic urge.pdf:pdf},
journal = {Cognitive Science Online},
mendeley-groups = {First Exam Citations/Ch1,First Exam Citations/Ch2},
number = {1},
pages = {1 -- 12},
title = {{Language origins without the semantic urge}},
volume = {3},
year = {2005}
}
@book{Ladefoged1996,
abstract = {This book gives a description of all the known ways in which the sounds of the world's languages differ. In doing so, it provides the empirical foundations for linguistic phonetics and phonology. Encapsulating the work of two leading figures in the field, it will be a standard work of reference for researchers in phonetics and linguistics for many years to come. The scope of the book is truly global, with data drawn from nearly 400 languages, many of them investigated at first hand by the authors. A picture of the full range of possible contrasting phonetic categories is created by comparing families of similar sounds across many different languages. Separate chapters deal with place of articulation, stops, nasals, fricatives, laterals, rhotics, clicks, vowels, and segments with multiple articulations. Each chapter is packed with illustrations documenting the articulatory and acoustic characteristics of the sounds discussed, and serving to illustrate the application of modern experimental techniques to descriptive phonetic studies.},
author = {Ladefoged, Peter and Maddieson, Ian},
booktitle = {Blackwell Publishing},
doi = {10.1177/136700699700100110},
isbn = {0631198148},
issn = {1367-0069},
mendeley-groups = {First Exam Citations/Ch2},
pmid = {2071501},
title = {{The Sounds of the World's Languages}},
year = {1996}
}
@article{Ramachandran2001,
abstract = {We investigated graphemecolour synaesthesia and found that:$\backslash$r$\backslash$n(1) The induced colours led to perceptual grouping and pop-out, (2) a grapheme$\backslash$r$\backslash$nrendered invisible through crowding' or lateral masking induced synaesthetic$\backslash$r$\backslash$ncolours  a form of blindsight  and (3) peripherally presented graphemes did$\backslash$r$\backslash$nnot induce colours even when they were clearly visible. Taken collectively, these$\backslash$r$\backslash$nand other experiments prove conclusively that synaesthesia is a genuine perceptual$\backslash$r$\backslash$nphenomenon, not an effect based on memory associations from childhood or$\backslash$r$\backslash$non vague metaphorical speech. We identify different subtypes of numbercolour$\backslash$r$\backslash$nsynaesthesia and propose that they are caused by hyperconnectivity between colour$\backslash$r$\backslash$nand number areas at different stages in processing; lower synaesthetes may$\backslash$r$\backslash$nhave cross-wiring (or cross-activation) within the fusiform gyrus, whereas higher$\backslash$r$\backslash$nsynaesthetes may have cross-activation in the angular gyrus. This hyperconnectivity$\backslash$r$\backslash$nmight be caused by a genetic mutation that causes defective pruning of connections$\backslash$r$\backslash$nbetween brain maps. The mutation may further be expressed selectively$\backslash$r$\backslash$n(due to transcription factors) in the fusiform or angular gyri, and this may explain$\backslash$r$\backslash$nthe existence of different forms of synaesthesia. If expressed very diffusely, there$\backslash$r$\backslash$nmay be extensive cross-wiring between brain regions that represent abstract$\backslash$r$\backslash$nconcepts, which would explain the link between creativity, metaphor and$\backslash$r$\backslash$nsynaesthesia (and the higher incidence of synaesthesia among artists and poets).$\backslash$r$\backslash$nAlso, hyperconnectivity between the sensory cortex and amygdala would explain$\backslash$r$\backslash$nthe heightened aversion synaesthetes experience when seeing numbers printed in$\backslash$r$\backslash$nthe wrong' colour. Lastly, kindling (induced hyperconnectivity in the temporal$\backslash$r$\backslash$nlobes of temporal lobe epilepsy [TLE] patients) may explain the purported higher$\backslash$r$\backslash$nincidence of synaesthesia in these patients. We conclude with a$\backslash$r$\backslash$nsynaesthesia-based theory of the evolution of language. Thus, our experiments on$\backslash$r$\backslash$nsynaesthesia and our theoretical framework attempt to link several seemingly$\backslash$r$\backslash$nunrelated facts about the human mind. Far from being a mere curiosity,$\backslash$r$\backslash$nsynaesthesia may provide a window into perception, thought and language.},
author = {Ramachandran, V.S. and Hubbard, E.H.},
doi = {10.1111/1468-0068.00363},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ramachandran, Hubbard - 2001 - SynaesthesiaA Window Into Perception, Thought and Language.pdf:pdf},
isbn = {1355-8250},
issn = {13558250},
journal = {Journal of Consciousness Studies},
mendeley-groups = {First Exam - To Read,First Exam Citations},
number = {12},
pages = {3--34},
pmid = {1000104869},
title = {{SynaesthesiaA Window Into Perception, Thought and Language}},
volume = {8},
year = {2001}
}
@book{Denes1993,
abstract = {Explains the basic mechanisms involved in spoken communication, merging the field of speech pathology, communications, psychology, engineering, and computer science.},
author = {Denes, P.B. and Pinson, E.N},
doi = {10.1121/1.1971294},
isbn = {0716723441},
issn = {00014966},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch2},
pages = {1--9},
title = {{The Speech Chain}},
year = {1993}
}
@article{Mesgarani2014,
abstract = {During speech perception, linguistic elements such as consonants and vowels are extracted from a complex acoustic speech signal. The superior temporal gyrus (STG) participates in high-order auditory processing of speech, but how it encodes phonetic information is poorly understood. We used high-density direct cortical surface recordings in humans while they listened to natural, continuous speech to reveal the STG representation of the entire English phonetic inventory. At single electrodes, we found response selectivity to distinct phonetic features. Encoding of acoustic properties was mediated by a distributed population response. Phonetic features could be directly related to tuning for spectrotemporal acoustic cues, some of which were encoded in a nonlinear fashion or by integration of multiple cues. These findings demonstrate the acoustic-phonetic representation of speech in human STG.},
author = {Mesgarani, N. and Cheung, C. and Johnson, K. and Chang, E. F.},
doi = {10.1126/science.1245994},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mesgarani et al. - 2014 - Phonetic Feature Encoding in Human Superior Temporal Gyrus.pdf:pdf},
isbn = {0036-8075},
issn = {0036-8075},
journal = {Science},
mendeley-groups = {First Exam Citations/Ch2},
pmid = {24482117},
title = {{Phonetic Feature Encoding in Human Superior Temporal Gyrus}},
year = {2014}
}
@article{Wijayasiri2017,
abstract = {The purpose of this study was to establish whether functional near-infrared spectroscopy (fNIRS), an emerging brain-imaging technique based on optical principles, is suitable for studying the brain activity that underlies effortful listening. In an event-related fNIRS experiment, normally-hearing adults listened to sentences that were either clear or degraded (noise vocoded). These sentences were presented simultaneously with a non-speech distractor, and on each trial participants were instructed to attend either to the speech or to the distractor. The primary region of interest for the fNIRS measurements was the left inferior frontal gyrus (LIFG), a cortical region involved in higher-order language processing. The fNIRS results confirmed findings previously reported in the functional magnetic resonance imaging (fMRI) literature. Firstly, the LIFG exhibited an elevated response to degraded versus clear speech, but only when attention was directed towards the speech. This attention-dependent increase in frontal brain activation may be a neural marker for effortful listening. Secondly, during attentive listening to degraded speech, the haemodynamic response peaked significantly later in the LIFG than in superior temporal cortex, possibly reflecting the engagement of working memory to help reconstruct the meaning of degraded sentences. The homologous region in the right hemisphere may play an equivalent role to the LIFG in some left-handed individuals. In conclusion, fNIRS holds promise as a flexible tool to examine the neural signature of effortful listening.},
author = {Wijayasiri, Pramudi and Hartley, Douglas E.H. and Wiggins, Ian M.},
doi = {10.1016/j.heares.2017.05.010},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wijayasiri, Hartley, Wiggins - 2017 - Brain activity underlying the recovery of meaning from degraded speech A functional near-infrared.pdf:pdf},
issn = {18785891},
journal = {Hearing Research},
keywords = {Auditory cortex,Functional near-infrared spectroscopy,Inferior frontal gyrus,Listening effort,Neuroimaging,Noise vocoding,Speech comprehension,fNIRS},
mendeley-groups = {First Exam Citations/Ch4},
title = {{Brain activity underlying the recovery of meaning from degraded speech: A functional near-infrared spectroscopy (fNIRS) study}},
year = {2017}
}
@article{Defenderfer2017,
abstract = {A B S T R A C T Functional near infrared spectroscopy (fNIRS) is a safe, non-invasive, relatively quiet imaging technique that is tolerant of movement artifact making it uniquely ideal for the assessment of hearing mechanisms. Previous research demonstrates the capacity for fNIRS to detect cortical changes to varying speech intelligibility, re-vealing a positive relationship between cortical activation amplitude and speech perception score. In the present study, we use an event-related design to investigate the hemodynamic response in the temporal lobe across different listening conditions. We presented participants with a speech recognition task using sentences in quiet, sentences in noise, and vocoded sentences. Hemodynamic responses were examined across conditions and then compared when speech perception was accurate compared to when speech perception was inaccurate in the context of noisy speech. Repeated measures, two-way ANOVAs revealed that the speech in noise condition (2.8 dB signal-to-noise ratio/SNR) demonstrated significantly greater activation than the easier listening conditions on multiple channels bilaterally. Further analyses comparing correct recognition trials to incorrect recognition trials (during the presentation phase of the trial) revealed that activation was significantly greater during correct trials. Lastly, during the repetition phase of the trial, where participants correctly repeated the sentence, the hemodynamic response demonstrated significantly higher deoxyhemoglobin than oxyhemoglobin, indicating a difference between the effects of perception and production on the cortical response. Using fNIRS, the present study adds meaningful evidence to the body of knowledge that describes the brain/behavior re-lationship related to speech perception.},
author = {Defenderfer, Jessica and Kerr-German, Anastasia and Hedrick, Mark and Buss, Aaron T},
doi = {10.1016/j.neuropsychologia.2017.09.004},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Defenderfer et al. - 2017 - Investigating the role of temporal lobe activation in speech perception accuracy with normal hearing adults.pdf:pdf},
keywords = {Event-related design,Hearing science,Speech perception,fNIRS},
mendeley-groups = {First Exam Citations/Ch4},
title = {{Investigating the role of temporal lobe activation in speech perception accuracy with normal hearing adults: An event-related fNIRS study}},
year = {2017}
}
@article{Tuller2017,
abstract = {The present paper explores the dynamics of speech production and perception in the context of syllabification and categorization. The selective review includes empirical work and dynamical models that account for changes in the perception and production of syllable structure as transitions between attractors in a dynamical system and that highlight the role of instabilities as a mechanism for regulating flexibility and change. Different conceptual approaches to changes in perceptual categorization are reviewed, including a nonlinear dynamic model, a related Bayesian approach, and a hybrid approach. Of particular importance are recent models that incorporate cognitive factors (such as attention, expectation, and memory) and that change slowly or quickly relative to the changing acoustic input. These dynamical models allow phenomena such as self-organization, emergence, and other hallmarks of complex adaptive systems and may also suggest a mechanism linking speech production and perception, providing an alternative description to the internal models often invoked.},
author = {Tuller, Betty and Lancia, Leonardo},
doi = {10.1016/j.wocn.2017.02.001},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tuller, Lancia - 2017 - Speech dynamics Converging evidence from syllabification and categorization.pdf:pdf},
issn = {00954470},
journal = {Journal of Phonetics},
keywords = {Categorization,Dynamical systems,Models of speech perception,Models of speech production,Syllabification},
mendeley-groups = {First Exam Citations/Ch2},
title = {{Speech dynamics: Converging evidence from syllabification and categorization}},
year = {2017}
}
@article{Best2007,
author = {Best, Catherine T. and Tyler, Michael D},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Best, Tyler - 2007 - Nonnative and second-language speech perception Commonalities and complementarities. Language experience in second.pdf:pdf},
mendeley-groups = {First Exam Citations/Ch2},
title = {{Nonnative and second-language speech perception: Commonalities and complementarities. Language experience in second language speech learning}},
volume = {10389},
year = {2007}
}
@article{Kuhl2004,
abstract = {Infants learn language with remarkable speed, but how they do it remains a mystery. New data show that infants use computational strategies to detect the statistical and prosodic patterns in language input, and that this leads to the discovery of phonemes and words. Social interaction with another human being affects speech learning in a way that resembles communicative learning in songbirds. The brain's commitment to the statistical and prosodic patterns that are experienced early in life might help to explain the long-standing puzzle of why infants are better language learners than adults. Successful learning by infants, as well as constraints on that learning, are changing theories of language acquisition.},
author = {Kuhl, Patricia K.},
doi = {10.1038/nrn1533},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuhl - 2004 - Early language acquisition cracking the speech code.pdf:pdf},
isbn = {1471-0048},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
mendeley-groups = {First Exam Citations/Ch3,First Exam Citations,First Exam Citations/Ch2},
number = {11},
pages = {831--843},
pmid = {15496861},
title = {{Early language acquisition: cracking the speech code}},
url = {http://www.nature.com/doifinder/10.1038/nrn1533},
volume = {5},
year = {2004}
}
@article{Byrne2000,
abstract = {We describe procedures and experimental results using speech from diverse source languages to build an ASR system for a single target language. This work is intended to improve ASR in languages for which large amounts of training data are not available. We have developed both knowledge-based and automatic methods to map phonetic units from the source languages to the target language. We employed HMM adaptation techniques and discriminative model combination to combine acoustic models from the individual source languages for recognition of speech in the target language. Experiments are described in which Czech Broadcast News is transcribed using acoustic models trained from small amounts of Czech read speech augmented by English, Spanish, Russian, and Mandarin acoustic models},
author = {Byrne, William and Beyerlein, Peter and Huerta, Juan M and Khudanpur, Sanjeev and Marthi, Bhaskara and Morgan, John and Peterek, Nino and Picone, Joe and Vergyri, Dimitra and Wang, W},
doi = {10.1109/ICASSP.2000.859138},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Byrne et al. - 2000 - Towards language independent acoustic modeling.pdf:pdf},
isbn = {0-7803-6293-4},
issn = {1520-6149},
journal = {Acoustics, Speech, and Signal Processing, 2000. ICASSP'00. Proceedings. 2000 IEEE International Conference on},
mendeley-groups = {First Exam Citations/Ch2},
number = {1},
pages = {II1029----II1032},
title = {{Towards language independent acoustic modeling}},
volume = {2},
year = {2000}
}
@article{Strange2007,
abstract = {Cross-language perception studies report influences of speech style and consonantal context on perceived similarity and discrimination of non-native vowels by inexperienced and experienced listeners. Detailed acoustic comparisons of distributions of vowels produced by native speakers of North German (NG), Parisian French (PF) and New York English (AE) in citation (di)syllables and in sentences (surrounded by labial and alveolar stops) are reported here. Results of within- and cross-language discriminant analyses reveal striking dissimilarities across languages in the spectral/temporal variation of coarticulated vowels. As expected, vocalic duration was most important in differentiating NG vowels; it did not contribute to PF vowel classification. Spectrally, NG long vowels showed little coarticulatory change, but back/low short vowels were fronted/raised in alveolar context. PF vowels showed greater coarticulatory effects overall; back and front rounded vowels were fronted, low and mid-low vowels were raised in both sentence contexts. AE mid to high back vowels were extremely fronted in alveolar contexts, with little change in mid-low and low long vowels. Cross-language discriminant analyses revealed varying patterns of spectral (dis)similarity across speech styles and consonantal contexts that could, in part, account for AE listeners' perception of German and French front rounded vowels, and "similar" mid-high to mid-low vowels.},
author = {Strange, Winifred and Weber, Andrea and Levy, Erika S. and Shafiro, Valeriy and Hisagi, Miwako and Nishi, Kanae},
doi = {10.1121/1.2749716},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Strange et al. - 2007 - Acoustic variability within and across German, French, and American English vowels Phonetic context effects.pdf:pdf},
isbn = {0001-4966},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
mendeley-groups = {First Exam Citations/Ch2},
number = {2},
pages = {1111--1129},
pmid = {17672658},
title = {{Acoustic variability within and across German, French, and American English vowels: Phonetic context effects}},
url = {http://asa.scitation.org/doi/10.1121/1.2749716},
volume = {122},
year = {2007}
}
@article{Graves2013,
abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7{\%} on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
archivePrefix = {arXiv},
arxivId = {arXiv:1303.5778v1},
author = {Graves, A and Mohamed, A.-R. and Hinton, G},
doi = {10.1109/ICASSP.2013.6638947},
eprint = {arXiv:1303.5778v1},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Graves, Mohamed, Hinton - 2013 - Speech recognition with deep recurrent neural networks.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {1520-6149},
journal = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
mendeley-groups = {First Exam Citations/Ch5},
number = {6},
pages = {6645--6649},
pmid = {27295638},
title = {{Speech recognition with deep recurrent neural networks}},
url = {files/543/Graves{\_}et{\_}al-2013-Speech{\_}recognition{\_}with{\_}deep{\_}recurrent{\_}neural{\_}networks.pdf},
year = {2013}
}
@article{Horiguchi2003,
abstract = {The authors describe the neuropsychological development of a 10-year-old boy with Noonan syndrome. The subject's IQ showed normal intelligence, although there was a discrepancy between verbal and performance IQs. Visual perception was delayed, with clumsiness and inattention affecting his performance. Both visual perception and hyperactivity improved with the subject's general development, but his inattention seemed to increase after the age of 9. The behavioral features and cognitive profile of our case resembled those of attention-deficit/hyperactivity disorder. We recommend that clinicians should evaluate cautiously the specific profile of cognitive development in Noonan syndrome with particular focus on controlling the patient's inattention.},
author = {Horiguchi, Toshihiro and Takeshita, Kazuhide},
doi = {10.1016/S0},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Horiguchi, Takeshita - 2003 - Neuropsychological developmental change in a case with Noonan syndrome longitudinal assessment.pdf:pdf},
isbn = {1514934809},
issn = {0387-7604},
journal = {Brain {\&} development},
keywords = {algometry,experimental pain,gender differences,pressure pain threshold},
mendeley-groups = {First Exam Citations/Ch3},
number = {4},
pages = {291--293},
pmid = {12767464},
title = {{Neuropsychological developmental change in a case with Noonan syndrome: longitudinal assessment.}},
volume = {25},
year = {2003}
}
@misc{Best1995,
author = {Best, Catherine T},
booktitle = {Speech Perception and Linguistic Experience: Issues in Cross-Language Research},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Best - 1995 - A Direct Realist View of Cross-Language Speech Perception.pdf:pdf},
isbn = {9780912752365},
issn = {091275236X},
mendeley-groups = {First Exam Citations/Ch4},
pages = {171--204},
title = {{A Direct Realist View of Cross-Language Speech Perception}},
year = {1995}
}
@article{Moses2016,
abstract = {OBJECTIVE The superior temporal gyrus (STG) and neighboring brain regions play a key role in human language processing. Previous studies have attempted to reconstruct speech information from brain activity in the STG, but few of them incorporate the probabilistic framework and engineering methodology used in modern speech recognition systems. In this work, we describe the initial efforts toward the design of a neural speech recognition (NSR) system that performs continuous phoneme recognition on English stimuli with arbitrary vocabulary sizes using the high gamma band power of local field potentials in the STG and neighboring cortical areas obtained via electrocorticography. APPROACH The system implements a Viterbi decoder that incorporates phoneme likelihood estimates from a linear discriminant analysis model and transition probabilities from an n-gram phonemic language model. Grid searches were used in an attempt to determine optimal parameterizations of the feature vectors and Viterbi decoder. MAIN RESULTS The performance of the system was significantly improved by using spatiotemporal representations of the neural activity (as opposed to purely spatial representations) and by including language modeling and Viterbi decoding in the NSR system. SIGNIFICANCE These results emphasize the importance of modeling the temporal dynamics of neural responses when analyzing their variations with respect to varying stimuli and demonstrate that speech recognition techniques can be successfully leveraged when decoding speech from neural signals. Guided by the results detailed in this work, further development of the NSR system could have applications in the fields of automatic speech recognition and neural prosthetics.},
author = {Moses, David A and Mesgarani, Nima and Leonard, Matthew K and Chang, Edward F},
doi = {10.1088/1741-2560/13/5/056004},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moses et al. - 2016 - Neural speech recognition continuous phoneme decoding using spatiotemporal representations of human cortical activ.pdf:pdf},
isbn = {1741-2552},
issn = {1741-2560},
journal = {Journal of Neural Engineering},
keywords = {43.71.An,43.71.Qr,43.71.Sy,43.71Es,43.72.Ne,87.19.L,87.85.D,87.85.E,87.85.Wc,electrocorticography,high gamma,human auditory cortex,neural speech recognition,speech perception,superior temporal gyrus},
mendeley-groups = {First Exam Citations/Ch3},
number = {5},
pages = {056004},
pmid = {27484713},
publisher = {IOP Publishing},
title = {{Neural speech recognition: continuous phoneme decoding using spatiotemporal representations of human cortical activity}},
url = {http://stacks.iop.org/1741-2552/13/i=5/a=056004?key=crossref.ac3fcc4576d40ed0daf5b1e6181cd472},
volume = {13},
year = {2016}
}
@article{Dmochowski2017,
abstract = {In neuroscience, stimulus-response relationships have traditionally been analyzed using either encoding or decoding models. Here we propose a hybrid approach that decomposes neural activity into multiple components, each representing a portion of the stimulus. The technique is implemented via canonical correlation analysis (CCA) by temporally filtering the stimulus (encoding) and spatially filtering the neural responses (decoding) such that the resulting components are maximally correlated. In contrast to existing methods, this approach recovers multiple correlated stimulus-response pairs, and thus affords a richer, multidimensional analysis of neural representations. We first validated the technique's ability to recover multiple stimulus-driven components using electroencephalographic (EEG) data simulated with a finite element model of the head. We then applied the technique to real EEG responses to auditory and audiovisual narratives experienced identically across subjects, as well as uniquely experienced video game play. During narratives, both auditory and visual stimulus-response correlations (SRC) were modulated by attention and tracked inter-subject correlations. During video game play, SRC varied with game difficulty and the presence of a dual task. Interestingly, the strongest component extracted for visual and auditory features of film clips had nearly identical spatial distributions, suggesting that the predominant encephalographic response to naturalistic stimuli is supramodal. The diversity of these findings demonstrates the utility of measuring multidimensional SRC via hybrid encoding-decoding.},
author = {Dmochowski, Jacek P. and Ki, Jason J. and DeGuzman, Paul and Sajda, Paul and Parra, Lucas C.},
doi = {10.1016/j.neuroimage.2017.05.037},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dmochowski et al. - 2017 - Extracting multidimensional stimulus-response correlations using hybrid encoding-decoding of neural activity.pdf:pdf},
issn = {10959572},
journal = {NeuroImage},
mendeley-groups = {First Exam Citations/Ch5,First Exam Citations},
number = {May},
pages = {1--13},
publisher = {Elsevier},
title = {{Extracting multidimensional stimulus-response correlations using hybrid encoding-decoding of neural activity}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2017.05.037},
year = {2017}
}
@article{Brodbeck2017,
author = {Brodbeck, Christian and Presacco, Alessandro and Simon, Jonathan Z},
doi = {10.1101/182881},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brodbeck, Presacco, Simon - 2017 - Neural source dynamics of brain responses to continuous stimuli speech processing from acoustics to.pdf:pdf},
mendeley-groups = {First Exam Citations/Ch3,First Exam Citations/Ch4,First Exam Citations},
pages = {1--27},
title = {{Neural source dynamics of brain responses to continuous stimuli : speech processing from acoustics to comprehension}},
year = {2017}
}
@article{Kosem2017,
author = {K{\"{o}}sem, Anne and Bosker, Hans Rutger and Takashima, Atsuko and Meyer, Antje and Jensen, Ole and Hagoort, Peter},
doi = {10.1101/175000},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/K{\"{o}}sem et al. - 2017 - Neural entrainment determines the words we hear.pdf:pdf},
isbn = {2012203566},
mendeley-groups = {First Exam Citations/Ch3,First Exam Citations/Ch4,First Exam Citations},
title = {{Neural entrainment determines the words we hear}},
year = {2017}
}
@article{Horton2014,
abstract = {Objective. Recent studies have shown that auditory cortex better encodes the envelope of attended speech than that of unattended speech during multi-speaker ('cocktail party') situations. We investigated whether these differences were sufficiently robust within single-trial electroencephalographic (EEG) data to accurately determine where subjects attended. Additionally, we compared this measure to other established EEG markers of attention. Approach. High-resolution EEG was recorded while subjects engaged in a two-speaker 'cocktail party' task. Cortical responses to speech envelopes were extracted by cross-correlating the envelopes with each EEG channel. We also measured steady-state responses (elicited via high-frequency amplitude modulation of the speech) and alpha-band power, both of which have been sensitive to attention in previous studies. Using linear classifiers, we then examined how well each of these features could be used to predict the subjects' side of attention at various epoch lengths. Main results. We found that the attended speaker could be determined reliably from the envelope responses calculated from short periods of EEG, with accuracy improving as a function of sample length. Furthermore, envelope responses were far better indicators of attention than changes in either alpha power or steady-state responses. Significance. These results suggest that envelope-related signals recorded in EEG data can be used to form robust auditory BCI's that do not require artificial manipulation (e.g., amplitude modulation) of stimuli to function.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Horton, Cort and Srinivasan, Ramesh and D'Zmura, Michael},
doi = {10.1088/1741-2560/11/4/046015},
eprint = {NIHMS150003},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Horton, Srinivasan, D'Zmura - 2014 - Envelope responses in single-trial EEG indicate attended speaker in a cocktail party'.pdf:pdf},
isbn = {2156623929},
issn = {1741-2560},
journal = {Journal of Neural Engineering},
keywords = {alpha lateralization,brain,computer interfaces,selective attention,speech envelopes,steady-state responses},
mendeley-groups = {First Exam Citations/Ch4,First Exam Citations},
month = {aug},
number = {4},
pages = {046015},
pmid = {24963838},
publisher = {IOP Publishing},
title = {{Envelope responses in single-trial EEG indicate attended speaker in a cocktail party'}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24963838 http://stacks.iop.org/1741-2552/11/i=4/a=046015?key=crossref.e543a3a482c9db8ce42ab850fec18319},
volume = {11},
year = {2014}
}
@article{Kayser2015,
abstract = {UNLABELLED: The entrainment of slow rhythmic auditory cortical activity to the temporal regularities in speech is considered to be a central mechanism underlying auditory perception. Previous work has shown that entrainment is reduced when the quality of the acoustic input is degraded, but has also linked rhythmic activity at similar time scales to the encoding of temporal expectations. To understand these bottom-up and top-down contributions to rhythmic entrainment, we manipulated the temporal predictive structure of speech by parametrically altering the distribution of pauses between syllables or words, thereby rendering the local speech rate irregular while preserving intelligibility and the envelope fluctuations of the acoustic signal. Recording EEG activity in human participants, we found that this manipulation did not alter neural processes reflecting the encoding of individual sound transients, such as evoked potentials. However, the manipulation significantly reduced the fidelity of auditory delta (but not theta) band entrainment to the speech envelope. It also reduced left frontal alpha power and this alpha reduction was predictive of the reduced delta entrainment across participants. Our results show that rhythmic auditory entrainment in delta and theta bands reflect functionally distinct processes. Furthermore, they reveal that delta entrainment is under top-down control and likely reflects prefrontal processes that are sensitive to acoustical regularities rather than the bottom-up encoding of acoustic features.$\backslash$n$\backslash$nSIGNIFICANCE STATEMENT: The entrainment of rhythmic auditory cortical activity to the speech envelope is considered to be critical for hearing. Previous work has proposed divergent views in which entrainment reflects either early evoked responses related to sound encoding or high-level processes related to expectation or cognitive selection. Using a manipulation of speech rate, we dissociated auditory entrainment at different time scales. Specifically, our results suggest that delta entrainment is controlled by frontal alpha mechanisms and thus support the notion that rhythmic auditory cortical entrainment is shaped by top-down mechanisms.},
author = {Kayser, Stephanie J and Ince, Robin A A and Gross, Joachim and Kayser, Christoph},
doi = {10.1523/JNEUROSCI.2243-15.2015},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kayser et al. - 2015 - Irregular Speech Rate Dissociates Auditory Cortical Entrainment, Evoked Responses, and Frontal Alpha.pdf:pdf},
isbn = {0270-6474},
issn = {1529-2401},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
keywords = {auditory cortex,auditory cortical activity to,considered to be critical,delta band,encoding or,for hearing,information theory,previous,reflects either early evoked,responses related to sound,rhythmic entrainment,significance statement,speech,the entrainment of rhythmic,the speech envelope is,views in which entrainment,work has proposed divergent},
mendeley-groups = {First Exam - To Read,First Exam Citations/Ch4},
number = {44},
pages = {14691--701},
pmid = {26538641},
title = {{Irregular Speech Rate Dissociates Auditory Cortical Entrainment, Evoked Responses, and Frontal Alpha.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4635123{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {35},
year = {2015}
}
@article{Morillon2015,
abstract = {Neuronal oscillations are comprised of rhythmic fluctuations of excitability that are synchronized in ensembles of neurons and thus function as temporal filters that dynamically organize sensory processing. When perception relies on anticipatory mechanisms, ongoing oscillations also provide a neurophysiological substrate for temporal prediction. In this article, we review evidence for this account with a focus on auditory perception. We argue that such "oscillatory temporal predictions" can selectively amplify neuronal sensitivity to inputs that occur in a predicted, task-relevant rhythm and optimize temporal selection. We elaborate this argument for a prototypic example, speech processing, where information is present at multiple time scales, with delta, theta, and low-gamma oscillations being specifically and simultaneously engaged, enabling multiplexing. We then consider the origin of temporal predictions, specifically the idea that the motor system is involved in the generation of such prior information. Finally, we place temporal predictions in the general context of internal models, discussing how they interact with feature-based or spatial predictions. We propose that complementary predictions interact synergistically according to a dominance hierarchy, shaping perception in the form of a multidimensional filter mechanism.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Morillon, Benjamin and Schroeder, Charles E.},
doi = {10.1111/nyas.12629},
eprint = {15334406},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Morillon, Schroeder - 2015 - Neuronal oscillations as a mechanistic substrate of auditory temporal prediction.pdf:pdf},
isbn = {0000000000000},
issn = {17496632},
journal = {Annals of the New York Academy of Sciences},
keywords = {Active sensing,Expectation,Neurophysiology,Perception,Sensorimotor},
mendeley-groups = {First Exam Citations/Ch3,First Exam Citations},
number = {1},
pages = {26--31},
pmid = {25773613},
title = {{Neuronal oscillations as a mechanistic substrate of auditory temporal prediction}},
volume = {1337},
year = {2015}
}
@inproceedings{Biesmans2015,
abstract = {Recent research has shown that it is possible to detect which of two simultaneous speakers a person is attending to, using brain recordings and the temporal envelope of the separate speech signals. However, a wide range of possible methods for extracting this speech envelope exists. This paper assesses the effect of different envelope extraction methods with varying degrees of auditory modelling on the performance of auditory attention detection (AAD), and more specifically on the detection accuracy. It is found that subband envelope extraction with proper power-law compression yields best performance, and that the use of several more detailed auditory models does not yield a further improvement in performance.},
author = {Biesmans, Wouter and Vanthornhout, Jonas and Wouters, Jan and Moonen, Marc and Francart, Tom and Bertrand, Alexander},
booktitle = {Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
doi = {10.1109/EMBC.2015.7319552},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Biesmans et al. - 2015 - Comparison of speech envelope extraction methods for EEG-based auditory attention detection in a cocktail party.pdf:pdf},
isbn = {9781424492718},
issn = {1557170X},
mendeley-groups = {First Exam Citations/Ch5,First Exam Citations},
pages = {5155--5158},
pmid = {27244743},
title = {{Comparison of speech envelope extraction methods for EEG-based auditory attention detection in a cocktail party scenario}},
volume = {2015-Novem},
year = {2015}
}
@article{Dmochowski2016,
abstract = {In neuroscience, stimulus-response relationships have traditionally been analyzed using either encoding or decoding models. Here we combined both techniques by decomposing neural activity into multiple components, each representing a portion of the stimulus. We tested this hybrid approach on encephalographic responses to auditory and audiovisual narratives identically experienced across subjects, as well as uniquely experienced video game play. The highest stimulus-response correlations (SRC) were detected for dynamic visual features. During narratives both auditory and visual SRC were modulated by attention and tracked correlations between subjects. During video game play, SRC was modulated by task difficulty and attentional state. Importantly, the strongest component extracted for visual and auditory features had nearly identical spatial distributions, suggesting that the predominant encephalographic response to naturalistic stimuli is supramodal. The variety of novel findings demonstrates the utility of measuring multidimensional stimulus-response correlations.},
author = {Dmochowski, Jacek P and Ki, Jason and Deguzman, Paul and Sajda, Paul and Parra, Lucas C},
doi = {10.1101/077230},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dmochowski et al. - 2016 - Multidimensional stimulus-response correlation reveals supramodal neural responses to naturalistic stimuli.pdf:pdf},
mendeley-groups = {First Exam Citations/Ch5,First Exam Citations/Ch4,First Exam Citations},
pages = {1--18},
title = {{Multidimensional stimulus-response correlation reveals supramodal neural responses to naturalistic stimuli}},
year = {2016}
}
@article{DiLiberto2016,
abstract = {The International Symposium on Hearing is a prestigious, triennial gathering where world-class scientists present and discuss the most recent advances in the field of human and animal hearing research. The 2015 edition will particularly focus on integrative approaches linking physiological, psychophysical and cognitive aspects of normal and impaired hearing. Like previous editions, the proceedings will contain about 50 chapters ranging from basic to applied research, and of interest to neuroscientists, psychologists, audiologists, engineers, otolaryngologists, and artificial intelligence researchers.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Di Liberto}, Giovanni M. and Lalor, Edmund C.},
doi = {10.1007/978-3-319-25474-6_35},
eprint = {arXiv:1011.1669v3},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Di Liberto, Lalor - 2016 - Isolating neural indices of continuous speech processing at the phonetic level.pdf:pdf},
isbn = {978-3-319-25472-2},
issn = {22148019},
journal = {Advances in Experimental Medicine and Biology},
keywords = {EEG,Hierarchical,Intelligibility,Natural speech,Noise vocoding,Priming},
mendeley-groups = {First Exam Citations/Ch3,First Exam Citations},
pages = {337--345},
pmid = {25246403},
title = {{Isolating neural indices of continuous speech processing at the phonetic level}},
volume = {894},
year = {2016}
}
@article{Crosse2015,
abstract = {UNLABELLED: Congruent audiovisual speech enhances our ability to comprehend a speaker, even in noise-free conditions. When incongruent auditory and visual information is presented concurrently, it can hinder a listener's perception and even cause him or her to perceive information that was not presented in either modality. Efforts to investigate the neural basis of these effects have often focused on the special case of discrete audiovisual syllables that are spatially and temporally congruent, with less work done on the case of natural, continuous speech. Recent electrophysiological studies have demonstrated that cortical response measures to continuous auditory speech can be easily obtained using multivariate analysis methods. Here, we apply such methods to the case of audiovisual speech and, importantly, present a novel framework for indexing multisensory integration in the context of continuous speech. Specifically, we examine how the temporal and contextual congruency of ongoing audiovisual speech affects the cortical encoding of the speech envelope in humans using electroencephalography. We demonstrate that the cortical representation of the speech envelope is enhanced by the presentation of congruent audiovisual speech in noise-free conditions. Furthermore, we show that this is likely attributable to the contribution of neural generators that are not particularly active during unimodal stimulation and that it is most prominent at the temporal scale corresponding to syllabic rate (2-6 Hz). Finally, our data suggest that neural entrainment to the speech envelope is inhibited when the auditory and visual streams are incongruent both temporally and contextually.$\backslash$n$\backslash$nSIGNIFICANCE STATEMENT: Seeing a speaker's face as he or she talks can greatly help in understanding what the speaker is saying. This is because the speaker's facial movements relay information about what the speaker is saying, but also, importantly, when the speaker is saying it. Studying how the brain uses this timing relationship to combine information from continuous auditory and visual speech has traditionally been methodologically difficult. Here we introduce a new approach for doing this using relatively inexpensive and noninvasive scalp recordings. Specifically, we show that the brain's representation of auditory speech is enhanced when the accompanying visual speech signal shares the same timing. Furthermore, we show that this enhancement is most pronounced at a time scale that corresponds to mean syllable length.},
author = {Crosse, M. J. and Butler, J. S. and Lalor, E. C.},
doi = {10.1523/JNEUROSCI.1829-15.2015},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Crosse, Butler, Lalor - 2015 - Congruent Visual Speech Enhances Cortical Entrainment to Continuous Auditory Speech in Noise-Free Conditi.pdf:pdf},
isbn = {0270-6474},
issn = {0270-6474},
journal = {Journal of Neuroscience},
mendeley-groups = {First Exam Citations/Ch4,First Exam Citations},
number = {42},
pages = {14195--14204},
pmid = {26490860},
title = {{Congruent Visual Speech Enhances Cortical Entrainment to Continuous Auditory Speech in Noise-Free Conditions}},
url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.1829-15.2015},
volume = {35},
year = {2015}
}
@article{Doelling2014,
abstract = {A growing body of research suggests that intrinsic neuronal slow ({\textless}. 10. Hz) oscillations in auditory cortex appear to track incoming speech and other spectro-temporally complex auditory signals. Within this framework, several recent studies have identified critical-band temporal envelopes as the specific acoustic feature being reflected by the phase of these oscillations. However, how this alignment between speech acoustics and neural oscillations might underpin intelligibility is unclear. Here we test the hypothesis that the 'sharpness' of temporal fluctuations in the critical band envelope acts as a temporal cue to speech syllabic rate, driving delta-theta rhythms to track the stimulus and facilitate intelligibility. We interpret our findings as evidence that sharp events in the stimulus cause cortical rhythms to re-align and parse the stimulus into syllable-sized chunks for further decoding. Using magnetoencephalographic recordings, we show that by removing temporal fluctuations that occur at the syllabic rate, envelope-tracking activity is reduced. By artificially reinstating these temporal fluctuations, envelope-tracking activity is regained. These changes in tracking correlate with intelligibility of the stimulus. Together, the results suggest that the sharpness of fluctuations in the stimulus, as reflected in the cochlear output, drive oscillatory activity to track and entrain to the stimulus, at its syllabic rate. This process likely facilitates parsing of the stimulus into meaningful chunks appropriate for subsequent decoding, enhancing perception and intelligibility. {\textcopyright} 2013 Elsevier Inc.},
author = {Doelling, Keith B. and Arnal, Luc H. and Ghitza, Oded and Poeppel, David},
doi = {10.1016/j.neuroimage.2013.06.035},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Doelling et al. - 2014 - Acoustic landmarks drive delta-theta oscillations to enable speech comprehension by facilitating perceptual par.pdf:pdf},
isbn = {1095-9572 (Electronic)$\backslash$r1053-8119 (Linking)},
issn = {10959572},
journal = {NeuroImage},
keywords = {Acoustic edge,Auditory cortex,MEG,Neural oscillation,Perceptual parsing,Speech},
mendeley-groups = {First Exam Citations/Ch3,First Exam Citations},
pages = {761--768},
pmid = {23791839},
title = {{Acoustic landmarks drive delta-theta oscillations to enable speech comprehension by facilitating perceptual parsing}},
volume = {85},
year = {2014}
}
@article{OSullivan2013,
abstract = {Traditionally, the use of electroencephalography (EEG) to study the neural processing of natural stimuli in humans has been hampered by the need to repeatedly present discrete stimuli. Progress has been made recently by the realization that cortical population activity tracks the amplitude envelope of speech stimuli. This has led to studies using linear regression methods which allow the presentation of continuous speech. One such method, known as stimulus reconstruction, has so far only been utilized in multi-electrode cortical surface recordings and magnetoencephalography (MEG). Here, in two studies, we show that such an approach is also possible with EEG, despite the poorer signal-to-noise ratio of the data. In the first study, we show that it is possible to decode attention in a naturalistic cocktail party scenario on a single trial (60 s) basis. In the second, we show that the representation of the envelope of auditory speech in the cortex is more robust when accompanied by visual speech. The sensitivity of this inexpensive, widely-accessible technology for the online monitoring of natural stimuli has implications for the design of future studies of the cocktail party problem and for the implementation of EEG-based brain-computer interfaces.},
author = {O'Sullivan, James A. and Crosse, Michael J. and Power, Alan J. and Lalor, Edmund C.},
doi = {10.1109/EMBC.2013.6610122},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Sullivan et al. - 2013 - The effects of attention and visual input on the representation of natural speech in EEG.pdf:pdf},
isbn = {9781457702167},
issn = {1557170X},
journal = {Conference proceedings : ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual Conference},
mendeley-groups = {First Exam Citations/Ch4,First Exam Citations},
pages = {2800--2803},
pmid = {24110309},
title = {{The effects of attention and visual input on the representation of natural speech in EEG}},
volume = {2013},
year = {2013}
}
@article{OSullivan2015,
abstract = {How humans solve the cocktail party problem remains unknown. However, progress has been made recently thanks to the realization that cortical activity tracks the amplitude envelope of speech. This has led to the development of regression methods for studying the neurophysiology of continuous speech. One such method, known as stimulus-reconstruction, has been successfully utilized with cortical surface recordings and magnetoencephalography (MEG). However, the former is invasive and gives a relatively restricted view of processing along the auditory hierarchy, whereas the latter is expensive and rare. Thus it would be extremely useful for research in many populations if stimulus-reconstruction was effective using electroencephalography (EEG), a widely available and inexpensive technology. Here we show that single-trial (60 s) unaveraged EEG data can be decoded to determine attentional selection in a naturalistic multispeaker environment. Furthermore, we show a significant correlation between our EEG-based measure of attention and performance on a high-level attention task. In addition, by attempting to decode attention at individual latencies, we identify neural processing at 200 ms as being critical for solving the cocktail party problem. These findings open up new avenues for studying the ongoing dynamics of cognition using EEG and for developing effective and natural brain-computer interfaces.},
author = {O'Sullivan, James A. and Power, Alan J. and Mesgarani, Nima and Rajaram, Siddharth and Foxe, John J. and Shinn-Cunningham, Barbara G. and Slaney, Malcolm and Shamma, Shihab A. and Lalor, Edmund C.},
doi = {10.1093/cercor/bht355},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Sullivan et al. - 2015 - Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG.pdf:pdf},
issn = {14602199},
journal = {Cerebral Cortex},
keywords = {BCI,EEG,attention,cocktail party,speech,stimulus-reconstruction},
mendeley-groups = {Alice+Pieman,First Exam Citations/Ch4,First Exam Citations},
number = {7},
pages = {1697--1706},
pmid = {24429136},
title = {{Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG}},
volume = {25},
year = {2015}
}
@article{Lalor2010,
abstract = {The human auditory system has evolved to efficiently process individual streams of speech. However, obtaining temporally detailed responses to distinct continuous natural speech streams has hitherto been impracticable using standard neurophysiological techniques. Here a method is described which provides for the estimation of a temporally precise electrophysiological response to uninterrupted natural speech. We have termed this response AESPA (Auditory Evoked Spread Spectrum Analysis) and it represents an estimate of the impulse response of the auditory system. It is obtained by assuming that the recorded electrophysiological function represents a convolution of the amplitude envelope of a continuous speech stream with the to-be-estimated impulse response. We present examples of these responses using both scalp and intracranially recorded human EEG, which were obtained while subjects listened to a binaurally presented recording of a male speaker reading naturally from a classic work of fiction. This method expands the arsenal of stimulation types that can now be effectively used to derive auditory evoked responses and allows for the use of considerably more ecologically valid stimulation parameters. Some implications for future research efforts are presented.},
author = {Lalor, Edmund C. and Foxe, John J.},
doi = {10.1111/j.1460-9568.2009.07055.x},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lalor, Foxe - 2010 - Neural responses to uninterrupted natural speech can be extracted with precise temporal resolution.pdf:pdf},
isbn = {1460-9568 (Electronic)$\backslash$n0953-816X (Linking)},
issn = {0953816X},
journal = {European Journal of Neuroscience},
keywords = {Auditory evoked potential,EEG,Impulse response,Speech},
mendeley-groups = {First Exam Citations/Ch4,First Exam Citations},
number = {1},
pages = {189--193},
pmid = {20092565},
title = {{Neural responses to uninterrupted natural speech can be extracted with precise temporal resolution}},
volume = {31},
year = {2010}
}
@article{Mesgarani2012,
abstract = {Humans possess a remarkable ability to attend to a single speaker's voice in a multi-talker background. How the auditory system manages to extract intelligible speech under such acoustically complex and adverse listening conditions is not known, and, indeed, it is not clear how attended speech is internally represented. Here, using multi-electrode surface recordings from the cortex of subjects engaged in a listening task with two simultaneous speakers, we demonstrate that population responses in non-primary human auditory cortex encode critical features of attended speech: speech spectrograms reconstructed based on cortical responses to the mixture of speakers reveal the salient spectral and temporal features of the attended speaker, as if subjects were listening to that speaker alone. A simple classifier trained solely on examples of single speakers can decode both attended words and speaker identity. We find that task performance is well predicted by a rapid increase in attention-modulated neural selectivity across both single-electrode and population-level cortical responses. These findings demonstrate that the cortical representation of speech does not merely reflect the external acoustic environment, but instead gives rise to the perceptual aspects relevant for the listener's intended goal.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Mesgarani, Nima and Chang, Edward F.},
doi = {10.1038/nature11020},
eprint = {NIHMS150003},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mesgarani, Chang - 2012 - Selective cortical representation of attended speaker in multi-talker speech perception.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
mendeley-groups = {First Exam Citations/Ch3,First Exam Citations},
number = {7397},
pages = {233--236},
pmid = {22522927},
title = {{Selective cortical representation of attended speaker in multi-talker speech perception}},
url = {http://www.nature.com/doifinder/10.1038/nature11020},
volume = {485},
year = {2012}
}
@article{Luo2010,
abstract = {Integrating information across sensory domains to construct a unified representation of multi-sensory signals is a fundamental characteristic of perception in ecological contexts. One provocative hypothesis deriving from neurophysiology suggests that there exists early and direct cross-modal phase modulation. We provide evidence, based on magnetoencephalography (MEG) recordings from participants viewing audiovisual movies, that low-frequency neuronal information lies at the basis of the synergistic coordination of information across auditory and visual streams. In particular, the phase of the 2-7 Hz delta and theta band responses carries robust (in single trials) and usable information (for parsing the temporal structure) about stimulus dynamics in both sensory modalities concurrently. These experiments are the first to show in humans that a particular cortical mechanism, delta-theta phase modulation across early sensory areas, plays an important "active" role in continuously tracking naturalistic audio-visual streams, carrying dynamic multi-sensory information, and reflecting cross-sensory interaction in real time.},
author = {Luo, Huan and Liu, Zuxiang and Poeppel, David},
doi = {10.1371/journal.pbio.1000445},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo, Liu, Poeppel - 2010 - Auditory cortex tracks both auditory and visual stimulus dynamics using low-frequency neuronal phase modulati.pdf:pdf},
isbn = {1545-7885 (Electronic)$\backslash$n1544-9173 (Linking)},
issn = {15449173},
journal = {PLoS Biology},
mendeley-groups = {Alice+Pieman,First Exam Citations/Ch4,First Exam Citations},
number = {8},
pages = {25--26},
pmid = {20711473},
title = {{Auditory cortex tracks both auditory and visual stimulus dynamics using low-frequency neuronal phase modulation}},
volume = {8},
year = {2010}
}
@article{Luo2007,
abstract = {How natural speech is represented in the auditory cortex constitutes a major challenge for cognitive neuroscience. Although many single-unit and neuroimaging studies have yielded valuable insights about the processing of speech and matched complex sounds, the mechanisms underlying the analysis of speech dynamics in human auditory cortex remain largely unknown. Here, we show that the phase pattern of theta band (4-8 Hz) responses recorded from human auditory cortex with magnetoencephalography (MEG) reliably tracks and discriminates spoken sentences and that this discrimination ability is correlated with speech intelligibility. The findings suggest that an ???200 ms temporal window (period of theta oscillation) segments the incoming speech signal, resetting and sliding to track speech dynamics. This hypothesized mechanism for cortical speech analysis is based on the stimulus-induced modulation of inherent cortical rhythms and provides further evidence implicating the syllable as a computational primitive for the representation of spoken language. ?? 2007 Elsevier Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Luo, Huan and Poeppel, David},
doi = {10.1016/j.neuron.2007.06.004},
eprint = {NIHMS150003},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo, Poeppel - 2007 - Phase Patterns of Neuronal Responses Reliably Discriminate Speech in Human Auditory Cortex.pdf:pdf},
isbn = {2122633255},
issn = {08966273},
journal = {Neuron},
keywords = {SYSNEURO},
mendeley-groups = {First Exam Citations/Ch4,First Exam Citations},
number = {6},
pages = {1001--1010},
pmid = {17582338},
title = {{Phase Patterns of Neuronal Responses Reliably Discriminate Speech in Human Auditory Cortex}},
volume = {54},
year = {2007}
}
@article{Howard2010,
abstract = {Speech stimuli give rise to neural activity in the listener that can be observed as waveforms using magnetoencephalography. Although waveforms vary greatly from trial to trial due to activity unrelated to the stimulus, it has been demonstrated that spoken sentences can be discriminated based on theta-band (3-7 Hz) phase patterns in single-trial response waveforms. Furthermore, manipulations of the speech signal envelope and fine structure that reduced intelligibility were found to produce correlated reductions in discrimination performance, suggesting a relationship between theta-band phase patterns and speech comprehension. This study investigates the nature of this relationship, hypothesizing that theta-band phase patterns primarily reflect cortical processing of low-frequency ({\textless}40 Hz) modulations present in the acoustic signal and required for intelligibility, rather than processing exclusively related to comprehension (e.g., lexical, syntactic, semantic). Using stimuli that are quite similar to normal spoken sentences in terms of low-frequency modulation characteristics but are unintelligible (i.e., their time-inverted counterparts), we find that discrimination performance based on theta-band phase patterns is equal for both types of stimuli. Consistent with earlier findings, we also observe that whereas theta-band phase patterns differ across stimuli, power patterns do not. We use a simulation model of the single-trial response to spoken sentence stimuli to demonstrate that phase-locked responses to low-frequency modulations of the acoustic signal can account not only for the phase but also for the power results. The simulation offers insight into the interpretation of the empirical results with respect to phase-resetting and power-enhancement models of the evoked response.},
author = {Howard, Mary F and Poeppel, David},
doi = {10.1152/jn.00251.2010},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Howard, Poeppel - 2010 - Discrimination of speech stimuli based on neuronal response phase patterns depends on acoustics but not compreh.pdf:pdf},
isbn = {1522-1598 (Electronic)$\backslash$r0022-3077 (Linking)},
issn = {0022-3077},
journal = {Journal of neurophysiology},
mendeley-groups = {First Exam Citations/Ch3,First Exam Citations},
pages = {2500--2511},
pmid = {20484530},
title = {{Discrimination of speech stimuli based on neuronal response phase patterns depends on acoustics but not comprehension.}},
volume = {104},
year = {2010}
}
@article{Horton2013,
abstract = {People are highly skilled at attending to one speaker in the presence of competitors, but the neural mechanisms supporting this remain unclear. Recent studies have argued that the auditory system enhances the gain of a speech stream relative to competitors by entraining (or "phase-locking") to the rhythmic structure in its acoustic envelope, thus ensuring that syllables arrive during periods of high neuronal excitability. We hypothesized that such a mechanism could also suppress a competing speech stream by ensuring that syllables arrive during periods of low neuronal excitability. To test this, we analyzed high-density EEG recorded from human adults while they attended to one of two competing, naturalistic speech streams. By calculating the cross-correlation between the EEG channels and the speech envelopes, we found evidence of entrainment to the attended speech's acoustic envelope as well as weaker yet significant entrainment to the unattended speech's envelope. An independent component analysis (ICA) decomposition of the data revealed sources in the posterior temporal cortices that displayed robust correlations to both the attended and unattended envelopes. Critically, in these components the signs of the correlations when attended were opposite those when unattended, consistent with the hypothesized entrainment-based suppressive mechanism.},
author = {Horton, C. and D'Zmura, M. and Srinivasan, R.},
doi = {10.1152/jn.01026.2012},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Horton, D'Zmura, Srinivasan - 2013 - Suppression of competing speech through entrainment of cortical oscillations.pdf:pdf},
isbn = {1522-1598 (Electronic)$\backslash$r0022-3077 (Linking)},
issn = {0022-3077},
journal = {Journal of Neurophysiology},
mendeley-groups = {First Exam Citations/Ch4,First Exam Citations},
number = {12},
pages = {3082--3093},
pmid = {23515789},
title = {{Suppression of competing speech through entrainment of cortical oscillations}},
url = {http://jn.physiology.org/cgi/doi/10.1152/jn.01026.2012},
volume = {109},
year = {2013}
}
@article{Ding2014,
abstract = {Speech recognition is robust to background noise. One underlying neural mechanism is that the auditory system segregates speech from the listening background and encodes it reliably. Such robust internal representation has been demonstrated in auditory cortex by neural activity entrained to the temporal envelope of speech. A paradox, however, then arises, as the spectro-temporal fine structure rather than the temporal envelope is known to be the major cue to segregate target speech from background noise. Does the reliable cortical entrainment in fact reflect a robust internal "synthesis" of the attended speech stream rather than direct tracking of the acoustic envelope? Here, we test this hypothesis by degrading the spectro-temporal fine structure while preserving the temporal envelope using vocoders. Magnetoencephalography (MEG) recordings reveal that cortical entrainment to vocoded speech is severely degraded by background noise, in contrast to the robust entrainment to natural speech. Furthermore, cortical entrainment in the delta-band (1-4. Hz) predicts the speech recognition score at the level of individual listeners. These results demonstrate that reliable cortical entrainment to speech relies on the spectro-temporal fine structure, and suggest that cortical entrainment to the speech envelope is not merely a representation of the speech envelope but a coherent representation of multiscale spectro-temporal features that are synchronized to the syllabic and phrasal rhythms of speech. ?? 2013 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Ding, Nai and Chatterjee, Monita and Simon, Jonathan Z.},
doi = {10.1016/j.neuroimage.2013.10.054},
eprint = {NIHMS150003},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding, Chatterjee, Simon - 2014 - Robust cortical entrainment to the speech envelope relies on the spectro-temporal fine structure.pdf:pdf},
issn = {10538119},
journal = {NeuroImage},
keywords = {Auditory cortex,Auditory scene analysis,Envelope entrainment,MEG},
mendeley-groups = {Alice+Pieman,First Exam Citations/Ch4,First Exam Citations},
pages = {41--46},
pmid = {24188816},
title = {{Robust cortical entrainment to the speech envelope relies on the spectro-temporal fine structure}},
volume = {88},
year = {2014}
}
@article{ZionGolumbic2013,
abstract = {The ability to focus on and understand one talker in a noisy social environment is a critical social-cognitive capacity, whose underlying neuronal mechanisms are unclear. We investigated the manner in which speech streams are represented in brain activity and the way that selective attention governs the brain's representation of speech using a "Cocktail Party"paradigm, coupled with direct recordings from the cortical surface in surgical epilepsy patients. We find that brain activity dynamically tracks speech streams using both low-frequency phase and high-frequency amplitude fluctuations and that optimal encoding likely combines the two. In and near low-level auditory cortices, attention "modulates"the representation by enhancing cortical tracking of attended speech streams, but ignored speech remains represented. In higher-order regions, the representation appears to become more "selective,"in that there is no detectable tracking of ignored speech. This selectivity itself seems to sharpen as a sentence unfolds. {\textcopyright} 2013 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {{Zion Golumbic}, Elana M. and Ding, Nai and Bickel, Stephan and Lakatos, Peter and Schevon, Catherine A. and McKhann, Guy M. and Goodman, Robert R. and Emerson, Ronald and Mehta, Ashesh D. and Simon, Jonathan Z. and Poeppel, David and Schroeder, Charles E.},
doi = {10.1016/j.neuron.2012.12.037},
eprint = {NIHMS150003},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zion Golumbic et al. - 2013 - Mechanisms underlying selective neuronal tracking of attended speech at a cocktail party.pdf:pdf},
isbn = {1097-4199 (Electronic)$\backslash$r0896-6273 (Linking)},
issn = {08966273},
journal = {Neuron},
mendeley-groups = {Alice+Pieman,First Exam Citations/Ch4,First Exam Citations},
number = {5},
pages = {980--991},
pmid = {23473326},
title = {{Mechanisms underlying selective neuronal tracking of attended speech at a "cocktail party"}},
volume = {77},
year = {2013}
}
@article{Ding2013,
abstract = {Natural sensory inputs, such as speech and music, are often rhythmic. Recent studies have consistently demonstrated that these rhythmic stimuli cause the phase of oscillatory, i.e. rhythmic, neural activity, recorded as local field potential (LFP), electroencephalography (EEG) or magnetoencephalography (MEG), to synchronize with the stimulus. This phase synchronization, when not accompanied by any increase of response power, has been hypothesized to be the result of phase resetting of ongoing, spontaneous, neural oscillations measurable by LFP, EEG, or MEG. In this article, however, we argue that this same phenomenon can be easily explained without any phase resetting, and where the stimulus-synchronized activity is generated independently of background neural oscillations. It is demonstrated with a simple (but general) stochastic model that, purely due to statistical properties, phase synchronization, as measured by 'inter-trial phase coherence', is much more sensitive to stimulus-synchronized neural activity than is power. These results question the usefulness of analyzing the power and phase of stimulus-synchronized activity as separate and complementary measures; particularly in the case of attempting to demonstrate whether stimulus-synchronized neural activity is generated by phase resetting of ongoing neural oscillations.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Ding, Nai and Simon, Jonathan Z.},
doi = {10.1007/s10827-012-0424-6},
eprint = {NIHMS150003},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding, Simon - 2013 - Power and phase properties of oscillatory neural responses in the presence of background activity.pdf:pdf},
isbn = {0929-5313},
issn = {09295313},
journal = {Journal of Computational Neuroscience},
keywords = {Entrainment,Neural oscillations,Phase coherence,Phase resetting},
mendeley-groups = {First Exam Citations/Ch4,First Exam Citations},
number = {2},
pages = {337--343},
pmid = {23007172},
title = {{Power and phase properties of oscillatory neural responses in the presence of background activity}},
volume = {34},
year = {2013}
}
@article{Ding2012,
abstract = {The cortical representation of the acoustic features of continuous speech is the foundation of speech perception. In this study, noninvasive mag- netoencephalography (MEG) recordings are obtained from human subjects actively listening to spoken narratives, in both simple and cocktail party-like auditory scenes. By modeling how acoustic fea- tures of speech are encoded in ongoing MEG activity as a spectro- temporal response function, we demonstrate that the slow temporal modulations of speech in a broad spectral region are represented bilaterally in auditory cortex by a phase-locked temporal code. For speech presented monaurally to either ear, this phase-locked response is always more faithful in the right hemisphere, but with a shorter latency in the hemisphere contralateral to the stimulated ear. When different spoken narratives are presented to each ear simultaneously (dichotic listening), the resulting cortical neural activity precisely encodes the acoustic features of both of the spoken narratives, but slightly weakened and delayed compared with the monaural response. Critically, the early sensory response to the attended speech is con- siderably stronger than that to the unattended speech, demonstrating top-down attentional gain control. This attentional gain is substantial even during the subjects' very first exposure to the speech mixture and therefore largely independent of knowledge of the speech content. Together, these findings characterize how the spectrotemporal fea- tures of speech are encoded in human auditory cortex and establish a single-trial-based paradigm to study the neural basis underlying the cocktail party phenomenon.},
author = {Ding, N. and Simon, J. Z.},
doi = {10.1152/jn.00297.2011},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding, Simon - 2012 - Neural coding of continuous speech in auditory cortex during monaural and dichotic listening.pdf:pdf},
issn = {0022-3077},
journal = {Journal of Neurophysiology},
mendeley-groups = {First Exam Citations/Ch4,First Exam Citations},
number = {1},
pages = {78--89},
pmid = {21975452},
title = {{Neural coding of continuous speech in auditory cortex during monaural and dichotic listening}},
url = {http://jn.physiology.org/cgi/doi/10.1152/jn.00297.2011},
volume = {107},
year = {2012}
}
@article{DiLiberto2015,
abstract = {The human ability to understand speech is underpinned by a hierarchical auditory system whose successive stages process increasingly complex attributes of the acoustic input. It has been suggested that to produce categorical speech perception, this system must elicit consistent neural responses to speech tokens (e.g., phonemes) despite variations in their acoustics. Here, using electroencephalography (EEG), we provide evidence for this categorical phoneme-level speech processing by showing that the relationship between continuous speech and neural activity is best described when that speech is represented using both low-level spectrotemporal information and categorical labeling of phonetic features. Furthermore, the mapping between phonemes and EEG becomes more discriminative for phonetic features at longer latencies, in line with what one might expect from a hierarchical system. Importantly, these effects are not seen for time-reversed speech. These findings may form the basis for future research on natural language processing in specific cohorts of interest and for broader insights into how brains transform acoustic input into meaning.},
author = {{Di Liberto}, Giovanni M. and O'Sullivan, James A. and Lalor, Edmund C.},
doi = {10.1016/j.cub.2015.08.030},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Di Liberto, O'Sullivan, Lalor - 2015 - Low-frequency cortical entrainment to speech reflects phoneme-level processing.pdf:pdf},
isbn = {1879-0445 (Electronic)$\backslash$r0960-9822 (Linking)},
issn = {09609822},
journal = {Current Biology},
mendeley-groups = {First Exam Citations/Ch3,First Exam Citations},
number = {19},
pages = {2457--2465},
pmid = {26412129},
publisher = {Elsevier Ltd},
title = {{Low-frequency cortical entrainment to speech reflects phoneme-level processing}},
url = {http://dx.doi.org/10.1016/j.cub.2015.08.030},
volume = {25},
year = {2015}
}
@article{Ding2014a,
abstract = {Auditory cortical activity is entrained to the temporal envelope of speech, which corresponds to the syllabic rhythm of speech. Such entrained cortical activity can be measured from subjects naturally listening to sentences or spoken passages, providing a reliable neural marker of online speech processing. A central question still remains to be answered about whether cortical entrained activity is more closely related to speech perception or non-speech-specific auditory encoding. Here, we review a few hypotheses about the functional roles of cortical entrainment to speech, e.g., encoding acoustic features, parsing syllabic boundaries, and selecting sensory information in complex listening environments. It is likely that speech entrainment is not a homogeneous response and these hypotheses apply separately for speech entrainment generated from different neural sources. The relationship between entrained activity and speech intelligibility is also discussed. A tentative conclusion is that theta-band entrainment (4-8 Hz) encodes speech features critical for intelligibility while delta-band entrainment (1-4 Hz) is related to the perceived, non-speech-specific acoustic rhythm. To further understand the functional properties of speech entrainment, a splitter's approach will be needed to investigate (1) not just the temporal envelope but what specific acoustic features are encoded and (2) not just speech intelligibility but what specific psycholinguistic processes are encoded by entrained cortical activity. Similarly, the anatomical and spectro-temporal details of entrained activity need to be taken into account when investigating its functional properties.},
author = {Ding, Nai and Simon, Jonathan Z.},
doi = {10.3389/fnhum.2014.00311},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding, Simon - 2014 - Cortical entrainment to continuous speech functional roles and interpretations.pdf:pdf},
issn = {1662-5161},
journal = {Frontiers in Human Neuroscience},
mendeley-groups = {Alice+Pieman,First Exam Citations/Ch4,First Exam Citations},
pmid = {24904354},
title = {{Cortical entrainment to continuous speech: functional roles and interpretations}},
url = {http://journal.frontiersin.org/article/10.3389/fnhum.2014.00311/abstract},
volume = {8},
year = {2014}
}
