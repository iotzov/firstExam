@article{Dick2001,
abstract = {Selective deficits in aphasic patients' grammatical production and comprehension are often cited as evidence that syntactic processing is modular and localizable in discrete areas of the brain (e.g., Y. Grodzinsky, 2000). The authors review a large body of experimental evidence suggesting that morpho-syntactic deficits can be observed in a number of aphasic and neurologically intact populations. They present new data showing that receptive agrammatism is found not only over a range of aphasic groups, but is also observed in neurologically intact individuals processing under stressful conditions. The authors suggest that these data are most compatible with a domain-general account of language, one that emphasizes the interaction of linguistic distributions with the properties of an associative processor working under normal or suboptimal conditions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Dick, Frederic and Bates, Elizabeth and Utman, Jennifer Aydelott and Wulfeck, Beverly and Dronkers, Nina and Gernsbacher, Morton Ann},
doi = {10.1037/0033-295X.108.4.759},
eprint = {arXiv:1011.1669v3},
issn = {0033295X},
journal = {Psychological Review},
mendeley-groups = {First Exam Citations},
number = {4},
pages = {759--788},
pmid = {11699116},
title = {{Language deficits, localization, and grammar: Evidence for a distributive model of language breakdown in aphasic patients and neurologically intact individuals}},
volume = {108},
year = {2001}
}
@book{Chomsky1986,
abstract = {While theoretical conceptualizations of language policy have grown increasingly rich, empirical data that test these models are less common. Further, there is little methodological guidance for those who wish to do research on lan- guage policy interpretation and appropriation. The ethnography of language policy is proposed as a method which makes macromicro connections by comparing critical discourse analyses of language policy with ethnographic data collection in some local context. A methodological heuristic is offered to guide data collection and sample data are presented from the School District of Philadelphia. It is argued that critical conceptualizations of educational language policy should be combined with empirical data collection of policy appropriation in educational settings},
author = {Chomsky, Noam},
booktitle = {Language Policy},
doi = {10.1007/s10993-009-9136-9},
isbn = {9780275900250},
issn = {15684555},
mendeley-groups = {First Exam Citations},
number = {2},
pages = {139--159},
title = {{Knowledge of Language}},
url = {http://www.springerlink.com/index/10.1007/s10993-009-9136-9},
volume = {8},
year = {1986}
}
@book{Fodor1983,
abstract = {(from the chapter) I want to argue that the current best candidates for treatment as modular cognitive systems share a certain functional role in the mental life of organisms; the discussion in this section is largely devoted to saying which functional role that is. As often happens in playing cognitive science, it is helpful to characterize the functions of psychological systems by analogy to the organization of idealized computing machines. So, I commence with a brief digression in the direction of computers. When philosophers of mind think about computers, it is often Turing machines that they are thinking about. And this is understandable. If there is an interesting analogy between minds qua minds and computers qua computers, it ought to be possible to couch it as an analogy between minds and Turing machines, since a Turing machine is, in a certain sense, as general as any kind of computer can be. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fodor, J.A.},
booktitle = {Reasoning: Studies of human inference and its foundations},
doi = {10.2307/413815},
eprint = {arXiv:1011.1669v3},
isbn = {0262560259},
issn = {00978507},
keywords = {Cognition},
mendeley-groups = {First Exam Citations},
number = {1983},
pages = {878--914},
pmid = {7707144},
title = {{The modularity of mind: an essay on faculty psychology}},
url = {http://proxy.library.lincoln.ac.uk/login?url=http://search.ebscohost.com/login.aspx?direct=true{\&}db=cat01180a{\&}AN=lincoln.125824{\&}site=eds-live{\&}scope=site},
year = {1983}
}
@article{Erber1969,
abstract = {Audio-visual observation of spoken spondaic words was found to be superior to recognition via audition-only under a wide range of S/N conditions. Data from five subjects supported the notion that observers rely increasingly more on visual cues for speech information as S/N ratio is degraded. Audition-only performance was found to be less variable among subjects than was audio-visual recognition. Increased variability in audio-visual scores at poorer S/N ratios was attributed to differences in lip-reading skill among untrained subjects. Speech levels so low that recognition by audition-only approximated chance behavior were found, nevertheless, to systematically improve observers' audio-visual scores as a function of increasing S/N ratio.},
author = {Erber, Norman P.},
doi = {10.1044/jshr.1202.423},
issn = {1092-4388},
journal = {Journal of Speech Language and Hearing Research},
mendeley-groups = {First Exam Citations},
number = {2},
pages = {423},
pmid = {5808871},
title = {{Interaction of Audition and Vision in the Recognition of Oral Speech Stimuli}},
url = {http://jslhr.pubs.asha.org/article.aspx?doi=10.1044/jshr.1202.423},
volume = {12},
year = {1969}
}
@article{Sumby1954,
abstract = {Oral speech intelligibility tests were conducted with, and without, supplementary visual observation of the speaker's facial and lip movements. The difference between these two conditions was examined as a function of the speech-to-noise ratio and of the size of the vocabulary under test. The visual contribution to oral speech intelligibility (relative to its possible contribution) is, to a first approximation, independent of the speech-to-noise ratio under test. However, since there is a much greater opportunity for the visual contribution at low speech-to-noise ratios, its absolute contribution can be exploited most profitably under these conditions.},
author = {Sumby, W. H. and Pollack, Irwin},
doi = {10.1121/1.1907309},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sumby, Pollack - 1954 - Visual Contribution to Speech Intelligibility in Noise.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
mendeley-groups = {First Exam Citations},
number = {2},
pages = {212--215},
pmid = {18647},
title = {{Visual Contribution to Speech Intelligibility in Noise}},
url = {http://asa.scitation.org/doi/10.1121/1.1907309},
volume = {26},
year = {1954}
}
@article{Poeppel2014,
abstract = {New tools and new ideas have changed how we think about the neurobiological foundations of speech and language processing. This perspective focuses on two areas of progress. First, focusing on spatial organization in the human brain, the revised functional anatomy for speech and language is discussed. The complexity of the network organization undermines the well-regarded classical model and suggests looking for more granular computational primitives, motivated both by linguistic theory and neural circuitry. Second, focusing on recent work on temporal organization, a potential role of cortical oscillations for speech processing is outlined. Such an implementational-level mechanism suggests one way to deal with the computational challenge of segmenting natural speech. {\textcopyright} 2014 Elsevier Ltd.},
author = {Poeppel, David},
doi = {10.1016/j.conb.2014.07.005},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Poeppel - 2014 - The neuroanatomic and neurophysiological infrastructure for speech and language.pdf:pdf},
isbn = {1873-6882 (Electronic)$\backslash$r0959-4388 (Linking)},
issn = {18736882},
journal = {Current Opinion in Neurobiology},
mendeley-groups = {First Exam Citations/to read},
pages = {142--149},
pmid = {25064048},
publisher = {Elsevier Ltd},
title = {{The neuroanatomic and neurophysiological infrastructure for speech and language}},
url = {http://dx.doi.org/10.1016/j.conb.2014.07.005},
volume = {28},
year = {2014}
}
@article{Riecke2015,
abstract = {In many natural listening situations, meaningful sounds (e.g., speech) fluctuate in slow rhythms among other sounds. When a slow rhythmic auditory stream is selectively attended, endogenous delta (1-4 Hz) oscillations in auditory cortex may shift their timing so that higher-excitability neuronal phases become aligned with salient events in that stream [1, 2]. As a consequence of this stream-brain phase entrainment [3], these events are processed and perceived more readily than temporally non-overlapping events [4-11], essentially enhancing the neural segregation between the attended stream and temporally noncoherent streams [12]. Stream-brain phase entrainment is robust to acoustic interference [13-20] provided that target stream-evoked rhythmic activity can be segregated from noncoherent activity evoked by other sounds [21], a process that usually builds up over time [22-27]. However, it has remained unclear whether stream-brain phase entrainment functionally contributes to this buildup of rhythmic streams or whether it is merely an epiphenomenon of it. Here, we addressed this issue directly by experimentally manipulating endogenous stream-brain phase entrainment in human auditory cortex with non-invasive transcranial alternating current stimulation (TACS) [28-30]. We assessed the consequences of these manipulations on the perceptual buildup of the target stream (the time required to recognize its presence in a noisy background), using behavioral measures in 20 healthy listeners performing a naturalistic listening task. Experimentally induced cyclic 4-Hz variations in stream-brain phase entrainment reliably caused a cyclic 4-Hz pattern in perceptual buildup time. Our findings demonstrate that strong endogenous delta/theta stream-brain phase entrainment accelerates the perceptual emergence of task-relevant rhythmic streams in noisy environments.},
author = {Riecke, Lars and Sack, Alexander T. and Schroeder, Charles E.},
doi = {10.1016/j.cub.2015.10.045},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Riecke, Sack, Schroeder - 2015 - Endogenous DeltaTheta Sound-Brain Phase Entrainment Accelerates the Buildup of Auditory Streaming.pdf:pdf},
isbn = {0960-9822},
issn = {09609822},
journal = {Current Biology},
mendeley-groups = {First Exam Citations/to read},
number = {24},
pages = {3196--3201},
pmid = {26628008},
publisher = {Elsevier Ltd},
title = {{Endogenous Delta/Theta Sound-Brain Phase Entrainment Accelerates the Buildup of Auditory Streaming}},
url = {http://dx.doi.org/10.1016/j.cub.2015.10.045},
volume = {25},
year = {2015}
}
@article{Park2015,
abstract = {Summary Humans show a remarkable ability to understand continuous speech even under adverse listening conditions. This ability critically relies on dynamically updated predictions of incoming sensory information, but exactly how top-down predictions improve speech processing is still unclear. Brain oscillations are a likely mechanism for these top-down predictions [1, 2]. Quasi-rhythmic components in speech are known to entrain low-frequency oscillations in auditory areas [3, 4], and this entrainment increases with intelligibility [5]. We hypothesize that top-down signals from frontal brain areas causally modulate the phase of brain oscillations in auditory cortex. We use magnetoencephalography (MEG) to monitor brain oscillations in 22 participants during continuous speech perception. We characterize prominent spectral components of speech-brain coupling in auditory cortex and use causal connectivity analysis (transfer entropy) to identify the top-down signals driving this coupling more strongly during intelligible speech than during unintelligible speech. We report three main findings. First, frontal and motor cortices significantly modulate the phase of speech-coupled low-frequency oscillations in auditory cortex, and this effect depends on intelligibility of speech. Second, top-down signals are significantly stronger for left auditory cortex than for right auditory cortex. Third, speech-auditory cortex coupling is enhanced as a function of stronger top-down signals. Together, our results suggest that low-frequency brain oscillations play a role in implementing predictive top-down control during continuous speech perception and that top-down control is largely directed at left auditory cortex. This suggests a close relationship between (left-lateralized) speech production areas and the implementation of top-down control in continuous speech perception.},
author = {Park, Hyojin and Ince, Robin A.A. and Schyns, Philippe G. and Thut, Gregor and Gross, Joachim},
doi = {10.1016/j.cub.2015.04.049},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Park et al. - 2015 - Frontal Top-Down Signals Increase Coupling of Auditory Low-Frequency Oscillations to Continuous Speech in Human Lis.pdf:pdf},
isbn = {doi:10.1016/j.cub.2015.04.049},
issn = {09609822},
journal = {Current Biology},
mendeley-groups = {First Exam Citations/to read},
number = {12},
pages = {1649--1653},
pmid = {26028433},
publisher = {The Authors},
title = {{Frontal Top-Down Signals Increase Coupling of Auditory Low-Frequency Oscillations to Continuous Speech in Human Listeners}},
url = {http://dx.doi.org/10.1016/j.cub.2015.04.049},
volume = {25},
year = {2015}
}
@article{Gross2013,
abstract = {Cortical oscillations are likely candidates for segmentation and coding of continuous speech. Here, we monitored continuous speech processing with magnetoencephalography (MEG) to unravel the principles of speech segmentation and coding. We demonstrate that speech entrains the phase of low-frequency (delta, theta) and the amplitude of high-frequency (gamma) oscillations in the auditory cortex. Phase entrainment is stronger in the right and amplitude entrainment is stronger in the left auditory cortex. Furthermore, edges in the speech envelope phase reset auditory cortex oscillations thereby enhancing their entrainment to speech. This mechanism adapts to the changing physical features of the speech envelope and enables efficient, stimulus-specific speech sampling. Finally, we show that within the auditory cortex, coupling between delta, theta, and gamma oscillations increases following speech edges. Importantly, all couplings (i.e., brain-speech and also within the cortex) attenuate for backward-presented speech, suggesting top-down control. We conclude that segmentation and coding of speech relies on a nested hierarchy of entrained cortical oscillations.},
author = {Gross, Joachim and Hoogenboom, Nienke and Thut, Gregor and Schyns, Philippe and Panzeri, Stefano and Belin, Pascal and Garrod, Simon},
doi = {10.1371/journal.pbio.1001752},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gross et al. - 2013 - Speech Rhythms and Multiplexed Oscillatory Sensory Coding in the Human Brain.pdf:pdf},
isbn = {1545-7885 (Electronic)
1544-9173 (Linking)},
issn = {15449173},
journal = {PLoS Biology},
mendeley-groups = {First Exam Citations/to read},
number = {12},
pmid = {24391472},
title = {{Speech Rhythms and Multiplexed Oscillatory Sensory Coding in the Human Brain}},
volume = {11},
year = {2013}
}
@article{VanRullen2016,
abstract = {Brain function involves oscillations at various frequencies. This could imply that perception and cognition operate periodically, as a succession of cycles mirroring the underlying oscillations. This age-old notion of discrete perception has resurfaced in recent years, fueled by advances in neuroscientific techniques. Contrary to earlier views of discrete perception as a unitary sampling rhythm, contemporary evidence points not to one but several rhythms of perception that may depend on sensory modality, task, stimulus properties, or brain region. In vision, for example, a sensory alpha rhythm (∼10 Hz) may coexist with at least one more rhythm performing attentional sampling at around 7 Hz. How these multiple periodic functions are orchestrated, and how internal sampling rhythms coordinate with overt sampling behavior, remain open questions.},
author = {VanRullen, Rufin},
doi = {10.1016/j.tics.2016.07.006},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/VanRullen - 2016 - Perceptual Cycles.pdf:pdf},
isbn = {1364-6613},
issn = {1879307X},
journal = {Trends in Cognitive Sciences},
keywords = {attention,discrete perception,oscillations,rhythms,sampling},
mendeley-groups = {First Exam Citations/to read},
number = {10},
pages = {723--735},
pmid = {27567317},
publisher = {Elsevier Ltd},
title = {{Perceptual Cycles}},
url = {http://dx.doi.org/10.1016/j.tics.2016.07.006},
volume = {20},
year = {2016}
}
@article{Hyafil2015,
abstract = {Neural oscillations are ubiquitously observed in the mammalian brain, but it has proven difficult to tie oscillatory patterns to specific cognitive operations. Notably, the coupling between neural oscillations at different timescales has recently received much attention, both from experimentalists and theoreticians. We review the mechanisms underlying various forms of this cross-frequency coupling. We show that different types of neural oscillators and cross-frequency interactions yield distinct signatures in neural dynamics. Finally, we associate these mechanisms with several putative functions of cross-frequency coupling, including neural representations of multiple environmental items, communication over distant areas, internal clocking of neural processes, and modulation of neural processing based on temporal predictions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hyafil, Alexandre and Giraud, Anne Lise and Fontolan, Lorenzo and Gutkin, Boris},
doi = {10.1016/j.tins.2015.09.001},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hyafil et al. - 2015 - Neural Cross-Frequency Coupling Connecting Architectures, Mechanisms, and Functions.pdf:pdf},
isbn = {1878-108X (Electronic)$\backslash$r0166-2236 (Linking)},
issn = {1878108X},
journal = {Trends in Neurosciences},
keywords = {Computational models,Cross-frequency coupling,Neural communication,Neural oscillations,Sequence representation,Stimulus parsing},
mendeley-groups = {First Exam Citations/to read},
number = {11},
pages = {725--740},
pmid = {26549886},
publisher = {Elsevier Ltd},
title = {{Neural Cross-Frequency Coupling: Connecting Architectures, Mechanisms, and Functions}},
url = {http://dx.doi.org/10.1016/j.tins.2015.09.001},
volume = {38},
year = {2015}
}
@article{Martin2017,
abstract = {Biological systems often detect species-specific signals in the environment. In humans, speech and language are species-specific signals of fundamental biological importance. To detect the linguistic signal, human brains must form hierarchical representations from a sequence of perceptual inputs distributed in time. What mechanism underlies this ability? One hypothesis is that the brain repurposed an available neurobiological mechanism when hierarchical linguistic representation became an efficient solution to a computational problem posed to the organism. Under such an account, a single mechanism must have the capacity to perform multiple, functionally related computations, e.g., detect the linguistic signal and perform other cognitive functions, while, ideally, oscillating like the human brain. We show that a computational model of analogy, built for an entirely different purpose-learning relational reasoning-processes sentences, represents their meaning, and, crucially, exhibits oscillatory activation patterns resembling cortical signals elicited by the same stimuli. Such redundancy in the cortical and machine signals is indicative of formal and mechanistic alignment between representational structure building and "cortical" oscillations. By inductive inference, this synergy suggests that the cortical signal reflects structure generation, just as the machine signal does. A single mechanism-using time to encode information across a layered network-generates the kind of (de)compositional representational hierarchy that is crucial for human language and offers a mechanistic linking hypothesis between linguistic representation and cortical computation.},
author = {Martin, Andrea E. and Doumas, Leonidas A.A.},
doi = {10.1371/journal.pbio.2000663},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martin, Doumas - 2017 - A mechanism for the cortical computation of hierarchical linguistic structure.pdf:pdf},
isbn = {1111111111},
issn = {15457885},
journal = {PLoS Biology},
mendeley-groups = {First Exam Citations/to read},
number = {3},
pages = {1--23},
pmid = {28253256},
title = {{A mechanism for the cortical computation of hierarchical linguistic structure}},
volume = {15},
year = {2017}
}
@article{Uddin2018,
abstract = {Environmental sounds (ES) can be understood easily when substituted for words in sentences, suggesting that linguistic context benefits may be mediated by processes more general than some language-specific theories assert. However, the underlying neural processing is not understood. EEG was recorded for spoken sentences ending in either a spoken word or a corresponding ES. Endings were either congruent or incongruent with the sentence frame, and thus were expected to produce N400 activity. However, if ES and word meanings are combined with language context by different mechanisms, different N400 responses would be expected. Incongruent endings (both words and ES) elicited frontocentral negativities corresponding to the N400 typically observed to incongruent spoken words. Moreover, sentential constraint had similar effects on N400 topographies to ES and words. Comparison of speech and ES responses suggests that understanding meaning in speech context may be mediated by similar neural mechanisms for these two types of stimuli.},
author = {Uddin, Sophia and Heald, Shannon L.M. and {Van Hedger}, Stephen C. and Nusbaum, Howard C.},
doi = {10.1016/j.bandl.2018.02.004},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Uddin et al. - 2018 - Hearing sounds as words Neural responses to environmental sounds in the context of fluent speech.pdf:pdf},
issn = {10902155},
journal = {Brain and Language},
keywords = {Context,Environmental sounds,Event-related potential,Language processing,N400,Sentence understanding},
mendeley-groups = {First Exam Citations/to read},
number = {February},
pages = {51--61},
title = {{Hearing sounds as words: Neural responses to environmental sounds in the context of fluent speech}},
volume = {179},
year = {2018}
}
@article{Keitel2017,
abstract = {The timing of slow auditory cortical activity aligns to the rhythmic fluctuations in speech. This entrainment is considered to be a marker of the prosodic and syllabic encoding of speech, and has been shown to correlate with intelligibility. Yet, whether and how auditory cortical entrainment is influenced by the activity in other speech–relevant areas remains unknown. Using source-localized MEG data, we quantified the dependency of auditory entrainment on the state of oscillatory activity in fronto-parietal regions. We found that delta band entrainment interacted with the oscillatory activity in three distinct networks. First, entrainment in the left anterior superior temporal gyrus (STG) was modulated by beta power in orbitofrontal areas, possibly reflecting predictive top-down modulations of auditory encoding. Second, entrainment in the left Heschl's Gyrus and anterior STG was dependent on alpha power in central areas, in line with the importance of motor structures for phonological analysis. And third, entrainment in the right posterior STG modulated theta power in parietal areas, consistent with the engagement of semantic memory. These results illustrate the topographical network interactions of auditory delta entrainment and reveal distinct cross-frequency mechanisms by which entrainment can interact with different cognitive processes underlying speech perception.},
author = {Keitel, Anne and Ince, Robin A.A. and Gross, Joachim and Kayser, Christoph},
doi = {10.1016/j.neuroimage.2016.11.062},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Keitel et al. - 2017 - Auditory cortical delta-entrainment interacts with oscillatory power in multiple fronto-parietal networks.pdf:pdf},
isbn = {1053-8119},
issn = {10959572},
journal = {NeuroImage},
keywords = {Auditory entrainment,Delta band,MEG,Prosodic parsing,Speech processing},
mendeley-groups = {First Exam Citations/to read},
number = {November 2016},
pages = {32--42},
pmid = {27903440},
publisher = {Elsevier},
title = {{Auditory cortical delta-entrainment interacts with oscillatory power in multiple fronto-parietal networks}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2016.11.062},
volume = {147},
year = {2017}
}
@article{Perez2015,
abstract = {Neuronal oscillations play a key role in auditory perception of verbal input, with the oscillatory rhythms of the brain showing synchronization with specific frequencies of speech. Here we investigated the neural oscillatory patterns associated with perceiving native, foreign, and unknown speech. Spectral power and phase synchronization were compared to those of a silent context. Power synchronization to native speech was found in frequency ranges corresponding to the theta band, while no synchronization patterns were found for the foreign speech context and the unknown language context. For phase synchrony, the native and unknown languages showed higher synchronization in the theta-band than the foreign language when compared to the silent condition. These results suggest that neural synchronization patterns are markedly different for native and foreign languages.},
author = {P{\'{e}}rez, Alejandro and Carreiras, Manuel and {Gillon Dowens}, Margaret and Du{\~{n}}abeitia, Jon Andoni},
doi = {10.1016/j.bandl.2015.05.008},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/P{\'{e}}rez et al. - 2015 - Differential oscillatory encoding of foreign speech.pdf:pdf},
isbn = {1090-2155 (Electronic)$\backslash$r0093-934X (Linking)},
issn = {10902155},
journal = {Brain and Language},
keywords = {Bilingualism,Foreign language,Language,Oscillations,Speech,Synchrony},
mendeley-groups = {First Exam Citations/to read},
pages = {51--57},
pmid = {26070104},
title = {{Differential oscillatory encoding of foreign speech}},
volume = {147},
year = {2015}
}
@article{Cibelli2015,
abstract = {Neural representations of words are thought to have a complex spatio-temporal cortical basis. It has been suggested that spoken word recognition is not a process of feed-forward computations from phonetic to lexical forms, but rather involves the online integration of bottom-up input with stored lexical knowledge. Using direct neural recordings from the temporal lobe, we examined cortical responses to words and pseudowords. We found that neural populations were not only sensitive to lexical status (real vs. pseudo), but also to cohort size (number of words matching the phonetic input at each time point) and cohort frequency (lexical frequency of those words). These lexical variables modulated neural activity from the posterior to anterior temporal lobe, and also dynamically as the stimuli unfolded on a millisecond time scale. Our findings indicate that word recognition is not purely modular, but relies on rapid and online integration of multiple sources of lexical knowledge.},
author = {Cibelli, Emily S. and Leonard, Matthew K. and Johnson, Keith and Chang, Edward F.},
doi = {10.1016/j.bandl.2015.05.005},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cibelli et al. - 2015 - The influence of lexical statistics on temporal lobe cortical dynamics during spoken word listening.pdf:pdf},
isbn = {1090-2155 (Electronic) 0093-934X (Linking)},
issn = {10902155},
journal = {Brain and Language},
keywords = {Electrocorticography (ECoG),Lexical statistics,Pseudowords,Temporal lobe,Word comprehension},
mendeley-groups = {First Exam Citations/to read},
pages = {66--75},
pmid = {26072003},
publisher = {Elsevier Inc.},
title = {{The influence of lexical statistics on temporal lobe cortical dynamics during spoken word listening}},
url = {http://dx.doi.org/10.1016/j.bandl.2015.05.005},
volume = {147},
year = {2015}
}
@article{Hagoort2014,
abstract = {Current views on the neurobiological underpinnings of language are discussed that deviate in a number of ways from the classical Wernicke-Lichtheim-Geschwind model. More areas than Broca's and Wernicke's region are involved in language. Moreover, a division along the axis of language production and language comprehension does not seem to be warranted. Instead, for central aspects of language processing neural infrastructure is shared between production and comprehension. Three different accounts of the role of Broca's area in language are discussed. Arguments are presented in favor of a dynamic network view, in which the functionality of a region is co-determined by the network of regions in which it is embedded at particular moments in time. Finally, core regions of language processing need to interact with other networks (e.g. the attentional networks and the ToM network) to establish full functionality of language and communication. {\textcopyright} 2014 .},
author = {Hagoort, Peter},
doi = {10.1016/j.conb.2014.07.013},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hagoort - 2014 - Nodes and networks in the neural architecture for language Broca's region and beyond.pdf:pdf},
isbn = {0959-4388},
issn = {18736882},
journal = {Current Opinion in Neurobiology},
mendeley-groups = {First Exam Citations/to read},
pages = {136--141},
pmid = {25062474},
publisher = {Elsevier Ltd},
title = {{Nodes and networks in the neural architecture for language: Broca's region and beyond}},
url = {http://dx.doi.org/10.1016/j.conb.2014.07.013},
volume = {28},
year = {2014}
}
@article{Jackendoff2003,
author = {Jackendoff, Ray},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jackendoff - 2003 - Pr{\'{e}}cis of foundations of language brain, meaning, grammar, evolution.pdf:pdf},
journal = {Behavioral and Brain Sciences},
keywords = {evolution of language,generative grammar,parallel architecture,semantics,syntax},
mendeley-groups = {First Exam Citations},
pages = {651--707},
title = {{Pr{\'{e}}cis of foundations of language: brain, meaning, grammar, evolution}},
url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/precis-of-foundations-of-language-brain-meaning-grammar-evolution/9CB09C09B950CD56DC8FFE8795C7A43F},
year = {2003}
}
@article{Andrews2009,
abstract = {The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.},
author = {Andrews, Mark and Vigliocco, Gabriella and Vinson, David},
doi = {10.1037/a0016261},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrews, Vigliocco, Vinson - 2009 - Integrating Experiential and Distributional Data to Learn Semantic Representations.pdf:pdf},
isbn = {1939-1471(Electronic);0033-295X(Print)},
issn = {0033295X},
journal = {Psychological Review},
keywords = {Bayesian models,computational models,distributional data,probabilistic models,semantic representations},
mendeley-groups = {First Exam Citations},
number = {3},
pages = {463--498},
pmid = {19618982},
title = {{Integrating Experiential and Distributional Data to Learn Semantic Representations}},
volume = {116},
year = {2009}
}
@article{Mitchell2008,
abstract = {The question of how the human brain represents conceptual knowledge has been debated in many scientific fields. Brain imaging studies have shown that different spatial patterns of neural activation are associated with thinking about different semantic categories of pictures and words (for example, tools, buildings, and animals). We present a computational model that predicts the functional magnetic resonance imaging (fMRI) neural activation associated with words for which fMRI data are not yet available. This model is trained with a combination of data from a trillion-word text corpus and observed fMRI data associated with viewing several dozen concrete nouns. Once trained, the model predicts fMRI activation for thousands of other concrete nouns in the text corpus, with highly significant accuracies over the 60 nouns for which we currently have fMRI data.},
author = {Mitchell, Tom M. and Shinkareva, Svetlana V. and Carlson, Andrew and Chang, Kai Min and Malave, Vicente L. and Mason, Robert A. and Just, Marcel Adam},
doi = {10.1126/science.1152876},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mitchell et al. - 2008 - Predicting human brain activity associated with the meanings of nouns.pdf:pdf},
isbn = {0036-8075},
issn = {00368075},
journal = {Science},
mendeley-groups = {First Exam Citations},
number = {5880},
pages = {1191--1195},
pmid = {18511683},
title = {{Predicting human brain activity associated with the meanings of nouns}},
volume = {320},
year = {2008}
}
@article{Wehbe2014,
abstract = {Story understanding involves many perceptual and cognitive subprocesses, from perceiving individual words, to parsing sentences, to understanding the relationships among the story characters. We present an integrated computational model of reading that incorporates these and additional subprocesses, simultaneously discovering their fMRI signatures. Our model predicts the fMRI activity associated with reading arbitrary text passages, well enough to distinguish which of two story segments is being read with 74{\%} accuracy. This approach is the first to simultaneously track diverse reading subprocesses during complex story processing and predict the detailed neural representation of diverse story features, ranging from visual word properties to the mention of different story characters and different actions they perform. We construct brain representation maps that replicate many results from a wide range of classical studies that focus each on one aspect of language processing and offer new insights on which type of information is processed by different areas involved in language processing. Additionally, this approach is promising for studying individual differences: it can be used to create single subject maps that may potentially be used to measure reading comprehension and diagnose reading disorders.},
author = {Wehbe, Leila and Murphy, Brian and Talukdar, Partha and Fyshe, Alona and Ramdas, Aaditya and Mitchell, Tom},
doi = {10.1371/journal.pone.0112575},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wehbe et al. - 2014 - Simultaneously Uncovering the Patterns of Brain Regions Involved in Different Story Reading Subprocesses.pdf:pdf},
isbn = {1932-6203 (Electronic)$\backslash$r1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
mendeley-groups = {First Exam Citations},
number = {11},
pages = {1--19},
pmid = {25426840},
title = {{Simultaneously Uncovering the Patterns of Brain Regions Involved in Different Story Reading Subprocesses}},
volume = {9},
year = {2014}
}
@article{Salmelin2007,
abstract = {Clinical evaluation of language function and basic neuroscience research into the neurophysiology of language are tied together. Whole-head MEG systems readily facilitate detailed spatiotemporal characterization of language processes. A fair amount of information is available about the cortical sequence of word perception and comprehension in the auditory and visual domain, which can be applied for clinical use. Language production remains, at present, somewhat less well charted. In clinical practice, the most obvious needs are noninvasive evaluation of the language-dominant hemisphere and mapping of areas involved in language performance to assist surgery. Multiple experimental designs and analysis approaches have been proposed for estimation of language lateralization. Some of them have been compared with the invasive Wada test and need to be tested further. Development of approaches for more comprehensive pre-surgical characterization of language cortex should build on basic neuroscience research, making use of parametric designs that allow functional mapping. Studies of the neural basis of developmental and acquired language disorders, such as dyslexia, stuttering, and aphasia can currently be regarded more as clinical or basic neuroscience research rather than as clinical routine. Such investigations may eventually provide tools for development of individually targeted training procedures and their objective evaluation. {\textcopyright} 2006 International Federation of Clinical Neurophysiology.},
author = {Salmelin, Riitta},
doi = {10.1016/j.clinph.2006.07.316},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Salmelin - 2007 - Clinical neurophysiology of language The MEG approach.pdf:pdf},
isbn = {1388-2457 (Print) 1388-2457 (Linking)},
issn = {13882457},
journal = {Clinical Neurophysiology},
keywords = {Magnetoencephalography,Picture naming,Reading,Speech perception,Speech production},
mendeley-groups = {First Exam Citations},
number = {2},
pages = {237--254},
pmid = {17008126},
title = {{Clinical neurophysiology of language: The MEG approach}},
volume = {118},
year = {2007}
}
@article{Berwick2013,
abstract = {Language serves as a cornerstone for human cognition, yet much about its evolution remains puzzling. Recent research on this question parallels Darwin's attempt to explain both the unity of all species and their diversity. What has emerged from this research is that the unified nature of human language arises from a shared, species-specific computational ability. This ability has identifiable correlates in the brain and has remained fixed since the origin of language approximately 100 thousand years ago. Although songbirds share with humans a vocal imitation learning ability, with a similar underlying neural organization, language is uniquely human. {\textcopyright} 2012 Elsevier Ltd.},
author = {Berwick, Robert C. and Friederici, Angela D. and Chomsky, Noam and Bolhuis, Johan J.},
doi = {10.1016/j.tics.2012.12.002},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Berwick et al. - 2013 - Evolution, brain, and the nature of language.pdf:pdf},
isbn = {1879-307X},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
mendeley-groups = {First Exam Citations},
number = {2},
pages = {98},
pmid = {23313359},
publisher = {Elsevier Ltd},
title = {{Evolution, brain, and the nature of language}},
url = {http://dx.doi.org/10.1016/j.tics.2012.12.002},
volume = {17},
year = {2013}
}
@article{Hale2011,
abstract = {This article examines cognitive process models of human sentence comprehension based on the idea of informed search. These models are rational in the sense that they strive to find a good syntactic analysis quickly. Informed search derives a new account of garden pathing that handles traditional counterexamples. It supports a symbolic explanation for local coherence as well as an algorithmic account of entropy reduction. The models are expressed in a broad framework for theories of human sentence comprehension.},
author = {Hale, John T.},
doi = {10.1111/j.1551-6709.2010.01145.x},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hale - 2011 - What a rational parser would do.pdf:pdf},
isbn = {03640213},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Comprehension,Entropy reduction,Heuristic,Rationality,Sentence processing,Syntax},
mendeley-groups = {First Exam Citations},
number = {3},
pages = {399--443},
title = {{What a rational parser would do}},
volume = {35},
year = {2011}
}
@article{Sanford2002,
abstract = {The study of processes underlying the interpretation of language often produces evidence that they are complete and occur incrementally. However, computational linguistics has shown that interpretations are often effective even if they are underspecified. We present evidence that similar underspecified representations are used by humans during comprehension, drawing on a scattered and varied literature. We also show how linguistic properties of focus, subordination and focalization can control depth of processing, leading to underspecified representations. Modulation of degrees of specification might provide a way forward in the development of models of the processing underlying language understanding.},
author = {Sanford, Anthony J. and Sturt, Patrick},
doi = {10.1016/S1364-6613(02)01958-7},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sanford, Sturt - 2002 - Depth of processing in language comprehension Not noticing the evidence.pdf:pdf},
isbn = {1364-6613},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
mendeley-groups = {First Exam Citations},
number = {9},
pages = {382--386},
pmid = {12200180},
title = {{Depth of processing in language comprehension: Not noticing the evidence}},
volume = {6},
year = {2002}
}
@article{Frank2012,
abstract = {It is generally assumed that hierarchical phrase structure plays a central role in human language. However, considerations of simplicity and evolutionary continuity suggest that hierarchical structure should not be invoked too hastily. Indeed, recent neurophysiological, behavioural and computational studies show that sequential sentence structure has considerable explanatory power and that hierarchical processing is often not involved. In this paper, we review evidence from the recent literature supporting the hypothesis that sequential structure may be fundamental to the comprehension, production and acquisition of human language. Moreover, we provide a preliminary sketch outlining a non-hierarchical model of language use and discuss its implications and testable predictions. If linguistic phenomena can be explained by sequential rather than hierarchical structure, this will have considerable impact in a wide range of fields, such as linguistics, ethology, cognitive neuroscience, psychology and computer science.},
author = {Frank, Stefan L. and Bod, Rens and Christiansen, Morten H.},
doi = {10.1098/rspb.2012.1741},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Frank, Bod, Christiansen - 2012 - How hierarchical is language use.pdf:pdf},
isbn = {0962-8452, 1471-2954},
issn = {14712954},
journal = {Proceedings of the Royal Society B: Biological Sciences},
keywords = {Cognitive neuroscience,Computational linguistics,Language evolution,Language structure,Psycholinguistics},
mendeley-groups = {First Exam Citations},
number = {1747},
pages = {4522--4531},
pmid = {22977157},
title = {{How hierarchical is language use?}},
volume = {279},
year = {2012}
}
@article{Jackendoff2017,
abstract = {We suggest that one way to approach the evolution of language is through reverse engineering: asking what components of the language faculty could have been useful in the absence of the full complement of components. We explore the possibilities offered by linear grammar, a form of language that lacks syntax and morphology altogether, and that structures its utterances through a direct mapping between semantics and phonology. A language with a linear grammar would have no syntactic categories or syntactic phrases, and therefore no syntactic recursion. It would also have no functional categories such as tense, agreement, and case inflection, and no derivational morphology. Such a language would still be capable of conveying certain semantic relations through word order-for instance by stipulating that agents should precede patients. However, many other semantic relations would have to be based on pragmatics and discourse context. We find evidence of linear grammar in a wide range of linguistic phenomena: pidgins, stages of late second language acquisition, home signs, village sign languages, language comprehension (even in fully syntactic languages), aphasia, and specific language impairment. We also find a full-blown language, Riau Indonesian, whose grammar is arguably close to a pure linear grammar. In addition, when subjects are asked to convey information through nonlinguistic gesture, their gestures make use of semantically based principles of linear ordering. Finally, some pockets of English grammar, notably compounds, can be characterized in terms of linear grammar. We conclude that linear grammar is a plausible evolutionary precursor of modern fully syntactic grammar, one that is still active in the human mind.},
author = {Jackendoff, Ray and Wittenberg, Eva},
doi = {10.3758/s13423-016-1073-y},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jackendoff, Wittenberg - 2017 - Linear grammar as a possible stepping-stone in the evolution of language.pdf:pdf},
issn = {15315320},
journal = {Psychonomic Bulletin and Review},
keywords = {Language evolution,Linear grammar},
mendeley-groups = {First Exam Citations},
number = {1},
pages = {219--224},
pmid = {27368633},
publisher = {Psychonomic Bulletin {\&} Review},
title = {{Linear grammar as a possible stepping-stone in the evolution of language}},
volume = {24},
year = {2017}
}
@article{Pinker2005,
abstract = {We examine the question of which aspects of language are uniquely human and uniquely linguistic in light of recent suggestions by Hauser, Chomsky, and Fitch that the only such aspect is syntactic recursion, the rest of language being either specific to humans but not to language (e.g. words and concepts) or not specific to humans (e.g. speech perception). We find the hypothesis problematic. It ignores the many aspects of grammar that are not recursive, such as phonology, morphology, case, agreement, and many properties of words. It is inconsistent with the anatomy and neural control of the human vocal tract. And it is weakened by experiments suggesting that speech perception cannot be reduced to primate audition, that word learning cannot be reduced to fact learning, and that at least one gene involved in speech and language was evolutionarily selected in the human lineage but is not specific to recursion. The recursion-only claim, we suggest, is motivated by Chomsky's recent approach to syntax, the Minimalist Program, which de-emphasizes the same aspects of language. The approach, however, is sufficiently problematic that it cannot be used to support claims about evolution. We contest related arguments that language is not an adaptation, namely that it is "perfect," non-redundant, unusable in any partial form, and badly designed for communication. The hypothesis that language is a complex adaptation for communication which evolved piecemeal avoids all these problems. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Pinker, Steven and Jackendoff, Ray},
doi = {10.1016/j.cognition.2004.08.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pinker, Jackendoff - 2005 - The faculty of language What's special about it.pdf:pdf},
isbn = {0010-0277},
issn = {00100277},
journal = {Cognition},
keywords = {Communication,Evolution,Language,Minimalism,Phonology,Syntax},
mendeley-groups = {First Exam Citations},
number = {2},
pages = {201--236},
pmid = {15694646},
title = {{The faculty of language: What's special about it?}},
volume = {95},
year = {2005}
}
@article{Ross2007,
abstract = {Viewing a speaker's articulatory movements substantially improves a listener's ability to understand spoken words, especially under noisy environmental conditions. It has been claimed that this gain is most pronounced when auditory input is weakest, an effect that has been related to a well-known principle of multisensory integration--"inverse effectiveness." In keeping with the predictions of this principle, the present study showed substantial gain in multisensory speech enhancement at even the lowest signal-to-noise ratios (SNRs) used (-24 dB), but it was also evident that there was a "special zone" at a more intermediate SNR of -12 dB where multisensory integration was additionally enhanced beyond the predictions of this principle. As such, we show that inverse effectiveness does not strictly apply to the multisensory enhancements seen during audiovisual speech perception. Rather, the gain from viewing visual articulations is maximal at intermediate SNRs, well above the lowest auditory SNR where the recognition of whole words is significantly different from zero. We contend that the multisensory speech system is maximally tuned for SNRs between extremes, where the system relies on either the visual (speech-reading) or the auditory modality alone, forming a window of maximal integration at intermediate SNR levels. At these intermediate levels, the extent of multisensory enhancement of speech recognition is considerable, amounting to more than a 3-fold performance improvement relative to an auditory-alone condition.},
author = {Ross, Lars A. and Saint-Amour, Dave and Leavitt, Victoria M. and Javitt, Daniel C. and Foxe, John J.},
doi = {10.1093/cercor/bhl024},
isbn = {1047-3211 (Print)$\backslash$r1047-3211 (Linking)},
issn = {10473211},
journal = {Cerebral Cortex},
keywords = {Audiovisual,Crossmodal,Inverse effectiveness,Lip-reading,Multisensory,Speech perception,Speech-reading},
mendeley-groups = {First Exam Citations},
number = {5},
pages = {1147--1153},
pmid = {16785256},
title = {{Do you see what I am saying? Exploring visual enhancement of speech comprehension in noisy environments}},
volume = {17},
year = {2007}
}
@article{Poulsen2017,
abstract = {We performed simultaneous recordings of electroencephalography (EEG) from multiple students in a classroom, and measured the inter-subject correlation (ISC) of activity evoked by a common video stimulus. The neural reliability, as quantified by ISC, has been linked to engagement and attentional modulation in earlier studies that used high-grade equipment in laboratory settings. Here we reproduce many of the results from these studies using portable low-cost equipment, focusing on the robustness of using ISC for subjects experiencing naturalistic stimuli. The present data shows that stimulus-evoked neural responses, known to be modulated by attention, can be tracked in for groups of students with synchronized EEG acquisition. This is a step towards real-time inference of engagement in the classroom.},
archivePrefix = {arXiv},
arxivId = {1604.03019},
author = {Poulsen, Andreas Trier and Kamronn, Simon and Dmochowski, Jacek and Parra, Lucas C. and Hansen, Lars Kai},
doi = {10.1038/srep43916},
eprint = {1604.03019},
issn = {20452322},
journal = {Scientific Reports},
mendeley-groups = {First Exam Citations},
pmid = {28266588},
title = {{EEG in the classroom: Synchronised neural recordings during video presentation}},
volume = {7},
year = {2017}
}
@article{Crosse2016,
abstract = {UNLABELLED Speech comprehension is improved by viewing a speaker's face, especially in adverse hearing conditions, a principle known as inverse effectiveness. However, the neural mechanisms that help to optimize how we integrate auditory and visual speech in such suboptimal conversational environments are not yet fully understood. Using human EEG recordings, we examined how visual speech enhances the cortical representation of auditory speech at a signal-to-noise ratio that maximized the perceptual benefit conferred by multisensory processing relative to unisensory processing. We found that the influence of visual input on the neural tracking of the audio speech signal was significantly greater in noisy than in quiet listening conditions, consistent with the principle of inverse effectiveness. Although envelope tracking during audio-only speech was greatly reduced by background noise at an early processing stage, it was markedly restored by the addition of visual speech input. In background noise, multisensory integration occurred at much lower frequencies and was shown to predict the multisensory gain in behavioral performance at a time lag of ∼250 ms. Critically, we demonstrated that inverse effectiveness, in the context of natural audiovisual (AV) speech processing, relies on crossmodal integration over long temporal windows. Our findings suggest that disparate integration mechanisms contribute to the efficient processing of AV speech in background noise. SIGNIFICANCE STATEMENT The behavioral benefit of seeing a speaker's face during conversation is especially pronounced in challenging listening environments. However, the neural mechanisms underlying this phenomenon, known as inverse effectiveness, have not yet been established. Here, we examine this in the human brain using natural speech-in-noise stimuli that were designed specifically to maximize the behavioral benefit of audiovisual (AV) speech. We find that this benefit arises from our ability to integrate multimodal information over longer periods of time. Our data also suggest that the addition of visual speech restores early tracking of the acoustic speech signal during excessive background noise. These findings support and extend current mechanistic perspectives on AV speech perception.},
author = {Crosse, M. J. and {Di Liberto}, G. M. and Lalor, E. C.},
doi = {10.1523/JNEUROSCI.1396-16.2016},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Crosse, Di Liberto, Lalor - 2016 - Eye Can Hear Clearly Now Inverse Effectiveness in Natural Audiovisual Speech Processing Relies on Lon.pdf:pdf},
isbn = {0270-6474},
issn = {0270-6474},
journal = {Journal of Neuroscience},
mendeley-groups = {First Exam Citations},
number = {38},
pages = {9888--9895},
pmid = {27656026},
title = {{Eye Can Hear Clearly Now: Inverse Effectiveness in Natural Audiovisual Speech Processing Relies on Long-Term Crossmodal Temporal Integration}},
url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.1396-16.2016},
volume = {36},
year = {2016}
}
@article{VanEyndhoven2017,
abstract = {OBJECTIVE: We aim to extract and denoise the attended speaker in a noisy, two-speaker acoustic scenario, relying on microphone array recordings from a binaural hearing aid, which are complemented with electroencephalography (EEG) recordings to infer the speaker of interest. METHODS: In this study, we propose a modular processing flow that first extracts the two speech envelopes from the microphone recordings, then selects the attended speech envelope based on the EEG, and finally uses this envelope to inform a multi-channel speech separation and denoising algorithm. RESULTS: Strong suppression of interfering (unattended) speech and background noise is achieved, while the attended speech is preserved. Furthermore, EEG-based auditory attention detection (AAD) is shown to be robust to the use of noisy speech signals. CONCLUSIONS: Our results show that AAD-based speaker extraction from microphone array recordings is feasible and robust, even in noisy acoustic environments, and without access to the clean speech signals to perform EEG-based AAD. SIGNIFICANCE: Current research on AAD always assumes the availability of the clean speech signals, which limits the applicability in real settings. We have extended this research to detect the attended speaker even when only microphone recordings with noisy speech mixtures are available. This is an enabling ingredient for new brain-computer interfaces and effective filtering schemes in neuro-steered hearing prostheses. Here, we provide a first proof of concept for EEG-informed attended speaker extraction and denoising.},
archivePrefix = {arXiv},
arxivId = {1602.05702},
author = {{Van Eyndhoven}, Simon and Francart, Tom and Bertrand, Alexander},
doi = {10.1109/TBME.2016.2587382},
eprint = {1602.05702},
isbn = {0018-9294},
issn = {15582531},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Auditory attention detection (AAD),EEG signal processing,auditory prostheses,blind source separation (BSS),brain-computer interface,multichannel Wiener filter (MWF),speech enhancement},
mendeley-groups = {First Exam Citations},
number = {5},
pages = {1045--1056},
pmid = {27392339},
title = {{EEG-Informed Attended Speaker Extraction from Recorded Speech Mixtures with Application in Neuro-Steered Hearing Prostheses}},
volume = {64},
year = {2017}
}
@article{Park2016,
abstract = {During continuous speech, lip movements provide visual temporal signals that facilitate speech processing. Here, using MEG we directly investigated how these visual signals interact with rhythmic brain activity in participants listening to and seeing the speaker. First, we investigated coherence between oscillatory brain activity and speaker's lip movements and demonstrated significant entrainment in visual cortex. We then used partial coherence to remove contributions of the coherent auditory speech signal from the lip-brain coherence. Comparing this synchronization between different attention conditions revealed that attending visual speech enhances the coherence between activity in visual cortex and the speaker's lips. Further, we identified a significant partial coherence between left motor cortex and lip movements and this partial coherence directly predicted comprehension accuracy. Our results emphasize the importance of visually entrained and attention-modulated rhythmic brain activity for the enhancement of audiovisual speech processing.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Park, Hyojin and Kayser, Christoph and Thut, Gregor and Gross, Joachim},
doi = {10.7554/eLife.14521},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Park et al. - 2016 - Lip movements entrain the observers' low-frequency brain oscillations to facilitate speech intelligibility.pdf:pdf},
isbn = {2050-084X},
issn = {2050084X},
journal = {eLife},
mendeley-groups = {First Exam Citations},
number = {MAY2016},
pmid = {27146891},
title = {{Lip movements entrain the observers' low-frequency brain oscillations to facilitate speech intelligibility}},
volume = {5},
year = {2016}
}
@article{Frank2018,
abstract = {Results from a recent neuroimaging study on spoken sentence comprehension have been interpreted as evidence for cortical entrainment to hierarchical syntactic structure. We present a simple computational model that predicts the power spectra from this study, even though the model's linguistic knowledge is restricted to the lexical level, and word-level representations are not combined into higher-level units (phrases or sentences). Hence, the cortical entrainment results can also be explained from the lexical properties of the stimuli, without recourse to hierarchical syntax.},
archivePrefix = {arXiv},
arxivId = {1706.05656},
author = {Frank, Stefan L. and Yang, Jinbiao},
doi = {10.1371/journal.pone.0197304},
eprint = {1706.05656},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Frank, Yang - 2018 - Lexical representation explains cortical entrainment during speech comprehension.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
mendeley-groups = {obama paper,First Exam Citations},
number = {5},
pages = {1--11},
title = {{Lexical representation explains cortical entrainment during speech comprehension}},
volume = {13},
year = {2018}
}
@article{Ding2015,
abstract = {The most critical attribute of human language is its unbounded combinatorial nature: smaller elements can be combined into larger structures on the basis of a grammatical system, resulting in a hierarchy of linguistic units, such as words, phrases and sentences. Mentally parsing and representing such structures, however, poses challenges for speech comprehension. In speech, hierarchical linguistic structures do not have boundaries that are clearly defined by acoustic cues and must therefore be internally and incrementally constructed during comprehension. We found that, during listening to connected speech, cortical activity of different timescales concurrently tracked the time course of abstract linguistic structures at different hierarchical levels, such as words, phrases and sentences. Notably, the neural tracking of hierarchical linguistic structures was dissociated from the encoding of acoustic cues and from the predictability of incoming words. Our results indicate that a hierarchy of neural processing timescales underlies grammar-based internal construction of hierarchical linguistic structure.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Ding, Nai and Melloni, Lucia and Zhang, Hang and Tian, Xing and Poeppel, David},
doi = {10.1038/nn.4186},
eprint = {15334406},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding et al. - 2015 - Cortical tracking of hierarchical linguistic structures in connected speech.pdf:pdf},
isbn = {1546-1726},
issn = {15461726},
journal = {Nature Neuroscience},
mendeley-groups = {obama paper,First Exam Citations},
number = {1},
pages = {158--164},
pmid = {26642090},
title = {{Cortical tracking of hierarchical linguistic structures in connected speech}},
volume = {19},
year = {2015}
}
@article{Wong2018,
author = {Wong, Daniel D E and Fuglsang, S{\o}ren A and Hjortkj{\ae}r, Jens and Ceolini, Enea},
doi = {10.1101/281345},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wong et al. - 2018 - A Comparison of Temporal Response Function Estimation Methods for EEG-based Auditory Attention Decoding.pdf:pdf},
journal = {Biorxiv},
mendeley-groups = {obama paper,First Exam Citations},
pages = {1--22},
title = {{A Comparison of Temporal Response Function Estimation Methods for EEG-based Auditory Attention Decoding}},
year = {2018}
}
@article{OSullivan2017,
abstract = {Objective . People who suffer from hearing impairments can find it difficult to follow a conversation in a multi-speaker environment. Current hearing aids can suppress background noise; however, there is little that can be done to help a user attend to a single conversation amongst many without knowing which speaker the user is attending to. Cognitively controlled hearing aids that use auditory attention decoding (AAD) methods are the next step in offering help. Translating the successes in AAD research to real-world applications poses a number of challenges, including the lack of access to the clean sound sources in the environment with which to compare with the neural signals. We propose a novel framework that combines single-channel speech separation algorithms with AAD. Approach . We present an end-to-end system that (1) receives a single audio channel containing a mixture of speakers that is heard by a listener along with the listener's neural signals, (2) automatically separates the individual speakers in the mixture, (3) determines the attended speaker, and (4) amplifies the attended speaker's voice to assist the listener. Main results . Using invasive electrophysiology recordings, we identified the regions of the auditory cortex that contribute to AAD. Given appropriate electrode locations, our system is able to decode the attention of subjects and amplify the attended speaker using only the mixed audio. Our quality assessment of the modified audio demonstrates a significant improvement in both subjective and objective speech quality measures. Significance . Our novel framework for AAD bridges the gap between the most recent advancements in speech processing technologies and speech prosthesis research and moves us closer to the development of cognitively controlled hearable devices for the hearing impaired.},
author = {O'Sullivan, James and Chen, Zhuo and Sheth, Sameer A. and McKhann, Guy and Mehta, Ashesh D. and Mesgarani, Nima},
doi = {10.1109/EMBC.2017.8037155},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Sullivan et al. - 2017 - Neural decoding of attentional selection in multi-speaker environments without access to separated sources.pdf:pdf},
isbn = {9781509028092},
issn = {1557170X},
journal = {Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
mendeley-groups = {obama paper,First Exam Citations},
pages = {1644--1647},
pmid = {29060199},
publisher = {IOP Publishing},
title = {{Neural decoding of attentional selection in multi-speaker environments without access to separated sources}},
year = {2017}
}
@article{Canolty2010,
abstract = {Recent studies suggest that cross-frequency coupling (CFC) might play a functional role in neuronal computation, communication and learning. In particular, the strength of phase-amplitude CFC differs across brain areas in a task-relevant manner, changes quickly in response to sensory, motor and cognitive events, and correlates with performance in learning tasks. Importantly, whereas high-frequency brain activity reflects local domains of cortical processing, low-frequency brain rhythms are dynamically entrained across distributed brain regions by both external sensory input and internal cognitive events. CFC might thus serve as a mechanism to transfer information from large-scale brain networks operating at behavioral timescales to the fast, local cortical processing required for effective computation and synaptic modification, thus integrating functional systems across multiple spatiotemporal scales. {\textcopyright} 2010 Elsevier Ltd.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Canolty, Ryan T. and Knight, Robert T.},
doi = {10.1016/j.tics.2010.09.001},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Canolty, Knight - 2010 - The functional role of cross-frequency coupling.pdf:pdf},
isbn = {1364-6613},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
mendeley-groups = {obama paper,First Exam Citations},
number = {11},
pages = {506--515},
pmid = {20932795},
publisher = {Elsevier Ltd},
title = {{The functional role of cross-frequency coupling}},
url = {http://dx.doi.org/10.1016/j.tics.2010.09.001},
volume = {14},
year = {2010}
}
@article{Zoefel2018,
abstract = {Due to their periodic nature, neural oscillations might represent an optimal “tool” for the processing of rhythmic stimulus input [1–3]. Indeed, the alignment of neural oscillations to a rhythmic stimulus, often termed phase entrainment, has been repeatedly demonstrated [4–7]. Phase entrainment is central to current theories of speech processing [8–10] and has been associated with successful speech comprehension [11–17]. However, typical manipulations that reduce speech intelligibility (e.g., addition of noise and time reversal [11, 12, 14, 16, 17]) could destroy critical acoustic cues for entrainment (such as “acoustic edges” [7]). Hence, the association between phase entrainment and speech intelligibility might only be “epiphenomenal”; i.e., both decline due to the same manipulation, without any causal link between the two [18]. Here, we use transcranial alternating current stimulation (tACS [19]) to manipulate the phase lag between neural oscillations and speech rhythm while measuring neural responses to intelligible and unintelligible vocoded stimuli with sparse fMRI. We found that this manipulation significantly modulates the BOLD response to intelligible speech in the superior temporal gyrus, and the strength of BOLD modulation is correlated with a phasic modulation of performance in a behavioral task. Importantly, these findings are absent for unintelligible speech and during sham stimulation; we thus demonstrate that phase entrainment has a specific, causal influence on neural responses to intelligible speech. Our results not only provide an important step toward understanding the neural foundation of human abilities at speech comprehension but also suggest new methods for enhancing speech perception that can be explored in the future. Using simultaneous tACS-fMRI recordings, Zoefel et al. show that the alignment of neural oscillations to stimulus rhythm causally modulates neural responses to speech. The effect is specific for intelligible speech, supporting the notion that neural entrainment is a mechanism tailored to optimize speech processing.},
author = {Zoefel, Benedikt and Archer-Boyd, Alan and Davis, Matthew H.},
doi = {10.1016/j.cub.2017.11.071},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zoefel, Archer-Boyd, Davis - 2018 - Phase Entrainment of Brain Oscillations Causally Modulates Neural Responses to Intelligible Speech.pdf:pdf},
isbn = {1879-0445 (Electronic)
0960-9822 (Linking)},
issn = {09609822},
journal = {Current Biology},
keywords = {BOLD,entrainment,fMRI,intelligibility,oscillation,phase,rhythm,speech,tACS,transcranial alternating current stimulation},
mendeley-groups = {obama paper,First Exam Citations,First Exam Citations/to read},
number = {3},
pages = {401--408.e5},
pmid = {29358073},
publisher = {Elsevier Ltd.},
title = {{Phase Entrainment of Brain Oscillations Causally Modulates Neural Responses to Intelligible Speech}},
url = {https://doi.org/10.1016/j.cub.2017.11.071},
volume = {28},
year = {2018}
}
@misc{Molinaro2018,
abstract = {Cortical oscillations phase-align to the quasi-rhythmic structure of the speech envelope. This speech-brain entrainment has been reported in two frequency bands, i.e., both in the theta band (4 - 8 Hz) and in the delta band ({\textless} 4 Hz). However, it is not clear if these two phenomena reflect passive synchronization of the auditory cortex to the acoustics of the speech input, or if they reflect higher processes involved in actively parsing speech information. Here we report two magnetoencephalography experiments in which we contrasted cortical entrainment to natural speech compared to qualitative different control conditions (Experiment 1: amplitude modulated white-noise; Experiment 2: spectrally rotated speech). We computed the coherence between the oscillatory brain activity and the envelope of the auditory stimuli. At the sensor level, we observed increased coherence for the delta and the theta band for all conditions in bilateral brain regions. However, only in the delta band (but not theta) speech entrainment was stronger than either of the control auditory inputs. Source reconstruction in the delta band showed that speech, compared to the control conditions, elicited larger coherence in the right superior temporal and left inferior frontal regions. In the theta band, no differential effects were observed for the speech compared to the control conditions. These results suggest that whereas theta entrainment mainly reflects perceptual processing of the auditory signal, delta entrainment involves additional higher order computations in the service of language processing. This article is protected by copyright. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:physics/0608246v3},
author = {Molinaro, Nicola and Lizarazu, Mikel},
booktitle = {European Journal of Neuroscience},
doi = {10.1111/ejn.13811},
eprint = {0608246v3},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Molinaro, Lizarazu - 2018 - Delta(but not theta)-band cortical entrainment involves speech-specific processing.pdf:pdf},
isbn = {0000000154871},
issn = {14609568},
keywords = {Coherence,Cortical entrainment,Inferior frontal cortex,Neuronal oscillations,Speech processing},
mendeley-groups = {obama paper,First Exam Citations},
pmid = {29283465},
primaryClass = {arXiv:physics},
title = {{Delta(but not theta)-band cortical entrainment involves speech-specific processing}},
year = {2018}
}
@article{Dai2018,
archivePrefix = {arXiv},
arxivId = {364042},
author = {Dai, Bohan and McQueen, James M. and Terporten, Ren{\'{e}} and Hagoort, Peter and K{\"{o}}sem, Anne},
doi = {10.1101/364042},
eprint = {364042},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dai et al. - 2018 - Distracting Linguistic Information Impairs Neural Entrainment to Attended Speech Bohan.pdf:pdf},
journal = {bioRxiv},
keywords = {Cocktail party,MEG,delta,informational masking,neural oscillations},
mendeley-groups = {First Exam Citations},
title = {{Distracting Linguistic Information Impairs Neural Entrainment to Attended Speech Bohan}},
year = {2018}
}
@article{Leonard2016,
abstract = {Humans are adept at understanding speech despite the fact that our natural listening environment is often filled with interference. An example of this capacity is phoneme restoration, in which part of a word is completely replaced by noise, yet listeners report hearing the whole word. The neurological basis for this unconscious fill-in phenomenon is unknown, despite being a fundamental characteristic of human hearing. Here, using direct cortical recordings in humans, we demonstrate that missing speech is restored at the acoustic-phonetic level in bilateral auditory cortex, in real-time. This restoration is preceded by specific neural activity patterns in a separate language area, left frontal cortex, which predicts the word that participants later report hearing. These results demonstrate that during speech perception, missing acoustic content is synthesized online from the integration of incoming sensory cues and the internal neural dynamics that bias word-level expectation and prediction. 1Department},
author = {Leonard, Matthew K. and Baud, Maxime O. and Sjerps, Matthias J. and Chang, Edward F.},
doi = {10.1038/ncomms13619},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Leonard et al. - 2016 - Perceptual restoration of masked speech in human cortex.pdf:pdf},
isbn = {2041-1723 (Electronic) 2041-1723 (Linking)},
issn = {2041-1723},
journal = {Nature Communications},
mendeley-groups = {First Exam Citations},
month = {dec},
pages = {13619},
pmid = {27996973},
publisher = {Nature Publishing Group},
title = {{Perceptual restoration of masked speech in human cortex}},
url = {http://dx.doi.org/10.1038/ncomms13619 http://www.nature.com/doifinder/10.1038/ncomms13619},
volume = {7},
year = {2016}
}
@article{Pernet2015,
abstract = {fMRI studies increasingly examine functions and properties of non-primary areas of human auditory cortex. However there is currently no standardized localization procedure to reliably identify specific areas across individuals such as the standard 'localizers' available in the visual domain. Here we present an fMRI 'voice localizer' scan allowing rapid and reliable localization of the voice-sensitive 'temporal voice areas' (TVA) of human auditory cortex. We describe results obtained using this standardized localizer scan in a large cohort of normal adult subjects. Most participants (94{\%}) showed bilateral patches of significantly greater response to vocal than non-vocal sounds along the superior temporal sulcus/gyrus (STS/STG). Individual activation patterns, although reproducible, showed high inter-individual variability in precise anatomical location. Cluster analysis of individual peaks from the large cohort highlighted three bilateral clusters of voice-sensitivity, or "voice patches" along posterior (TVAp), mid (TVAm) and anterior (TVAa) STS/STG, respectively. A series of extra-temporal areas including bilateral inferior prefrontal cortex and amygdalae showed small, but reliable voice-sensitivity as part of a large-scale cerebral voice network. Stimuli for the voice localizer scan and probabilistic maps in MNI space are available for download.},
author = {Pernet, Cyril R. and McAleer, Phil and Latinus, Marianne and Gorgolewski, Krzysztof J. and Charest, Ian and Bestelmeyer, Patricia E.G. and Watson, Rebecca H. and Fleming, David and Crabbe, Frances and Valdes-Sosa, Mitchell and Belin, Pascal},
doi = {10.1016/j.neuroimage.2015.06.050},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pernet et al. - 2015 - The human voice areas Spatial organization and inter-individual variability in temporal and extra-temporal cortic.pdf:pdf},
isbn = {1095-9572 (Electronic)$\backslash$r1053-8119 (Linking)},
issn = {10959572},
journal = {NeuroImage},
keywords = {Amygdala,Auditory cortex,Functional magnetic resonance imaging,Inferior prefrontal cortex,Superior temporal gyrus,Superior temporal sulcus,Voice},
mendeley-groups = {First Exam Citations},
pages = {164--174},
pmid = {26116964},
publisher = {Elsevier B.V.},
title = {{The human voice areas: Spatial organization and inter-individual variability in temporal and extra-temporal cortices}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2015.06.050},
volume = {119},
year = {2015}
}
@article{Hausfeld2018,
abstract = {Often, in everyday life, we encounter auditory scenes comprising multiple simultaneous sounds and succeed to selectively attend to only one sound, typically the most relevant for ongoing behavior. Studies using basic sounds and two-talker stimuli have shown that auditory selective attention aids this by enhancing the neural representations of the attended sound in auditory cortex. It remains unknown, however, whether and how this selective attention mechanism operates on representations of auditory scenes containing natural sounds of different categories. In this high-field fMRI study we presented participants with simultaneous voices and musical instruments while manipulating their focus of attention. We found an attentional enhancement of neural sound representations in temporal cortex - as defined by spatial activation patterns - at locations that depended on the attended category (i.e., voices or instruments). In contrast, we found that in frontal cortex the site of enhancement was independent of the attended category and the same regions could flexibly represent any attended sound regardless of its category. These results are relevant to elucidate the interacting mechanisms of bottom-up and top-down processing when listening to real-life scenes comprised of multiple sound categories.},
author = {Hausfeld, Lars and Riecke, Lars and Formisano, Elia},
doi = {10.1016/j.neuroimage.2018.02.065},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hausfeld, Riecke, Formisano - 2018 - Acoustic and higher-level representations of naturalistic auditory scenes in human auditory and fro.pdf:pdf},
issn = {10959572},
journal = {NeuroImage},
keywords = {Attention networks,Audition,Auditory scenes,High-field MRI,MVPA,Selective attention},
mendeley-groups = {First Exam Citations},
number = {February},
pages = {472--483},
publisher = {Elsevier Ltd},
title = {{Acoustic and higher-level representations of naturalistic auditory scenes in human auditory and frontal cortex}},
url = {https://doi.org/10.1016/j.neuroimage.2018.02.065},
volume = {173},
year = {2018}
}
@article{Staeren2009,
abstract = {The ability to recognize sounds allows humans and animals to efficiently detect behaviorally relevant events, even in the absence of visual information. Sound recognition in the human brain has been assumed to proceed through several functionally specialized areas, culminating in cortical modules where category-specific processing is carried out [1-5]. In the present high-resolution fMRI experiment, we challenged this model by using well-controlled natural auditory stimuli and by employing an advanced analysis strategy based on an iterative machine-learning algorithm [6] that allows modeling of spatially distributed, as well as localized, response patterns. Sounds of cats, female singers, acoustic guitars, and tones were controlled for their time-varying spectral characteristics and presented to subjects at three different pitch levels. Sound category information-not detectable with conventional contrast-based methods analysis-could be detected with multivoxel pattern analyses and attributed to spatially distributed areas over the supratemporal cortices. A more localized pattern was observed for processing of pitch laterally to primary auditory areas. Our findings indicate that distributed neuronal populations within the human auditory cortices, including areas conventionally associated with lower-level auditory processing, entail categorical representations of sounds beyond their physical properties. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Staeren, No{\"{e}}l and Renvall, Hanna and {De Martino}, Federico and Goebel, Rainer and Formisano, Elia},
doi = {10.1016/j.cub.2009.01.066},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Staeren et al. - 2009 - Sound Categories Are Represented as Distributed Patterns in the Human Auditory Cortex.pdf:pdf},
isbn = {1879-0445 (Electronic)$\backslash$n0960-9822 (Linking)},
issn = {09609822},
journal = {Current Biology},
keywords = {SYSNEURO},
mendeley-groups = {First Exam Citations},
number = {6},
pages = {498--502},
pmid = {19268594},
title = {{Sound Categories Are Represented as Distributed Patterns in the Human Auditory Cortex}},
volume = {19},
year = {2009}
}
@article{Mauk2004,
abstract = {A complete understanding of sensory and motor processing requires characterization of how the nervous system processes time in the range of tens to hundreds of milliseconds (ms). Temporal processing on this scale is required for simple sensory problems, such as interval, duration, and motion discrimination, as well as complex forms of sensory processing, such as speech recognition. Timing is also required for a wide range of motor tasks from eyelid conditioning to playing the piano. Here we review the behavioral, electrophysiological, and theoretical literature on the neural basis of temporal processing. These data suggest that temporal processing is likely to be distributed among different structures, rather than relying on a centralized timing area, as has been suggested in internal clock models. We also discuss whether temporal processing relies on specialized neural mechanisms, which perform temporal computations independent of spatial ones. We suggest that, given the intricate link between temporal and spatial information in most sensory and motor tasks, timing and spatial processing are intrinsic properties of neural function, and specialized timing mechanisms such as delay lines, oscillators, or a spectrum of different time constants are not required. Rather temporal processing may rely on state-dependent changes in network dynamics.},
author = {Mauk, Michael D. and Buonomano, Dean V.},
doi = {10.1146/annurev.neuro.27.070203.144247},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mauk, Buonomano - 2004 - The Neural Basis of Temporal Processing.pdf:pdf},
isbn = {0147-006X},
issn = {0147-006X},
journal = {Annual Review of Neuroscience},
keywords = {cerebellum,characterization of how the,conditioning,cortex,dynamics,hundreds of milliseconds,i abstract a complete,in the range of,interval,motor processing requires,ms,nervous system processes time,scale is required for,simple,temporal processing on this,tens to,timing,understanding of sensory and},
mendeley-groups = {First Exam Citations},
month = {jul},
number = {1},
pages = {307--340},
pmid = {15217335},
title = {{The Neural Basis of Temporal Processing}},
url = {http://www.annualreviews.org/doi/10.1146/annurev.neuro.27.070203.144247},
volume = {27},
year = {2004}
}
@article{Riecke2018,
abstract = {Speech is crucial for communication in everyday life. Speech-brain entrainment, the alignment of neural activity to the slow temporal fluctuations (envelope) of acoustic speech input, is a ubiquitous element of current theories of speech processing. Associations between speech-brain entrainment and acoustic speech signal, listening task, and speech intelligibility have been observed repeatedly. However, a methodological bottleneck has prevented so far clarifying whether speech-brain entrainment contributes functionally to (i.e., causes) speech intelligibility or is merely an epiphenomenon of it. To address this long-standing issue, we experimentally manipulated speech-brain entrainment without concomitant acoustic and task-related variations, using a brain stimulation approach that enables modulating listeners' neural activity with transcranial currents carrying speech-envelope information. Results from two experiments involving a cocktail-party-like scenario and a listening situation devoid of aural speech-amplitude envelope input reveal consistent effects on listeners' speech-recognition performance, demonstrating a causal role of speech-brain entrainment in speech intelligibility. Our findings imply that speech-brain entrainment is critical for auditory speech comprehension and suggest that transcranial stimulation with speech-envelope-shaped currents can be utilized to modulate speech comprehension in impaired listening conditions. Riecke et al. study how humans can recognize speech. Using electric brain stimulation, they find that synchronization of ongoing brain activity with the rhythm of auditory speech modulates the intelligibility of this speech. This implies that the brain can employ its ongoing temporal activity as a critical instrument in speech recognition.},
author = {Riecke, Lars and Formisano, Elia and Sorger, Bettina and Başkent, Deniz and Gaudrain, Etienne},
doi = {10.1016/j.cub.2017.11.033},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Riecke et al. - 2018 - Neural Entrainment to Speech Modulates Speech Intelligibility.pdf:pdf},
issn = {09609822},
journal = {Current Biology},
keywords = {entrainment,envelope,neural oscillation,speech,transcranial current stimulation,transcranial electric stimulation},
mendeley-groups = {First Exam Citations},
number = {2},
pages = {161--169.e5},
pmid = {29290557},
title = {{Neural Entrainment to Speech Modulates Speech Intelligibility}},
volume = {28},
year = {2018}
}
@article{Theunissen2014,
abstract = {We might be forced to listen to a high-frequency tone at our audiologist's office or we might enjoy falling asleep with a white-noise machine, but the sounds that really matter to us are the voices of our companions or music from our favourite radio station. The auditory system has evolved to process behaviourally relevant natural sounds. Research has shown not only that our brain is optimized for natural hearing tasks but also that using natural sounds to probe the auditory system is the best way to understand the neural computations that enable us to comprehend speech or appreciate music.},
author = {Theunissen, Fr{\'{e}}d{\'{e}}ric E. and Elie, Julie E.},
doi = {10.1038/nrn3731},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Theunissen, Elie - 2014 - Neural processing of natural sounds.pdf:pdf},
isbn = {1471-0048 (Electronic)$\backslash$r1471-003X (Linking)},
issn = {14710048},
journal = {Nature Reviews Neuroscience},
mendeley-groups = {First Exam Citations},
number = {6},
pages = {355--366},
pmid = {24840800},
publisher = {Nature Publishing Group},
title = {{Neural processing of natural sounds}},
url = {http://dx.doi.org/10.1038/nrn3731},
volume = {15},
year = {2014}
}
@article{Moerel2014,
abstract = {While advances in magnetic resonance imaging (MRI) throughout the last decades have enabled the detailed anatomical and functional inspection of the human brain non-invasively, to date there is no consensus regarding the precise subdivision and topography of the areas forming the human auditory cortex. Here, we propose a topography of the human auditory areas based on insights on the anatomical and functional properties of human auditory areas as revealed by studies of cyto- and myelo-architecture and fMRI investigations at ultra-high magnetic field (7 Tesla). Importantly, we illustrate that - whereas a group-based approach to analyze functional (tonotopic) maps is appropriate to highlight the main tonotopic axis - the examination of tonotopic maps at single subject level is required to detail the topography of primary and non-primary areas that may be more variable across subjects. Furthermore, we show that considering multiple maps indicative of anatomical (i.e. myelination) as well as of functional properties (e.g. broadness of frequency tuning) is helpful in identifying auditory cortical areas in individual human brains. We propose and discuss a topography of areas that is consistent with old and recent anatomical post mortem characterizations of the human auditory cortex and that may serve as a working model for neuroscience studies of auditory functions.},
author = {Moerel, Michelle and {De Martino}, Federico and Formisano, Elia},
doi = {10.3389/fnins.2014.00225},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moerel, De Martino, Formisano - 2014 - An anatomical and functional topography of human auditory cortical areas.pdf:pdf},
isbn = {1662-453X},
issn = {1662453X},
journal = {Frontiers in Neuroscience},
keywords = {Auditory cortical areas,Cytoarchitectonic parcellation,Human auditory cortex,Tonotopy,Ultra-high field fMRI},
mendeley-groups = {First Exam Citations},
number = {8 JUL},
pages = {1--14},
pmid = {25120426},
title = {{An anatomical and functional topography of human auditory cortical areas}},
volume = {8},
year = {2014}
}
@article{Blanco-Elorrieta2017,
abstract = {For a bilingual human, every utterance requires a choice about which language to use. This choice is commonly regarded as part of general executive control, engaging prefrontal and anterior cingulate cortices similarly to many types of effortful task-switching. However, while language control within artificial switching paradigms has been heavily studied, the neurobiology of natural switching within socially cued situations has not been characterized. Additionally, although theoretical models address how language control mechanisms adapt to the distinct demands of different interactional contexts, these predictions have not been empirically tested. We used MEG (RRID: NIFINV:nlx{\_}inv{\_}090918) to investigate language switching in multiple contexts ranging from completely artificial to the comprehension of a fully natural bilingual conversation recorded "in the wild". Our results showed less anterior cingulate and prefrontal cortex involvement for more natural switching. In production, voluntary switching did not engage the prefrontal cortex nor elicit behavioral switch-costs. In comprehension, while laboratory switches recruited executive control areas, fully natural switching within a conversation only engaged auditory cortices. Multivariate pattern analyses revealed that in production, interlocutor identity was represented in a sustained fashion throughout the different stages of language planning until speech onset. In comprehension, however, a bi-phasic pattern was observed: interlocutor identity was first represented at the presentation of the interlocutor and then again at the presentation of the auditory word. In all, our findings underscore the importance of ecologically valid experimental paradigms and offer the first neurophysiological characterization of language control in a range of situations simulating real life to various degrees.Significant statement: Bilingualism is an inherently social phenomenon, interactional context fully determining language choice. This research addresses the neural mechanisms underlying multilingual individuals' ability to successfully adapt to varying conversational contexts both while speaking and listening. Our results showed that interactional context critically determines language control networks' engagement: switching under external constraints heavily recruited prefrontal control regions while natural, voluntary switching did not. These findings challenge conclusions derived from artificial switching paradigms which suggested that language switching is intrinsically effortful. Further, our results predict that the so-called bilingual advantage should be limited to individuals who need to control their languages according to external cues and thus would not occur by virtue of an experience in which switching is fully free.},
author = {Blanco-Elorrieta, Esti and Pylkk{\"{a}}nen, Liina},
doi = {10.1523/JNEUROSCI.0553-17.2017},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blanco-Elorrieta, Pylkk{\"{a}}nen - 2017 - Bilingual language switching in the lab vs. in the wild The spatio-temporal dynamics of adaptive la.pdf:pdf},
isbn = {1529-2401},
issn = {0270-6474},
journal = {The Journal of Neuroscience},
keywords = {ability to successfully adapt,adaptive cognitive control,bilingualism,bilingualism is an inherently,dresses the neural mechanisms,interactional context fully determining,language choice,language control,language switching,magnetoencephalography,prefrontal cortex,significance statement,social phenomenon,this research ad-,to varying conversational con-,underlying multilingual individuals},
mendeley-groups = {First Exam Citations},
number = {37},
pages = {0553--17},
pmid = {28821648},
title = {{Bilingual language switching in the lab vs. in the wild: The spatio-temporal dynamics of adaptive language control}},
url = {http://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.0553-17.2017},
volume = {37},
year = {2017}
}
@article{Lauter1985,
author = {Lauter, Judith L and Herscovitch, Peter and Formby, Craig and Raichle, Marcus E},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lauter et al. - 1985 - Tonotopic organization in human auditory cortex revealed by positron emission tomography.pdf:pdf},
journal = {Hearing Res},
mendeley-groups = {First Exam Citations},
pages = {199--205},
title = {{Tonotopic organization in human auditory cortex revealed by positron emission tomography}},
volume = {20},
year = {1985}
}
@article{Janata2002,
abstract = {Western tonal music relies on a formal geometric structure that determines distance relationships within a harmonic or tonal space. In functional magnetic resonance imaging experiments, we identified an area in the rostromedial prefrontal cortex that tracks activation in tonal space. Different voxels in this area exhibited selectivity for different keys. Within the same set of consistently activated voxels, the topography of tonality selectivity rearranged itself across scanning sessions. The tonality structure was thus maintained as a dynamic topography in cortical areas known to be at a nexus of cognitive, affective, and mnemonic processing.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Janata, Petr and Birk, Jeffrey L. and {Van Horn}, John D. and Leman, Marc and Tillmann, Barbara and Bharucha, Jamshed J.},
doi = {10.1126/science.1076262},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Janata et al. - 2002 - The cortical topography of tonal structures underlying western music.pdf:pdf},
isbn = {00368075},
issn = {00368075},
journal = {Science},
mendeley-groups = {First Exam Citations},
number = {5601},
pages = {2167--2170},
pmid = {12481131},
title = {{The cortical topography of tonal structures underlying western music}},
volume = {298},
year = {2002}
}
@article{Tallon-Baudry1999,
abstract = {We experience objects as whole, complete entities irrespective of whether they are perceived by our sensory systems or are recalled from memory. However, it is also known that many of the properties of objects are encoded and processed in different areas of the brain. How then, do coherent representations emerge? One theory suggests that rhythmic synchronization of neural discharges in the gamma band (around 40 Hz) may provide the necessary spatial and temporal links that bind together the processing in different brain areas to build a coherent percept. In this article we propose that this mechanism could also be used more generally for the construction of object representations that are driven by sensory input or internal, top-down processes. The review will focus on the literature on gamma oscillatory activities in humans and will describe the different types of gamma responses and how to analyze them. Converging evidence that suggests that one particular type of gamma activity (induced gamma activity) is observed during the construction of an object representation will be discussed.},
author = {Tallon-Baudry and Bertrand},
doi = {10.1016/S1364-6613(99)01299-1},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tallon-Baudry, Bertrand - 1999 - Oscillatory gamma activity in humans and its role in object representation.pdf:pdf},
isbn = {1879-307X (Electronic)$\backslash$n1364-6613 (Linking)},
issn = {1879-307X},
journal = {Trends in Cognitive Sciences},
mendeley-groups = {First Exam Citations},
number = {4},
pages = {151--162},
pmid = {10322469},
title = {{Oscillatory gamma activity in humans and its role in object representation}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10322469},
volume = {3},
year = {1999}
}
@article{Krizman2010,
abstract = {Many sounds in the environment, including speech, are temporally dynamic. The auditory brainstem is exquisitely sensitive to temporal features of the incoming acoustic stream, and by varying the speed of presentation of these auditory signals it is possible to investigate the precision with which temporal cues are represented at a subcortical level. Therefore, to determine the effects of stimulation rate on the auditory brainstem response (ABR), we recorded evoked responses to both a click and a consonant-vowel speech syllable (/da/) presented at three rates (15.4, 10.9 and 6.9 Hz). We hypothesized that stimulus rate affects the onset to speech-evoked responses to a greater extent than click-evoked responses and that subcomponents of the speech- ABR are distinctively affected. While the click response was invariant with changes in stimulus rate, timing of the onset response to /da/ varied systematically, increasing in peak latency as presentation rate increased. Contrasts between the click- and speech-evoked onset responses likely reflect acoustic differences, where the speech stimulus onset is more gradual, has more delineated spectral information, and is more susceptible to backward masking by the subsequent formant transition. The frequency-following response (FFR) was also rate dependent, with response magnitude of the higher frequencies ({\textgreater}400 Hz), but not the frequencies corresponding to the fundamental frequency, diminishing with increasing rate. The selective impact of rate on high-frequency components of the FFR implicates the involvement of distinct underlying neural mechanisms for high- versus low-frequency components of the response. Furthermore, the different rate sensitivities of the speech-evoked onset response and subcomponents of the FFR support the involvement of different neural streams for these two responses. Taken together, these differential effects of rate on the ABR components likely reflect distinct aspects of auditory function such that varying rate of presentation of complex stimuli may be expected to elicit unique patterns of abnormality, depending on the clinical population.},
author = {Krizman, Jennifer and Skoe, Erika and Kraus, Nina},
doi = {10.1159/000289572},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizman, Skoe, Kraus - 2010 - Stimulus rate and subcortical auditory processing of speech.pdf:pdf},
isbn = {1420-3030},
issn = {14203030},
journal = {Audiology and Neurotology},
keywords = {Auditory brainstem response,Brainstem,Frequency-following response,Stimulation rate,Subcortical auditory structures},
mendeley-groups = {First Exam Citations},
number = {5},
pages = {332--342},
pmid = {20215743},
title = {{Stimulus rate and subcortical auditory processing of speech}},
volume = {15},
year = {2010}
}
@article{Ghitza2017,
abstract = {ABSTRACTOscillation-based models of speech perception postulate a cortical computation principle by which decoding is performed within a time-varying window structure, synchronised with the input on multiple time scales. The windows are generated by a segmentation process, implemented by a cascade of oscillators. This paper tests the hypothesis that prosodic segmentation is driven by a ?flexible? (in contrast to autonomous, ?rigid?) oscillator in the delta range (0.5?3 Hz) by tracking prosodic rhythms, such that intelligibility is impaired when the ability of this oscillator to synchronise to these rhythms is impaired. In setting phrasal boundaries, both bottom-up acoustic-driven and top-down context-invoked processes interact in a manner that is difficult to decompose. The present experiments used context-free random-digit strings in order to focus exclusively on bottom-up processes. Two experiments are reported. Listeners performed a target identification task, listening to stimuli with prescribed chunking patterns (Experiment I) or chunking rates (Experiment II), followed by a target. Irrespective of the chunking pattern, performance is high only for targets inside of a chunk, pointing to the benefit of acoustic prosodic segmentation in digit retrieval. Importantly, performance remains high as long as the chunking rate is within the frequency range of neuronal delta, but sharply deteriorates for higher rates. This data provides psychophysical evidence for the role of acoustic-driven segmentation, with flexible delta oscillations at the core, in digit retrieval.},
author = {Ghitza, Oded},
doi = {10.1080/23273798.2016.1232419},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghitza - 2017 - Acoustic-driven delta rhythms as prosodic markers.pdf:pdf},
isbn = {2327-3798},
issn = {23273801},
journal = {Language, Cognition and Neuroscience},
keywords = {Prosodic segmentation,chunking,delta rhythms,synchronisation,word retrieval},
mendeley-groups = {First Exam Citations/Ch4},
number = {5},
pages = {545--561},
publisher = {Taylor {\&} Francis},
title = {{Acoustic-driven delta rhythms as prosodic markers}},
url = {http://dx.doi.org/10.1080/23273798.2016.1232419},
volume = {32},
year = {2017}
}
@article{Bosker2018,
abstract = {This psychoacoustic study provides behavioural evidence that neural entrainment in the theta range (3–9 Hz) causally shapes speech perception. Adopting the “rate normalization” paradigm (presenting compressed carrier sentences followed by uncompressed target words), we show that uniform compression of a speech carrier to syllable rates inside the theta range influences perception of subsequent uncompressed targets, but compression outside theta range does not. However, the influence of carriers – compressed outside theta range – on target perception is salvaged when carriers are “repackaged” to have a packet rate inside theta. This suggests that the brain can only successfully entrain to syllable/packet rates within theta range, with a causal influence on the perception of subsequent speech, in line with recent neuroimaging data. Thus, this study points to a central role for sustained theta entrainment in rate normalisation and contributes to our understanding of the functional role of brain oscillations in speech perception.},
author = {Bosker, Hans Rutger and Ghitza, Oded},
doi = {10.1080/23273798.2018.1439179},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bosker, Ghitza - 2018 - Entrained theta oscillations guide perception of subsequent speech behavioural evidence from rate normalisation.pdf:pdf},
issn = {23273801},
journal = {Language, Cognition and Neuroscience},
keywords = {Neural entrainment,rate normalisation,speech rate,theta oscillations},
mendeley-groups = {First Exam Citations/Ch4,obama paper},
number = {0},
pages = {1--13},
publisher = {Taylor {\&} Francis},
title = {{Entrained theta oscillations guide perception of subsequent speech: behavioural evidence from rate normalisation}},
url = {https://doi.org/10.1080/23273798.2018.1439179},
volume = {0},
year = {2018}
}
@article{Ghitza2014,
abstract = {Studies on the intelligibility of time-compressed speech have shown flawless performance for moderate compression factors, a sharp deterioration for compression factors above three, and an improved performance as a result of "repackaging"-a process of dividing the time-compressed waveform into fragments, called packets, and delivering the packets in a prescribed rate. This intricate pattern of performance reflects the reliability of the auditory system in processing speech streams with different information transfer rates; the knee-point of performance defines the auditory channel capacity. This study is concerned with the cortical computation principle that determines channel capacity. Oscillation-based models of speech perception hypothesize that the speech decoding process is guided by a cascade of oscillations with theta as "master," capable of tracking the input rhythm, with the theta cycles aligned with the intervocalic speech fragments termed $\theta$-syllables; intelligibility remains high as long as theta is in sync with the input, and it sharply deteriorates once theta is out of sync. In the study described here the hypothesized role of theta was examined by measuring the auditory channel capacity of time-compressed speech undergone repackaging. For all speech speeds tested (with compression factors of up to eight), packaging rate at capacity equals 9 packets/s-aligned with the upper limit of cortical theta, $\theta$max (about 9 Hz)-and the packet duration equals the duration of one uncompressed $\theta$-syllable divided by the compression factor. The alignment of both the packaging rate and the packet duration with properties of cortical theta suggests that the auditory channel capacity is determined by theta. Irrespective of speech speed, the maximum information transfer rate through the auditory channel is the information in one uncompressed $\theta$-syllable long speech fragment per one $\theta$max cycle. Equivalently, the auditory channel capacity is 9 $\theta$-syllables/s.},
author = {Ghitza, Oded},
doi = {10.3389/fpsyg.2014.00652},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghitza - 2014 - Behavioral evidence for the role of cortical $\Theta$ oscillations in determining auditory channel capacity for speech.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Auditory channel capacity,Brain rhythms,Fast speech,Information transfer rate,Intelligibility,Phonetic variability,Theta oscillations},
mendeley-groups = {First Exam Citations/Ch4},
number = {JUL},
pages = {1--12},
pmid = {25071631},
title = {{Behavioral evidence for the role of cortical $\Theta$ oscillations in determining auditory channel capacity for speech}},
volume = {5},
year = {2014}
}
@article{Lakatos2005,
abstract = {EEG oscillations are hypothesized to reflect cyclical variations in the neuronal excitability, with particular frequency bands reflecting differing spatial scales of brain operation. However, despite decades of clinical and scientific investigation, there is no unifying theory of EEG organization, and the role of ongoing activity in sensory processing remains controversial. This study analyzed laminar profiles of synaptic activity [current source density CSD] and multiunit activity (MUA), both spontaneous and stimulus-driven, in primary auditory cortex of awake macaque monkeys. Our results reveal that the EEG is hierarchically organized; delta (1-4 Hz) phase modulates theta (4-10 Hz) amplitude, and theta phase modulates gamma (30-50 Hz) amplitude. This oscillatory hierarchy controls baseline excitability and thus stimulus-related responses in a neuronal ensemble. We propose that the hierarchical organization of ambient oscillatory activity allows auditory cortex to structure its temporal activity pattern so as to optimize the processing of rhythmic inputs.},
author = {Lakatos, Peter and Shah, Ankoor S and Knuth, Kevin H and Ulbert, Istvan and Karmos, George and Schroeder, Charles E},
doi = {10.1152/jn.00263.2005},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lakatos et al. - 2005 - An Oscillatory Hierarchy Controlling Neuronal Excitability and Stimulus Processing in the Auditory Cortex An Osc.pdf:pdf},
isbn = {1855618567},
issn = {0022-3077},
journal = {Journal of neurophysiology},
keywords = {Acoustic Stimulation,Acoustic Stimulation: methods,Animals,Auditory,Auditory Cortex,Auditory Cortex: cytology,Auditory Cortex: physiology,Auditory Perception,Auditory Perception: physiology,Auditory: physiology,Brain Mapping,Cortical Synchronization,Evoked Potentials,Macaca mulatta,Male,Neurons,Neurons: physiology,Spectrum Analysis,Time Factors},
mendeley-groups = {First Exam Citations/Ch4},
pages = {1904--1911},
pmid = {15901760},
title = {{An Oscillatory Hierarchy Controlling Neuronal Excitability and Stimulus Processing in the Auditory Cortex An Oscillatory Hierarchy Controlling Neuronal Excitability and Stimulus Processing in the Auditory Cortex}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15901760},
volume = {94},
year = {2005}
}
@article{Sams1991,
abstract = {Neuromagnetic responses were recorded over the left hemisphere to find out in which cortical area the heard and seen speech are integrated. Auditory stimuli were Finnish /pa/ syllables presented together with a videotaped face articulating either the concordant syllable /pa/ (84{\%} of stimuli, V = A) or the discordant syllable /ka/ (16{\%}, V ≠ A). In some subjects the probabilities were reversed. The subjects heard V ≠ A stimuli as /ta/ or /ka/. The magnetic responses to infrequent perceptions elicited a specific waveform which could be explained by activity in the supratemporal auditory cortex. The results show that visual information from articulatory movements has an entry into the auditory cortex. {\textcopyright} 1991.},
author = {Sams, Mikko and Aulanko, Reijo and H{\"{a}}m{\"{a}}l{\"{a}}inen, Matti and Hari, Riitta and Lounasmaa, Olli V. and Lu, Sing Teh and Simola, Juha},
doi = {10.1016/0304-3940(91)90914-F},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sams et al. - 1991 - Seeing speech visual information from lip movements modifies activity in the human auditory cortex.pdf:pdf},
isbn = {0304-3940 (Print)$\backslash$r0304-3940 (Linking)},
issn = {03043940},
journal = {Neuroscience Letters},
keywords = {Audio-visual interaction,Audition,Auditory cortex,Evoked responses,Intersensory convergence,MEG,Man,Neuromagnetism,Speech perception},
mendeley-groups = {First Exam Citations/Ch4},
number = {1},
pages = {141--145},
pmid = {1881611},
title = {{Seeing speech: visual information from lip movements modifies activity in the human auditory cortex}},
volume = {127},
year = {1991}
}
@article{Zatorre2001,
abstract = {We used positron emission tomography to examine the response of human auditory cortex to spectral and temporal variation. Volunteers listened to sequences derived from a standard stimulus, consisting of two pure tones separated by one octave alternating with a random duty cycle. In one series of five scans, spectral information (tone spacing) remained constant while speed of alternation was doubled at each level. In another five scans, speed was kept constant while the number of tones sampled within the octave was doubled at each level, resulting in increasingly fine frequency differences. Results indicated that (i) the core auditory cortex in both hemispheres responded to temporal variation, while the anterior superior temporal areas bilaterally responded to the spectral variation; and (ii) responses to the temporal features were weighted towards the left, while responses to the spectral features were weighted towards the right. These findings confirm the specialization of the left-hemisphere auditory cortex for rapid temporal processing, and indicate that core areas are especially involved in these processes. The results also indicate a complementary hemispheric specialization in right-hemisphere belt cortical areas for spectral processing. The data provide a unifying framework to explain hemispheric asymmetries in processing speech and tonal patterns. We propose that differences exist in the temporal and spectral resolution of corresponding fields in the two hemispheres, and that they may be related to anatomical hemispheric asymmetries in myelination and spacing of cortical columns.},
author = {Zatorre, R J and Belin, P},
doi = {10.1093/cercor/11.10.946},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zatorre, Belin - 2001 - Spectral and temporal processing in human auditory cortex.pdf:pdf},
isbn = {1047-3211 (Print)},
issn = {1047-3211},
journal = {Cerebral cortex (New York, N.Y. : 1991)},
keywords = {Acoustic Stimulation,Auditory Cortex,Auditory Cortex: physiology,Auditory Cortex: radionuclide imaging,Auditory Perception,Auditory Perception: physiology,Dominance, Cerebral,Dominance, Cerebral: physiology,Female,Humans,Male,Tomography, Emission-Computed},
mendeley-groups = {First Exam Citations/Ch4},
number = {10},
pages = {946--53},
pmid = {11549617},
title = {{Spectral and temporal processing in human auditory cortex.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11549617},
volume = {11},
year = {2001}
}
@article{Patterson2002,
abstract = {An fMRI experiment was performed to identify the main stages of melody processing in the auditory pathway. Spectrally matched sounds that produce no pitch, fixed pitch, or melody were all found to activate Heschl's gyrus (HG) and planum temporale (PT). Within this region, sounds with pitch produced more activation than those without pitch only in the lateral half of HG. When the pitch was varied to produce a melody, there was activation in regions beyond HG and PT, specifically in the superior temporal gyrus (STG) and planum polare (PP). The results support the view that there is hierarchy of pitch processing in which the center of activity moves anterolaterally away from primary auditory cortex as the processing of melodic sounds proceeds.},
author = {Patterson, Roy D. and Uppenkamp, Stefan and Johnsrude, Ingrid S. and Griffiths, Timothy D.},
doi = {10.1016/S0896-6273(02)01060-7},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Patterson et al. - 2002 - The processing of temporal pitch and melody information in auditory cortex.pdf:pdf},
isbn = {0896-6273 (Print)},
issn = {08966273},
journal = {Neuron},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch4},
number = {4},
pages = {767--776},
pmid = {12441063},
title = {{The processing of temporal pitch and melody information in auditory cortex}},
volume = {36},
year = {2002}
}
@article{Friederici2011,
abstract = {Language processing is a trait of human species. The knowledge about its neurobiological basis has been increased considerably over the past decades. Different brain regions in the left and right hemisphere have been identified to support particular language functions. Networks involving the temporal cortex and the inferior frontal cortex with a clear left lateralization were shown to support syntactic processes, whereas less lateralized temporo-frontal networks subserve semantic processes. These networks have been substantiated both by functional as well as by structural connectivity data. Electrophysiological measures indicate that within these networks syntactic processes of local structure building precede the assignment of grammatical and semantic relations in a sentence. Suprasegmental prosodic information overtly available in the acoustic language input is processed predominantly in a temporo-frontal network in the right hemisphere associated with a clear electrophysiological marker. Studies with patients suffering from lesions in the corpus callosum reveal that the posterior portion of this structure plays a crucial role in the interaction of syntactic and prosodic information during language processing.},
author = {Friederici, A. D.},
doi = {10.1152/physrev.00006.2011},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Friederici - 2011 - The Brain Basis of Language Processing From Structure to Function.pdf:pdf},
issn = {0031-9333},
journal = {Physiological Reviews},
mendeley-groups = {First Exam Citations/Ch4},
number = {4},
pages = {1357--1392},
pmid = {22013214},
title = {{The Brain Basis of Language Processing: From Structure to Function}},
url = {http://physrev.physiology.org/cgi/doi/10.1152/physrev.00006.2011},
volume = {91},
year = {2011}
}
@article{Belin2000,
abstract = {The human voice contains in its acoustic structure a wealth of information on the speaker's identity and emotional state which we perceive with remarkable ease and accuracy. Although the perception of speaker-related features of voice plays a major role in human communication, little is known about its neural basis. Here we show, using functional magnetic resonance imaging in human volunteers, that voice-selective regions can be found bilaterally along the upper bank of the superior temporal sulcus (STS). These regions showed greater neuronal activity when subjects listened passively to vocal sounds, whether speech or non-speech, than to non-vocal environmental sounds. Central STS regions also displayed a high degree of selectivity by responding significantly more to vocal sounds than to matched control stimuli, including scrambled voices and amplitude-modulated noise. Moreover, their response to stimuli degraded by frequency filtering paralleled the subjects' behavioural performance in voice-perception tasks that used these stimuli. The voice-selective areas in the STS may represent the counterpart of the face-selective areas in human visual cortex; their existence sheds new light on the functional architecture of the human auditory cortex.},
author = {Belin, P and Zatorre, R J and Lafaille, P and Ahad, P and Pike, B},
doi = {10.1038/35002078},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Belin et al. - 2000 - Voice-selective areas in human auditory cortex.pdf:pdf},
isbn = {0028-0836 (Print)},
issn = {0028-0836},
journal = {Nature},
mendeley-groups = {First Exam Citations/Ch4},
number = {6767},
pages = {309--312},
pmid = {10659849},
title = {{Voice-selective areas in human auditory cortex.}},
volume = {403},
year = {2000}
}
@article{Zatorre2002,
abstract = {We examine the evidence that speech and musical sounds exploit different acoustic cues: speech is highly dependent on rapidly changing broadband sounds, whereas tonal patterns tend to be slower, although small and precise changes in frequency are important. We argue that the auditory cortices in the two hemispheres are relatively specialized, such that temporal resolution is better in left auditory cortical areas and spectral resolution is better in right auditory cortical areas. We propose that cortical asymmetries might have developed as a general solution to the need to optimize processing of the acoustic environment in both temporal and frequency domains.},
author = {Zatorre, Robert J. and Belin, Pascal and Penhune, Virginia B.},
doi = {10.1016/S1364-6613(00)01816-7},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zatorre, Belin, Penhune - 2002 - Structure and function of auditory cortex Music and speech.pdf:pdf},
isbn = {1364-6613},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
mendeley-groups = {First Exam Citations/Ch4},
number = {1},
pages = {37--46},
pmid = {11849614},
title = {{Structure and function of auditory cortex: Music and speech}},
volume = {6},
year = {2002}
}
@article{Patel2003,
abstract = {The comparative study of music and language is drawing an increasing amount of research interest. Like language, music is a human universal involving perceptually discrete elements organized into hierarchically structured sequences. Music and language can thus serve as foils for each other in the study of brain mechanisms underlying complex sound processing, and comparative research can provide novel insights into the functional and neural architecture of both domains. This review focuses on syntax, using recent neuroimaging data and cognitive theory to propose a specific point of convergence between syntactic processing in language and music. This leads to testable predictions, including the prediction that that syntactic comprehension problems in Broca's aphasia are not selective to language but influence music perception as well.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Patel, Aniruddh D.},
doi = {10.1038/nn1082},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Patel - 2003 - Language, music, syntax and the brain.pdf:pdf},
isbn = {1097-6256 (Print)$\backslash$n1097-6256 (Linking)},
issn = {10976256},
journal = {Nature Neuroscience},
mendeley-groups = {First Exam Citations/Ch4},
number = {7},
pages = {674--681},
pmid = {12830158},
title = {{Language, music, syntax and the brain}},
volume = {6},
year = {2003}
}
@article{Haegens2018,
abstract = {Here we review the role of brain oscillations in sensory processing. We examine the idea that neural entrainment of intrinsic oscillations underlies the processing of rhythmic stimuli in the context of simple isochronous rhythms as well as in music and speech. This has been a topic of growing interest over recent years; however, many issues remain highly controversial: how do fluctuations of intrinsic neural oscillations—both spontaneous and entrained to external stimuli—affect perception, and does this occur automatically or can it be actively controlled by top-down factors? Some of the controversy in the literature stems from confounding use of terminology. Moreover, it is not straightforward how theories and findings regarding isochronous rhythms generalize to more complex, naturalistic stimuli, such as speech and music. Here we aim to clarify terminology, and distinguish between different phenomena that are often lumped together as reflecting “neural entrainment” but may actually vary in their mechanistic underpinnings. Furthermore, we discuss specific caveats and confounds related to making inferences about oscillatory mechanisms from human electrophysiological data.},
author = {Haegens, Saskia and {Zion Golumbic}, Elana},
doi = {10.1016/j.neubiorev.2017.12.002},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haegens, Zion Golumbic - 2018 - Rhythmic facilitation of sensory processing A critical review.pdf:pdf},
issn = {18737528},
journal = {Neuroscience and Biobehavioral Reviews},
keywords = {Entrainment,Music,Oscillations,Rhythm,Speech,Synchronization},
mendeley-groups = {First Exam Citations/Ch4,obama paper},
number = {December 2017},
pages = {150--165},
pmid = {29223770},
publisher = {Elsevier},
title = {{Rhythmic facilitation of sensory processing: A critical review}},
url = {https://doi.org/10.1016/j.neubiorev.2017.12.002},
volume = {86},
year = {2018}
}
@article{Cummins2012,
author = {Cummins, Fred},
doi = {10.3389/fpsyg.2012.00364},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cummins - 2012 - Oscillators and syllables A cautionary note.pdf:pdf},
isbn = {1664-1078},
issn = {16641078},
journal = {Frontiers in Psychology},
mendeley-groups = {First Exam Citations/Ch4},
number = {OCT},
pages = {1--2},
pmid = {23060833},
title = {{Oscillators and syllables: A cautionary note}},
volume = {3},
year = {2012}
}
@article{Buzsaki2011,
author = {Buzs{\'{a}}ki, Gy{\"{o}}rgy and Draguhn, Andreas},
doi = {10.1126/science.1099745},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Buzs{\'{a}}ki, Draguhn - 2011 - Neuronal Oscillations in Cortical Networks.pdf:pdf},
isbn = {1095-9203 (Electronic){\$}\backslash{\$}n0036-8075 (Linking)},
issn = {0036-8075},
journal = {American Association for the Advancement of Science},
mendeley-groups = {First Exam Citations/Ch4},
number = {5679},
pages = {1926--1929},
pmid = {15218136},
title = {{Neuronal Oscillations in Cortical Networks}},
url = {http://www.jstor.org/stable/3837193},
volume = {304},
year = {2011}
}
@article{Wang2010,
abstract = {In our multisensory environment our sensory systems are continuously receiving information that is often interrelated and must be integrated. Recent work in animals and humans has demonstrated that input to one sensory modality can reset the phase of ambient cortical oscillatory activity in another. The periodic fluctuations in neuronal excitability reflected in these oscillations can thereby be aligned to forthcoming anticipated sensory input. In the auditory domain, the example par excellence is speech, because of its inherently rhythmic structure. In contrast, fluctuations of oscillatory phase in the visual system are argued to reflect periodic sampling of the environment. Thus rhythmic structure is imposed on, rather than extracted from, the visual sensory input. Given this distinction, we suggest that cross-modal phase reset subserves separate functions in the auditory and visual systems. We propose a modality-dependent role for cross-modal input in temporal prediction whereby an auditory event signals the visual system to look now, but a visual event signals the auditory system that it needs to hear what is coming.This article is part of a Special Issue entitled {\textless}Human Auditory Neuroimaging{\textgreater}.{\textcopyright} 2013 Elsevier B.V.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Wang, Xiao-Jing},
doi = {10.1152/physrev.00035.2008},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang - 2010 - Neurophysiological and Computational Principles of Cortical Rhythms in Cognition.pdf:pdf},
isbn = {1097-4199 (Electronic)$\backslash$r0896-6273 (Linking)},
issn = {0031-9333},
journal = {Physiological Reviews},
keywords = {*Electrophysiology,*Neurons,*Sensory Integration,15,1989,2001b,2007,2014,27,3-dione,3-dione: pharmacology,4093,4100,6-Cyano-7-nitroquinoxaline-2,AMPA,AMPA: antagonists {\&} inhibitors,AMPA: metabolism,Accuracy,Acoustic Stimulation,Acoustic Stimulation/*methods,Acoustic Stimulation: methods,Action Potentials,Action Potentials: physiology,Adult,Afferent Pathways,Afferent Pathways: physiology,Affine transformation,Algorithms,Analysis of Variance,Animals,Area 17,Area 18,Area CM,Arousal,Arousal: physiology,Association cortex,Asynchrony,Attention,Attention/physiology,Attention: physiology,Audio-visual,Audiovisual,Audition,Auditory,Auditory Cortex,Auditory Cortex/*physiology,Auditory Cortex: physiology,Auditory Perception,Auditory Perception/*physiology,Auditory Perception: physiology,Auditory cortex,Auditory: physiology,Automated,Automaticity,BOLD,Bayes Theorem,Beta Gamma,Beta Rhythm,Bicuculline methiodide,Bimodal,Bimodal neuron,Binding,Brain,Brain Injuries,Brain Injuries: pathology,Brain Mapping,Brain Mapping/methods,Brain Mapping: methods,Brain Physiology. Oscillations. Biological rhythms,Brain oscillations,Brain: cytology,Brain: physiology,Brain: physiopathology,Brain: radionuclide imaging,Cat,Cerebral Cortex,Cerebral Cortex: physiology,Cerebral cortex,Choice Behavior/physiology,Coloring Agents,Coloring Agents: analysis,Complications,Computation,Computer-Assisted,Connections,Connectivity,Conscious recollection,Consolidation,Contrast sensitivity,Cortex,Cortical Synchronization,Cortical Synchronization: physiology,Cross-frequency coupling,Cross-modal,Crossmodal,Cues,Current Source Density,Current source density (CSD),Delta rhythm,Detection,EEG,ERP,Electric Stimulation,Electric Stimulation: methods,Electricity,Electro-corticography (ECoG),Electrocorticography,Electrocorticography (ECoG),Electrode localization,Electrodes,Electroencephalography,Electroencephalography: methods,Electroencephalography: statistics {\&} numerical dat,Electrophysiology,Emission-Computed,Endogenous,Entorhinal Cortex,Entorhinal Cortex: physiology,Epilepsy,Epilepsy surgery,Epilepsy: pathology,Epilepsy: physiopathology,Event-related potential (ERP),Evoked Potentials,Evoked Potentials: physiology,Evoked response,Excitation,Excitatory Amino Acid Antagonists,Excitatory Amino Acid Antagonists: pharmacology,Exogenous,Extra striate visual cortex,FMRI,Face,Facial Expression,Fano factor,Feedback,Feedback-feedforward,Female,Ferret,Field potential,Functional Laterality,Functional Laterality: physiology,Functional imaging,Functional magnetic resonance imaging,Functional mapping,Gamma,Gamma rhythm,Gamma-aminobutyric acid,Gamma-band activity,Global optimisation,Glutamate,Hand/physiology,Hearing,Hippocampus,Hippocampus: anatomy {\&} histology,Hippocampus: physiology,Human,Human visual cortex,Humans,Illusions,Illusions: physiology,Illusions: psychology,Image Processing,Image co-registration,Imaging,Implanted,In vivo application accuracy,Information,Inhibition,Injections,Integrate-and-fire network,Integration,Interdependency,Intracranial,Intracranial EEG,Intraoperative imaging,Invasive EEG,Invasive human recordings,Inverse effectiveness,Laura source estimation,Least-Squares Analysis,Light,MEG,MRI,Macaca,Macaca fascicularis,Macaca fascicularis: physiology,Macaca mulatta,Magnetic Resonance Imaging,Magnetic Resonance Imaging: methods,Magnetic resonance spectroscopy,Magnocellular pathway,Male,Memory,Memory systems,Memory/physiology,Mental Processes,Mental Processes: physiology,Microelectrodes,Middle Aged,Mismatch negativity,Models,Monkey,Monte Carlo Method,Motion Perception,Motion Quartet,Motor,Multi-resolution search,Multimodal interaction,Multimodal registration,Multiplex,Multisensory,Multisensory integration,Multisensory perception,N-Methyl-D-Aspartate,N-Methyl-D-Aspartate: antagonists {\&} inh,N-Methyl-D-Aspartate: metabolism,Natural stimuli,Neocortex,Neocortex: anatomy {\&} histology,Neocortex: physiology,Nerve Net,Nerve Net: cytology,Nerve Net: physiology,Neural Pathways,Neural Pathways: physiology,Neural circuits,Neural code,Neuroanatomy,Neurological,Neurons,Neurons: chemistry,Neurons: physiology,Neurophysiology,Neuropsychology {\&} Neurology [2520],New World monkey,New York,Noise,Normal Distribution,Ocular,Ocular: physiology,Orientation,Orientation: physiology,Orienting,Oscillation,Oscillations,Oscillatory,Oscillatory activity,Oscillometry,Oxygen,Oxygen: blood,P3 latency,Parietal Lobe,Parietal Lobe: physiology,Parvocellular pathway,Pattern Recognition,Perception,Perception: physiology,Perceptual enhancement,Periodicity,Phase,Phase resetting,Photic Stimulation,Photic Stimulation: methods,Physical Stimulation,Physiological,Physiological: physiology,Plasticity,Poisson Distribution,Polysensory,Posterior auditory field,Power,Principles of integration,Proprioception,Psychological,Psychological Theory,Psychological: physiology,Psychomotor Performance,Psychomotor Performance: physiology,Psychophysics,Rats,Reaction Time,Reaction Time/*physiology,Reaction Time: physiology,Real-time electrophysiology,Receptors,Redundancy gain,Redundant signals effect (RSE),Reference Values,Regression Analysis,Research,Response time,Retrograde tracers,Reward,Robustness,SYSNEURO,Sensation,Sensation: physiology,Sensory,Sensory Receptor Cells,Sensory Receptor Cells: physiology,Sensory Thresholds,Sensory Thresholds: physiology,Sensory integration,Sensory processing,Signal Detection,Signal Processing,Simple reaction time,Single-trial analyses,Somatosensory,Somatosensory Cortex,Somatosensory Cortex/*cytology/*physiology,Somatosensory Cortex: anatomy {\&} histology,Somatosensory Cortex: blood supply,Somatosensory Cortex: physiology,Somatosensory/physiology,Sound Localization,Sound Localization: physiology,Spatial attention,Species Specificity,Speech,Speech Perception,Speech Perception: physiology,Speech perception,Statistical,Stereoelectroencephalography,Stereotactic electroencephalography (SEEG),Stereotaxy,Stochastic Processes,Structure,Subthreshold facilitation,Superadditivity,Superior colliculus,Superior temporal sulcus,Synchronization,Sysneuro,Tactile,Task Performance and Analysis,Temporal Lobe,Temporal Lobe: physiology,Thalamus,Theta,Time Factors,Time-frequency analysis,Tomography,Touch,Touch/*physiology,Touch: physiology,United States,V1,Valine,Valine: analogs {\&} derivatives,Valine: pharmacology,Vision,Visual,Visual Cortex,Visual Cortex: anatomy {\&} histology,Visual Cortex: physiology,Visual Fields,Visual Pathways,Visual Pathways: physiology,Visual Perception,Visual Perception/*physiology,Visual Perception: physiology,Visual learning,Visual perception,Visual-auditory,Visual/physiology,Visual: drug effects,Visual: physiology,Voice,WGA-HRP,abbreviation,accepted april 15,acop,additive model,all rights reserved,ambiguous perception,amplitude coupling,amplitude envelope correlations,and behaviors,and head,and to amplify the,april 11,area cm,areas,audiovisual,audition,auditory,auditory cortex,been used to help,behaving cat,behavioral,behavioral rhythm,beta,beta oscillations,bimodal,binding,bistable perception,brain rhythms,canonical,cat,characteristic frequency,chronization,cognitive,cognitive neuroscience,cognitive processing,colorectal tumor,computation,computer-aided drug design,connectivity,context,convergence,cortex,cortical,cross-frequency coupling,cross-modal,crossmodal,crossmodal {\'{a}} multimodal interplay,csd,current source density,cyclophilin,default,deficits,divisive normalization,driver and,ecog,eeg,eeg source analysis,electrocorticogram,electrocorticography,electroencephalography,electrophysiological studies,electrophysiology,empirical mode decomposition,en-,entrainment,erp,erps,event-related potential,evoked response,eye contact,eye movement,eyes-re-head,fMRI,fmri,free energy,free energy perturbation,frequency tuning,fries et al,frontal eye field,gamma,gamma band,gamma oscillations,gamma-b,gamma-band frequency,gaze shifts,gaze shifts are coordinated,ghazanfar,gray et al,growing fields in the,hancement,head-re-space,heading,hepatoma,hfo,high frequency oscillation,high frequency oscillations,high gamma,high-frequency,high-frequency activity,hiv,hsv,human,identify sensory and cognitive,immunotherapy,in individuals with schizophrenia,influence our perceptions,insula,interactions between sensory modalities,intracranial,intracranial eeg,intracranial recordings,intrinsic coupling modes,joost x,journal of neuroscience,language,large-scale neuronal networks,lateral belt,laura source estimation,local field,local field potential,local field potential (LFP),looming biases in monkey,macaque,macaque monkey,magnetoencephalography,maier and asif a,mcgurk,memory,microcircuit,mismatch negativity,monkey,motion,motor,motor readiness,movements of the eyes,mst,multimodal integration,multiscale interactions,multisensory,multisensory enhancement,multisensory integration,multisensory interactions,multiunit activity,muscle artifacts,natural sounds,neural model,neural networks,neuronal,neurosciences is the study,of attention,of behaviorally relevant stimuli,of multisensory,oncolytic therapy,one of the fastest,onset latency,optic flow,oscillations,oscillatory activity,p300,perception,phase,phase coding,phase coupling,phase resetting,phase syn-,phase-amplitude coupling,phase-resetting,posteromedial cortex,potential,power,precuing,predictive coding,press on behalf of,principle,psychology,published by oxford university,pulse,quartett,reading,reafference,received february 14,receiver operating characteristic (ROC),recordings have long,redundant signals effect,reference frame,reset {\'{a}} rodent {\'{a}},reverse transcriptase,revised april 14,rhythmic sampling,rse,self-motion,sensorimotor,sensory,sensory processing,single-neuron level,single-trial analyses,single-unit electrophysiology,social cognition,somatosensory,sound,sound-induced flash illusion,spatial,spatial attention,spatial orientation,speech,spike synchrony,spike-field coherence,stimulus detection,stimulus discrimination,stroboscopic alternative motion,super-additivity,superior colliculus,superior temporal,superior temporal sulcus,suppression,synchronization,synchrony,systems,systems neuroscience,target selec-,temporal measure,that rapidly reorient the,the guarantors of brain,the spatial rule,theta band,theta oscillations,they may affect shifts,through analyses of,time-frequency analysis,tion,tonotopy,touch,tpo,translation,vestibular,vision,visual,visual axis,visual cortex,visual discrimination,visual dominance,visual fixation,visual motion,{\ss} the author,{\'{a}} phase},
mendeley-groups = {First Exam Citations/Ch4},
month = {jul},
number = {3},
pages = {1195--1268},
pmid = {23856236},
title = {{Neurophysiological and Computational Principles of Cortical Rhythms in Cognition}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23177956{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/10944237{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pmc/articles/PMC16941/pdf/pq009777.pdf{\%}5Cnhttp://dx.doi.org/10.1016/j.neuroimage.2012.06.039{\%}5Cnhttp://www.pubmedcentral.nih.gov/arti},
volume = {90},
year = {2010}
}
@article{Muresan2008,
abstract = {We present a method that estimates the strength of neuronal oscillations at the cellular level, relying on autocorrelation histograms computed on spike trains. The method delivers a number, termed oscillation score, that estimates the degree to which a neuron is oscillating in a given frequency band. Moreover, it can also reliably identify the oscillation frequency and strength in the given band, independently of the oscillation in other frequency bands, and thus it can handle superimposed oscillations on multiple scales (theta, alpha, beta, gamma, etc.). The method is relatively simple and fast. It can cope with a low number of spikes, converging exponentially fast with the number of spikes, to a stable estimation of the oscillation strength. It thus lends itself to the analysis of spike-sorted single-unit activity from electrophysiological recordings. We show that the method performs well on experimental data recorded from cat visual cortex and also compares favorably to other methods. In addition, we provide a measure, termed confidence score, that determines the stability of the oscillation score estimate over trials.},
author = {Muresan, R. C. and Jurjut, O. F. and Moca, V. V. and Singer, W. and Nikolic, D.},
doi = {10.1152/jn.00772.2007},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Muresan et al. - 2008 - The Oscillation Score An Efficient Method for Estimating Oscillation Strength in Neuronal Activity.pdf:pdf},
issn = {0022-3077},
journal = {Journal of Neurophysiology},
mendeley-groups = {First Exam Citations/Ch4},
number = {3},
pages = {1333--1353},
pmid = {18160427},
title = {{The Oscillation Score: An Efficient Method for Estimating Oscillation Strength in Neuronal Activity}},
url = {http://jn.physiology.org/cgi/doi/10.1152/jn.00772.2007},
volume = {99},
year = {2008}
}
@article{Arnal2012,
abstract = {Many theories of perception are anchored in the central notion that the brain continuously updates an internal model of the world to infer the probable causes of sensory events. In this framework, the brain needs not only to predict the causes of sensory input, but also when they are most likely to happen. In this article, we review the neurophysiological bases of sensory predictions of "what' (predictive coding) and 'when' (predictive timing), with an emphasis on low-level oscillatory mechanisms. We argue that neural rhythms offer distinct and adapted computational solutions to predicting 'what' is going to happen in the sensory environment and 'when'. {\textcopyright} 2012 Elsevier Ltd.},
author = {Arnal, Luc H. and Giraud, Anne Lise},
doi = {10.1016/j.tics.2012.05.003},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arnal, Giraud - 2012 - Cortical oscillations and sensory predictions.pdf:pdf},
isbn = {1364-6613},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
mendeley-groups = {First Exam Citations/Ch4,obama paper},
number = {7},
pages = {390--398},
pmid = {22682813},
publisher = {Elsevier Ltd},
title = {{Cortical oscillations and sensory predictions}},
url = {http://dx.doi.org/10.1016/j.tics.2012.05.003},
volume = {16},
year = {2012}
}
@article{Rangaswamy2002,
abstract = {Background: In this study, the magnitude and spatial distribution of beta power in the resting electroencephalogram (EEG) were examined to address the possibility of an excitation-inhibition imbalance in the central nervous system of alcoholics. Methods: Log transformed absolute power in the Beta 1 (12.5-16 Hz), Beta 2 (16.5-20 Hz), and Beta 3 (20.5-28 Hz) bands in the eyes-closed EEG of 307 alcohol-dependent subjects and 307 unaffected age- and gender-matched control subjects were compared using a multivariate repeated measures design. Effect of gender, age, and drinking variables was examined separately. Results: Increased Beta 1 (12.5-16 Hz) and Beta 2 (16.5-20 Hz) absolute power was observed in alcohol-dependent subjects at all loci over the scalp. The increase was most prominent in the central region. Increased Beta 3 (20.5-28 Hz) power was frontal in the alcoholics. Age and clinical variables did not influence the increase. Male alcoholics had significantly higher beta power in all three bands. In female alcoholics the increase did not reach statistical significance. Conclusions: Beta power in all three bands of resting EEG is elevated in alcoholics. This feature is more prominent in male alcoholics. The increased beta power in the resting EEG may be an electrophysiological index of the imbalance in the excitation-inhibition homeostasis in the cortex. {\textcopyright} 2002 Society of Biological Psychiatry.},
author = {Rangaswamy, Madhavi and Porjesz, Bernice and Chorlian, David B. and Wang, Kongming and Jones, Kevin A. and Bauer, Lance O and Rohrbaugh, John and O'Connor, Sean J and Kuperman, Samuel and Reich, Theodore and Begleiter, Henri},
doi = {10.1016/S0006-3223(02)01362-8},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rangaswamy et al. - 2002 - Beta power in the EEG of alcoholics.pdf:pdf},
issn = {00063223},
journal = {Biological Psychiatry},
keywords = {Absolute Power,Alcoholism,EEG,Theta},
mendeley-groups = {First Exam Citations/Ch4},
number = {8},
pages = {831--842},
pmid = {12711923},
title = {{Beta power in the EEG of alcoholics}},
url = {http://doi.wiley.com/10.1097/01.ALC.0000060523.95470.8F http://linkinghub.elsevier.com/retrieve/pii/S0006322302013628},
volume = {52},
year = {2002}
}
@article{Michel1992,
abstract = {FFT dipole approximation and 3-dimensional dipole modelling were used to determine the locations of the equivalent dipole model sources of the delta, theta, alpha, beta-1 and beta-2 frequency bands in 13 normal subjects during resting. From each subject, 2 successive data sets were analysed, each consisting of 10 epochs of 2 sec randomly collected during 30 min. ANOVAs showed that over subjects, the source locations of EEG frequency bands differed significantly in the vertical and antero-posterior dimensions. Results of data set 2 confirmed those of data set 1. The source of delta was deepest and most anterior, theta more posterior and less deep, alpha most posterior and highest on the vertical dimension, beta-1 deeper and slightly more anterior than alpha, and beta-2 again more anterior and deeper than beta-1. Thus, the depth of source location was not linearly related to temporal frequency. The sources of all 5 bands were oriented in the sagittal direction; delta mean fields had steeper gradients anteriorly, alpha and beta-1 posteriorly. The power map for any frequency was well described by a single phase angle. The results indicate that the different EEG frequency bands during a given EEG epoch are generated by neural populations in different brain locations. {\textcopyright} 1992.},
author = {Michel, C. M. and Lehmann, D. and Henggeler, B. and Brandeis, D.},
doi = {10.1016/0013-4694(92)90180-P},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Michel et al. - 1992 - Localization of the sources of EEG delta, theta, alpha and beta frequency bands using the FFT dipole approximatio.pdf:pdf},
issn = {00134694},
journal = {Electroencephalography and Clinical Neurophysiology},
keywords = {Alpha EEG source localization,Delta, theta, alpha, beta EEG source localization,EEG phase angles,EEG source localization in the frequency domain,FFT dipole approximation,Spontaneous EEG sources},
mendeley-groups = {First Exam Citations/Ch4},
number = {1},
pages = {38--44},
pmid = {1370142},
title = {{Localization of the sources of EEG delta, theta, alpha and beta frequency bands using the FFT dipole approximation}},
volume = {82},
year = {1992}
}
@article{Petroni2017,
author = {Petroni, Agustin and Cohen, Samantha S. and Ai, Lei and Langer, Nicolas and Henin, Simon and Vanderwal, Tamara and Milham, Michael P. and Parra, Lucas C.},
doi = {10.1523/ENEURO.0244-17.2017},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Petroni et al. - 2017 - Age and sex modulate the variability of neural responses to naturalistic videos.pdf:pdf},
issn = {2373-2822},
journal = {bioRxiv},
mendeley-groups = {First Exam Citations/Ch4,obama paper},
title = {{Age and sex modulate the variability of neural responses to naturalistic videos}},
url = {http://www.biorxiv.org/content/early/2017/07/14/089060},
year = {2017}
}
@article{Cohen2017,
abstract = {It is said that we lose track of time -that " time flies " -when we are engrossed in a story. How does engagement with the story cause this distorted perception of time, and what are its neural correlates? People commit both time and attentional resources to an engaging stimulus. For narrative videos, attentional engagement can be represented as the level of similarity between the electroencephalographic responses of different viewers. Here we show that this measure of neural engagement predicted the duration of time that viewers were willing to commit to narrative videos. Contrary to popular wisdom, engagement did not distort the average perception of time duration. Rather, more similar brain responses resulted in a more uniform perception of time across viewers. These findings suggest that by capturing the attention of an audience, narrative videos bring both neural processing and the subjective perception of time into synchrony.},
author = {Cohen, Samantha S. and Henin, Simon and Parra, Lucas C.},
doi = {10.1038/s41598-017-04402-4},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohen, Henin, Parra - 2017 - Engaging narratives evoke similar neural activity and lead to similar time perception(2).pdf:pdf},
issn = {20452322},
journal = {Scientific Reports},
mendeley-groups = {First Exam Citations/Ch4,obama paper},
number = {1},
pages = {1--10},
publisher = {Springer US},
title = {{Engaging narratives evoke similar neural activity and lead to similar time perception}},
volume = {7},
year = {2017}
}
@article{Silipo1999,
author = {Silipo, Rosaria and Greenberg, Steven and Arai, Takayuki},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Silipo, Greenberg, Arai - 1999 - Temporal constraints on speech intelligibility as deduced from exceedingly sparse spectral representati.pdf:pdf},
journal = {Proceedings of Eurospeech},
mendeley-groups = {First Exam Citations/Ch4},
pages = {2687--2690},
title = {{Temporal constraints on speech intelligibility as deduced from exceedingly sparse spectral representations}},
year = {1999}
}
@article{Drullman1994,
abstract = {This paper describes two experiments on the effect of reduced spectral contrast on the speech-reception threshold (SRT) for sentences in a background of interfering sound. Signal processing is performed by smoothing the envelope of the squared short-time fast Fourier transform by a convolution with a Gaussian-shaped filter, and overlapping additions to reconstruct a continuous signal. In the first experiment the effect of reduced spectral contrast on the SRT for male speech is investigated and compared with previously obtained results for female speech [ter Keurs et al., J. Acoust. Soc. Am. 91, 2872-2880 (1992)]. Spectral energy is smeared over bandwidths of 1/8, 1/4, 1/3, 1/2, 1, 2, and 4 oct. The results show that, despite the differences in spectral pattern between male and female voices, the SRT in noise increases similarly for both voices for smearing bandwidths over 1/3 oct. In terms of the ripple density of the spectral envelope the results indicate that the range of lower spectral modulations, up to a limit of about 1.5 periods/oct, is sufficient for the intelligibility of speech in interfering sounds. In the second experiment the extent of the threshold difference between a speech masker and a noise masker is investigated for spectral smearing bandwidths of 1/2, 1, and 2 oct. The release from masking found for the speech masker relative to the (steady-state) noise masker decreases with spectral envelope smearing.},
author = {Drullman, Rob and Festen, Joost M. and Plomp, Reinier},
doi = {10.1121/1.408467},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Drullman, Festen, Plomp - 1994 - Effect of temporal envelope smearing on speech reception.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
mendeley-groups = {First Exam Citations/Ch4},
number = {2},
pages = {1053--1064},
pmid = {8473608},
title = {{Effect of temporal envelope smearing on speech reception}},
url = {http://asa.scitation.org/doi/10.1121/1.408467},
volume = {95},
year = {1994}
}
@article{Obleser2007,
abstract = {Speech processing in auditory cortex and beyond is a remarkable yet poorly understood faculty of the listening brain. Here we show that stop consonants, as the most transient constituents of speech, are sufficient to involve speech perception circuits in the human superior temporal cortex. Left anterolateral superior temporal cortex showed a stronger response in blood oxygenation leveldependent functional magnetic resonance imaging (fMRI) to intelligible consonantal bursts compared with incomprehensible control sounds matched for spectrotemporal complexity. Simultaneously, the left posterior superior temporal plane (including planum temporale [PT]) exhibited a noncategorical responsivity to complex stimulus acoustics across all trials, showing no preference for intelligible speech sounds. Multistage hierarchical processing of speech sounds is thus revealed with fMRI, providing evidence for a role of the PT in the fundamental stages of the acoustic analysis of complex sounds, including speech.},
author = {Obleser, Jonas and Zimmermann, Jonas and {Van Meter}, John and Rauschecker, Josef P.},
doi = {10.1093/cercor/bhl133},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Obleser et al. - 2007 - Multiple stages of auditory speech perception reflected in event-related fMRI.pdf:pdf},
isbn = {1047-3211},
issn = {10473211},
journal = {Cerebral Cortex},
keywords = {Auditory cortex,Consonants,Hierarchical processing,Planum temporale,Speech,fMRI},
mendeley-groups = {First Exam Citations/Ch4},
number = {10},
pages = {2251--2257},
pmid = {17150986},
title = {{Multiple stages of auditory speech perception reflected in event-related fMRI}},
volume = {17},
year = {2007}
}
@article{Ghitza2009,
abstract = {This study was motivated by the prospective role played by brain rhythms in speech perception. The intelligibility - in terms of word error rate - of natural-sounding, synthetically generated sentences was measured using a paradigm that alters speech-energy rhythm over a range of frequencies. The material comprised 96 semantically unpredictable sentences, each approximately 2 s long (6-8 words per sentence), generated by a high-quality text-to-speech (TTS) synthesis engine. The TTS waveform was time-compressed by a factor of 3, creating a signal with a syllable rhythm three times faster than the original, and whose intelligibility is poor ({\textless}50{\%} words correct). A waveform with an artificial rhythm was produced by automatically segmenting the time-compressed waveform into consecutive 40-ms fragments, each followed by a silent interval. The parameters varied were the length of the silent interval (0-160 ms) and whether the lengths of silence were equal ('periodic') or not ('aperiodic'). The performance curve (word error rate as a function of mean duration of silence) was U-shaped. The lowest word error rate (i.e., highest intelligibility) occurred when the silence was 80 ms long and inserted periodically. This is also the condition for which word error rate increased when the silence was inserted aperiodically. These data are consistent with a model (TEMPO) in which low-frequency brain rhythms affect the ability to decode the speech signal. In TEMPO, optimum intelligibility is achieved when the syllable rhythm is within the range of the high theta-frequency brain rhythms (6-12 Hz), comparable to the rate at which segments and syllables are articulated in conversational speech.},
author = {Ghitza, Oded and Greenberg, Steven},
doi = {10.1159/000208934},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghitza, Greenberg - 2009 - On the possible role of brain rhythms in speech perception Intelligibility of time-compressed speech with per.pdf:pdf},
issn = {00318388},
journal = {Phonetica},
mendeley-groups = {First Exam Citations/Ch4},
number = {1-2},
pages = {113--126},
pmid = {19390234},
title = {{On the possible role of brain rhythms in speech perception: Intelligibility of time-compressed speech with periodic and aperiodic insertions of silence}},
volume = {66},
year = {2009}
}
@article{Ghitza2013b,
abstract = {A RECENT COMMENTARY (OSCILLATORS AND SYLLABLES: a cautionary note. Cummins, 2012) questions the validity of a class of speech perception models inspired by the possible role of neuronal oscillations in decoding speech (e.g., Ghitza, 2011; Giraud and Poeppel, 2012). In arguing against the approach, Cummins raises a cautionary flag "from a phonetician's point of view." Here we respond to his arguments from an auditory processing viewpoint, referring to a phenomenological model of Ghitza (2011) taken as a representative of the criticized approach. We shall conclude by proposing the theta-syllable as an information unit defined by cortical function-an alternative to the conventional, ambiguously defined syllable. In the large context, the resulting discussion debate should be viewed as a subtext of acoustic and auditory phonetics vs. articulatory and motor theories of speech reception.},
author = {Ghitza, Oded},
doi = {10.3389/fpsyg.2013.00138},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghitza - 2013 - The theta-syllable A unit of speech information defined by cortical function.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Cascaded neuronal oscillations,Everyday speech,Hierarchical window structure,Syllabic parsing,Synchronization},
mendeley-groups = {First Exam Citations/Ch4},
number = {MAR},
pages = {1--5},
pmid = {23519170},
title = {{The theta-syllable: A unit of speech information defined by cortical function}},
volume = {4},
year = {2013}
}
@article{Greenberg2003,
abstract = {Temporal properties of the speech signal are of potentially great importance for understanding spoken language and may provide significant insight into the manner in which listeners process spoken language with so little apparent effort. It is the thesis of this study that durational properties of phonetic segments differentially reflect the amount of information contained within a syllable, and that syllable prominence is an indirect measure of linguistic entropy. The ability to understand spoken language appears to depend on a broad distribution of syllable duration, ranging between 50 and 400 ms (for American English), which is reflected in the modulation spectrum of the acoustic signal. The upper branch of the modulation spectrum (6-20 Hz) reflects unstressed syllables, while the lower branch ({\textless} 5 Hz) represents mostly heavily stressed syllables. Low-pass filtering the modulation spectrum reduces the intelligibility of spoken sentences in a manner consistent with the differential contribution of stressed and unstressed syllables to understanding spoken language. The origins of this phenomenon are investigated in terms of the durational properties of phonetic segments contained in a corpus of spontaneous American English telephone dialogues (SWITCHBOARD). Forty-five minutes of this material was manually annotated with respect to stress accent, and the relation between accent level and segmental duration examined. Statistical analysis indicates that much of the temporal variation observed at the syllabic and phonetic-segment levels can be accounted for in terms of two basic parameters: (1) stress-accent pattern and (2) position of the segment within the syllable. Segments are generally longest in heavily stressed syllables and shortest in syllables without stress. However, the magnitude of accent's impact on duration varies as a function of syllable position. Duration of the nucleus is heavily affected by stress-accent level - heavily stressed nuclei are, on average, twice as long as their unstressed counterparts, while the duration of the onset is also significantly sensitive to stress, but to a lesser degree. In contrast, stress has relatively little impact on coda duration. This pattern of durational variation suggests that the vocalic nucleus absorbs much of the impact of stress accent and potentially sets the register for interpreting the phonetic segments contained within the syllable. Moreover, the data imply that linguistic entropy is not uniformly distributed across the syllable - the onset and nucleus convey more information than the coda. {\textcopyright} 2003 Elsevier Ltd. All rights reserved.},
author = {Greenberg, Steven and Carvey, Hannah and Hitchcock, Leah and Chang, Shuangyu},
doi = {10.1016/j.wocn.2003.09.005},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Greenberg et al. - 2003 - Temporal properties of spontaneous speech - A syllable-centric perspective.pdf:pdf},
issn = {00954470},
journal = {Journal of Phonetics},
mendeley-groups = {First Exam Citations/Ch4},
number = {3-4},
pages = {465--485},
title = {{Temporal properties of spontaneous speech - A syllable-centric perspective}},
volume = {31},
year = {2003}
}
@article{Pellegrino2011,
abstract = {This article is a crosslinguistic investigation of the hypothesis that the average information rate conveyed during speech communication results from a trade-off between average information density and speech rate. The study, based on seven languages, shows a negative correlation between density and rate, indicating the existence of several encoding strategies. However, these strategies do not necessarily lead to a constant information rate. These results are further investigated in relation to the notion of syllabic complexity.},
author = {Pellegrino, Fran{\c{c}}ois and Coup{\'{e}}, Christophe and Marsico, Egidio},
doi = {10.1353/lan.2011.0057},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pellegrino, Coup{\'{e}}, Marsico - 2011 - Across-Language Perspective on Speech Information Rate.pdf:pdf},
issn = {1535-0665},
journal = {Language},
keywords = {cross-lan-,information theory,speech communication,speech rate,working memory},
mendeley-groups = {First Exam Citations/Ch2},
number = {3},
pages = {539--558},
pmid = {21252317},
title = {{Across-Language Perspective on Speech Information Rate}},
url = {http://muse.jhu.edu/content/crossref/journals/language/v087/87.3.pellegrino.html},
volume = {87},
year = {2011}
}
@article{Ghitza2012,
abstract = {Recent hypotheses on the potential role of neuronal oscillations in speech perception propose that speech is processed on multi-scale temporal analysis windows formed by a cascade of neuronal oscillators locked to the input pseudo-rhythm. In particular, Ghitza (2011) proposed that the oscillators are in the theta, beta, and gamma frequency bands with the theta oscillator the master, tracking the input syllabic rhythm and setting a time-varying, hierarchical window structure synchronized with the input. In the study described here the hypothesized role of theta was examined by measuring the intelligibility of speech with a manipulated modulation spectrum. Each critical-band signal was manipulated by controlling the degree of temporal envelope flatness. Intelligibility of speech with critical-band envelopes that are flat is poor; inserting extra information, restricted to the input syllabic rhythm, markedly improves intelligibility. It is concluded that flattening the critical-band envelopes prevents the theta oscillator from tracking the input rhythm, hence the disruption of the hierarchical window structure that controls the decoding process. Reinstating the input-rhythm information revives the tracking capability, hence restoring the synchronization between the window structure and the input, resulting in the extraction of additional information from the flat modulation spectrum.},
author = {Ghitza, Oded},
doi = {10.3389/fpsyg.2012.00238},
isbn = {1664-1078 (Electronic)},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Cascaded neuronal oscillations,Hierarchical window structure,Intelligibility,Modulation spectrum,Speech perception,Syllabic parsing,Synchronization,Theta band},
mendeley-groups = {First Exam Citations/Ch4,obama paper},
number = {JUL},
pages = {1--12},
pmid = {22811672},
title = {{On the role of theta-driven syllabic parsing in decoding speech: Intelligibility of speech with a manipulated modulation spectrum}},
volume = {3},
year = {2012}
}
@article{Elliott2009,
abstract = {We systematically determined which spectrotemporal modulations in speech are necessary for comprehension by human listeners. Speech comprehension has been shown to be robust to spectral and temporal degradations, but the specific relevance of particular degradations is arguable due to the complexity of the joint spectral and temporal information in the speech signal. We applied a novel modulation filtering technique to recorded sentences to restrict acoustic information quantitatively and to obtain a joint spectrotemporal modulation transfer function for speech comprehension, the speech MTF. For American English, the speech MTF showed the criticality of low modulation frequencies in both time and frequency. Comprehension was significantly impaired when temporal modulations {\textless}12 Hz or spectral modulations {\textless}4 cycles/kHz were removed. More specifically, the MTF was bandpass in temporal modulations and low-pass in spectral modulations: temporal modulations from 1 to 7 Hz and spectral modulations {\textless}1 cycles/kHz were the most important. We evaluated the importance of spectrotemporal modulations for vocal gender identification and found a different region of interest: removing spectral modulations between 3 and 7 cycles/kHz significantly increases gender misidentifications of female speakers. The determination of the speech MTF furnishes an additional method for producing speech signals with reduced bandwidth but high intelligibility. Such compression could be used for audio applications such as file compression or noise removal and for clinical applications such as signal processing for cochlear implants.},
author = {Elliott, Taffeta M. and Theunissen, Fr{\'{e}}d{\'{e}}ric E.},
doi = {10.1371/journal.pcbi.1000302},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Elliott, Theunissen - 2009 - The modulation transfer function for speech intelligibility.pdf:pdf},
issn = {1553734X},
journal = {PLoS Computational Biology},
mendeley-groups = {First Exam Citations/Ch4},
number = {3},
pmid = {19266016},
title = {{The modulation transfer function for speech intelligibility}},
volume = {5},
year = {2009}
}
@article{Goswami2013,
author = {Goswami, Usha and Leong, Victoria},
doi = {10.1515/lp-2013-0004},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goswami, Leong - 2013 - Speech rhythm and temporal structure Converging perspectives.pdf:pdf},
isbn = {1868-6354},
issn = {1868-6354},
journal = {Laboratory Phonology},
mendeley-groups = {First Exam Citations/Ch4},
number = {1},
pages = {67--92},
title = {{Speech rhythm and temporal structure: Converging perspectives?}},
url = {https://www.degruyter.com/view/j/lp.2013.4.issue-1/lp-2013-0004/lp-2013-0004.xml},
volume = {4},
year = {2013}
}
@article{Ding2009,
abstract = {Natural sounds such as speech contain multiple levels and multiple types of temporal modulations. Because of nonlinearities of the auditory system, however, the neural response to multiple, simultaneous temporal modulations cannot be predicted from the neural responses to single modulations. Here we show the cortical neural representation of an auditory stimulus simultaneously frequency modulated (FM) at a high rate, f(FM) approximately 40 Hz, and amplitude modulation (AM) at a slow rate, f(AM) {\textless}15 Hz. Magnetoencephalography recordings show fast FM and slow AM stimulus features evoke two separate but not independent auditory steady-state responses (aSSR) at f(FM) and f(AM), respectively. The power, rather than phase locking, of the aSSR of both decreases with increasing stimulus f(AM). The aSSR at f(FM) is itself simultaneously amplitude modulated and phase modulated with fundamental frequency f(AM), showing that the slow stimulus AM is not only encoded in the neural response at f(AM) but also encoded in the instantaneous amplitude and phase of the neural response at f(FM). Both the amplitude modulation and phase modulation of the aSSR at f(FM) are most salient for low stimulus f(AM) but remain observable at the highest tested f(AM) (13.8 Hz). The instantaneous amplitude of the aSSR at f(FM) is successfully predicted by a model containing temporal integration on two time scales, approximately 25 and approximately 200 ms, followed by a static compression nonlinearity.},
author = {Ding, Nai and Simon, Jonathan Z},
doi = {10.1152/jn.00523.2009},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding, Simon - 2009 - Neural Representations of Complex Temporal Modulations in the Human Auditory Cortex.pdf:pdf},
issn = {0022-3077},
journal = {Journal of Neurophysiology},
mendeley-groups = {First Exam Citations/Ch4},
number = {5},
pages = {2731--2743},
pmid = {19692508},
title = {{Neural Representations of Complex Temporal Modulations in the Human Auditory Cortex}},
url = {http://www.physiology.org/doi/10.1152/jn.00523.2009},
volume = {102},
year = {2009}
}
@article{Tzounopoulos2009,
abstract = {Mechanisms of plasticity have traditionally been ascribed to higher-order sensory processing areas such as the cortex, whereas early sensory processing centers have been considered largely hard-wired. In agreement with this view, the auditory brainstem has been viewed as a nonplastic site, important for preserving temporal information and minimizing transmission delays. However, recent groundbreaking results from animal models and human studies have revealed remarkable evidence for cellular and behavioral mechanisms for learning and memory in the auditory brainstem. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
author = {Tzounopoulos, Thanos and Kraus, Nina},
doi = {10.1016/j.neuron.2009.05.002},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tzounopoulos, Kraus - 2009 - Learning to Encode Timing Mechanisms of Plasticity in the Auditory Brainstem.pdf:pdf},
isbn = {0896-6273},
issn = {08966273},
journal = {Neuron},
mendeley-groups = {First Exam Citations/Ch4},
number = {4},
pages = {463--469},
pmid = {19477149},
publisher = {Elsevier Inc.},
title = {{Learning to Encode Timing: Mechanisms of Plasticity in the Auditory Brainstem}},
url = {http://dx.doi.org/10.1016/j.neuron.2009.05.002},
volume = {62},
year = {2009}
}
@article{Parker2005,
abstract = {Recent electrophysiological investigations of the auditory system in primates along with functional neuroimaging studies of auditory perception in humans have suggested there are two pathways arising from the primary auditory cortex. In the primate brain, a 'ventral' pathway is thought to project anteriorly from the primary auditory cortex to prefrontal areas along the superior temporal gyrus while a separate 'dorsal' route connects these areas posteriorly via the inferior parietal lobe. We use diffusion MRI tractography, a noninvasive technique based on diffusion-weighted MRI, to investigate the possibility of a similar pattern of connectivity in the human brain for the first time. The dorsal pathway from Wernicke's area to Broca's area is shown to include the arcuate fasciculus and connectivity to Brodmann area 40, lateral superior temporal gyrus (LSTG), and lateral middle temporal gyrus. A ventral route between Wernicke's area and Broca's area is demonstrated that connects via the external capsule/uncinate fasciculus and the medial superior temporal gyrus. Ventral connections are also observed in the lateral superior and middle temporal gyri. The connections are stronger in the dominant hemisphere, in agreement with previous studies of functional lateralization of auditory-language processing. {\textcopyright} 2004 Elsevier Inc. All rights reserved.},
author = {Parker, Geoffrey J M and Luzzi, Simona and Alexander, Daniel C. and Wheeler-Kingshott, Claudia A M and Ciccarelli, Olga and {Lambon Ralph}, Matthew A.},
doi = {10.1016/j.neuroimage.2004.08.047},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Parker et al. - 2005 - Lateralization of ventral and dorsal auditory-language pathways in the human brain.pdf:pdf},
issn = {10538119},
journal = {NeuroImage},
keywords = {Diffusion-weighted imaging,Language,Lateralization,Tractography},
mendeley-groups = {First Exam Citations/Ch4},
number = {3},
pages = {656--666},
pmid = {15652301},
title = {{Lateralization of ventral and dorsal auditory-language pathways in the human brain}},
volume = {24},
year = {2005}
}
@misc{Bhattacharyya2017,
author = {Bhattacharyya, Neil},
booktitle = {Medscape},
mendeley-groups = {First Exam Citations/Ch4},
title = {{Auditory Brainstem Response Audiometry}},
url = {https://emedicine.medscape.com/article/836277-overview{\#}showall},
year = {2017}
}
@article{Sininger1993,
abstract = {The auditory brain stem response (ABR) is felt to be an objective technique for predicting hearing thresholds because a voluntary response is not required from the subject. However, determination of ABR threshold can be a subjective process. This article discusses a technique, termed Fsp, which adds objectivity to ABR threshold detection by creating a ratio of signal plus averaged background noise over an estimate of the averaged background noise for any given averaged ABR. Fsp values have an F distribution. Consequently, the confidence of true detection for a given ABR can be determined by comparing its calculated Fsp value to statistical tables. Using a technique such as Fsp not only adds objectivity to ABR threshold detection, but also optimizes test time by allowing the averaging process to stop as soon as the background noise has been reduced and the true neural potential can be judged to be present. The estimate of the background noise can be used as a weighting factor to reduce the influence of noisy segments during the averaging process as well. Using this technique, we have found ABR threshold to be within 5 or 6 dB of psychophysical threshold for like (click) stimuli and, in our pediatric clinic, ABR click thresholds are within 10 dB of puretone average for children with losses ranging from mild to profound.},
author = {Sininger, Yvonne S.},
doi = {10.1097/00003446-199302000-00004},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sininger - 1993 - Auditory Brain Stem Response for Objective Measures of Hearing.pdf:pdf},
issn = {0196-0202},
journal = {Ear and Hearing},
mendeley-groups = {First Exam Citations/Ch4},
number = {1},
pages = {23--30},
pmid = {8444334},
title = {{Auditory Brain Stem Response for Objective Measures of Hearing}},
url = {http://content.wkhealth.com/linkback/openurl?sid=WKPTLP:landingpage{\&}an=00003446-199302000-00004},
volume = {14},
year = {1993}
}
@article{Meyer2018,
abstract = {The negotiation of social order is intimately connected to the capacity to infer and track status relationships. Despite the foundational role of status in social cognition, we know little about how the brain constructs status from social interactions that display it. Although emerging cognitive neuroscience reveals that status judgments depend on the intraparietal sulcus, a brain region that supports the comparison of targets along a quantitative continuum, we present evidence that status judgments do not necessarily reduce to ranking targets along a quantitative continuum. The process of judging status also fits a social interdependence analysis. Consistent with third-party perceivers judging status by inferring whose goals are dictating the terms of the interaction and who is subordinating their desires to whom, status judgments were associated with increased recruitment of medial pFC and STS, brain regions implicated in mental state inference},
archivePrefix = {arXiv},
arxivId = {1511.04103},
author = {Meyer, Lars and Gumbert, Matthias},
doi = {10.1162/jocn_a_01236},
eprint = {1511.04103},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Meyer, Gumbert - 2018 - Synchronization of Electrophysiological Responses with Speech Benefits Syntactic Information Processing.pdf:pdf},
isbn = {9780192880512},
issn = {0898-929X},
journal = {Journal of Cognitive Neuroscience},
mendeley-groups = {First Exam Citations/Ch4},
number = {3},
pages = {1--10},
pmid = {23647519},
title = {{Synchronization of Electrophysiological Responses with Speech Benefits Syntactic Information Processing}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/jocn{\_}a{\_}01236},
volume = {26},
year = {2018}
}
@article{Rauschecker2009,
abstract = {Speech and language are considered uniquely human abilities: animals have communication systems, but they do not match human linguistic skills in terms of recursive structure and combinatorial power. Yet, in evolution, spoken language must have emerged from neural mechanisms at least partially available in animals. In this paper, we will demonstrate how our understanding of speech perception, one important facet of language, has profited from findings and theory in nonhuman primate studies. Chief among these are physiological and anatomical studies showing that primate auditory cortex, across species, shows patterns of hierarchical structure, topographic mapping and streams of functional processing. We will identify roles for different cortical areas in the perceptual processing of speech and review functional imaging work in humans that bears on our understanding of how the brain decodes and monitors speech. A new model connects structures in the temporal, frontal and parietal lobes linking speech perception and production.},
author = {Rauschecker, Josef P. and Scott, Sophie K.},
doi = {10.1038/nn.2331},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rauschecker, Scott - 2009 - Maps and streams in the auditory cortex Nonhuman primates illuminate human speech processing.pdf:pdf},
issn = {10976256},
journal = {Nature Neuroscience},
mendeley-groups = {First Exam Citations/Ch4},
number = {6},
pages = {718--724},
pmid = {19471271},
title = {{Maps and streams in the auditory cortex: Nonhuman primates illuminate human speech processing}},
volume = {12},
year = {2009}
}
@article{Hickok2004,
abstract = {Despite intensive work on language-brain relations, and a fairly impressive accumulation of knowledge over the last several decades, there has been little progress in developing large-scale models of the functional anatomy of language that integrate neuropsychological, neuroimaging, and psycholinguistic data. Drawing on relatively recent developments in the cortical organization of vision, and on data from a variety of sources, we propose a new framework for understanding aspects of the functional anatomy of language which moves towards remedying this situation. The framework posits that early cortical stages of speech perception involve auditory fields in the superior temporal gyrus bilaterally (although asymmetrically). This cortical processing system then diverges into two broad processing streams, a ventral stream, which is involved in mapping sound onto meaning, and a dorsal stream, which is involved in mapping sound onto articulatory-based representations. The ventral stream projects ventro-laterally toward inferior posterior temporal cortex (posterior middle temporal gyrus) which serves as an interface between sound-based representations of speech in the superior temporal gyrus (again bilaterally) and widely distributed conceptual representations. The dorsal stream projects dorso-posteriorly involving a region in the posterior Sylvian fissure at the parietal-temporal boundary (area Spt), and ultimately projecting to frontal regions. This network provides a mechanism for the development and maintenance of "parity" between auditory and motor representations of speech. Although the proposed dorsal stream represents a very tight connection between processes involved in speech perception and speech production, it does not appear to be a critical component of the speech perception process under normal (ecologically natural) listening conditions, that is, when speech input is mapped onto a conceptual representation. We also propose some degree of bi-directionality in both the dorsal and ventral pathways. We discuss some recent empirical tests of this framework that utilize a range of methods. We also show how damage to different components of this framework can account for the major symptom clusters of the fluent aphasias, and discuss some recent evidence concerning how sentence-level processing might be integrated into the framework. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Hickok, Gregory and Poeppel, David},
doi = {10.1016/j.cognition.2003.10.011},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hickok, Poeppel - 2004 - Dorsal and ventral streams a framework for understanding aspects of the functional anatomy of language.pdf:pdf},
issn = {00100277},
journal = {Cognition},
keywords = {Aphasia,Dorsal and ventral streams,Functional anatomy,Language,Speech perception,Speech production},
mendeley-groups = {First Exam Citations/Ch4},
number = {1-2},
pages = {67--99},
pmid = {15037127},
title = {{Dorsal and ventral streams: a framework for understanding aspects of the functional anatomy of language}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027703002282},
volume = {92},
year = {2004}
}
@misc{Hickok2000,
abstract = {The functional neuroanatomy of speech perception has been difficult to characterize. Part of the difficulty, we suggest, stems from the fact that the neural systems supporting 'speech perception' vary as a function of the task. Specifically, the set of cognitive and neural systems involved in performing traditional laboratory speech perception tasks, such as syllable discrimination or identification, only partially overlap those involved in speech perception as it occurs during natural language comprehension. In this review, we argue that cortical fields in the posterior-superior temporal lobe, bilaterally, constitute the primary substrate for constructing sound-based representations of speech, and that these sound-based representations interface with different supramodal systems in a task-dependent manner. Tasks that require access to the mental lexicon (i.e. accessing meaning-based representations) rely on auditory-to-meaning interface systems in the cortex in the vicinity of the left temporal-parietal-occipital junction. Tasks that require explicit access to speech segments rely on auditory-motor interface systems in the left frontal and parietal lobes. This auditory-motor interface system also appears to be recruited in phonological working memory. {\textcopyright} 2000 Elsevier Science Ltd.},
author = {Hickok, Gregory and Poeppel, David},
booktitle = {Trends in Cognitive Sciences},
doi = {10.1016/S1364-6613(00)01463-7},
issn = {13646613},
mendeley-groups = {First Exam Citations/Ch4},
number = {4},
pages = {131--138},
pmid = {10740277},
title = {{Towards a functional neuroanatomy of speech perception}},
volume = {4},
year = {2000}
}
@article{Zoefel2016,
abstract = {Phase entrainment of neural oscillations, the brain's adjustment to rhythmic stimulation, is a central component in recent theories of speech comprehension: the alignment between brain oscillations and speech sound improves speech intelligibility. However, phase entrainment to everyday speech sound could also be explained by oscillations passively following the low-level periodicities (e.g., in sound amplitude and spectral content) of auditory stimulation-and not by an adjustment to the speech rhythm per se. Recently, using novel speech/noise mixture stimuli, we have shown that behavioral performance can entrain to speech sound even when high-level features (including phonetic information) are not accompanied by fluctuations in sound amplitude and spectral content. In the present study, we report that neural phase entrainment might underlie our behavioral findings. We observed phase-locking between electroencephalogram (EEG) and speech sound in response not only to original (unprocessed) speech but also to our constructed "high-level" speech/noise mixture stimuli. Phase entrainment to original speech and speech/noise sound did not differ in the degree of entrainment, but rather in the actual phase difference between EEG signal and sound. Phase entrainment was not abolished when speech/noise stimuli were presented in reverse (which disrupts semantic processing), indicating that acoustic (rather than linguistic) high-level features play a major role in the observed neural entrainment. Our results provide further evidence for phase entrainment as a potential mechanism underlying speech processing and segmentation, and for the involvement of high-level processes in the adjustment to the rhythm of speech.},
author = {Zoefel, Benedikt and VanRullen, Rufin},
doi = {10.1016/j.neuroimage.2015.08.054},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zoefel, VanRullen - 2016 - EEG oscillations entrain their phase to high-level features of speech sound.pdf:pdf},
issn = {10959572},
journal = {NeuroImage},
keywords = {Auditory,EEG,High-level,Intelligibility,Oscillation,Phase entrainment,Speech},
mendeley-groups = {First Exam Citations/Ch4},
pages = {16--23},
pmid = {26341026},
title = {{EEG oscillations entrain their phase to high-level features of speech sound}},
volume = {124},
year = {2016}
}
@article{Millman2015,
abstract = {The negotiation of social order is intimately connected to the capacity to infer and track status relationships. Despite the foundational role of status in social cognition, we know little about how the brain constructs status from social interactions that display it. Although emerging cognitive neuroscience reveals that status judgments depend on the intraparietal sulcus, a brain region that supports the comparison of targets along a quantitative continuum, we present evidence that status judgments do not necessarily reduce to ranking targets along a quantitative continuum. The process of judging status also fits a social interdependence analysis. Consistent with third-party perceivers judging status by inferring whose goals are dictating the terms of the interaction and who is subordinating their desires to whom, status judgments were associated with increased recruitment of medial pFC and STS, brain regions implicated in mental state inference},
archivePrefix = {arXiv},
arxivId = {1511.04103},
author = {Millman, Rebecca E. and Johnson, Sam R. and Prendergast, Garreth},
doi = {10.1162/jocn_a_00719},
eprint = {1511.04103},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Millman, Johnson, Prendergast - 2015 - The Role of Phase-locking to the Temporal Envelope of Speech in Auditory Perception and Speech In.pdf:pdf},
isbn = {9780192880512},
issn = {0898-929X},
journal = {Journal of Cognitive Neuroscience},
mendeley-groups = {First Exam Citations/Ch4},
month = {mar},
number = {3},
pages = {533--545},
pmid = {23647519},
title = {{The Role of Phase-locking to the Temporal Envelope of Speech in Auditory Perception and Speech Intelligibility}},
url = {http://www.mitpressjournals.org/doi/10.1162/jocn{\_}a{\_}00719},
volume = {27},
year = {2015}
}
@article{Zoefel2015,
abstract = {Constantly bombarded with input, the brain has the need to filter out relevant information while ignoring the irrelevant rest. A powerful tool may be represented by neural oscillations which entrain their high-excitability phase to important input while their low-excitability phase attenuates irrelevant information. Indeed, the alignment between brain oscillations and speech improves intelligibility and helps dissociating speakers during a "cocktail party". Although well-investigated, the contribution of low- and high-level processes to phase entrainment to speech sound has only recently begun to be understood. Here, we review those findings, and concentrate on three main results: (1) Phase entrainment to speech sound is modulated by attention or predictions, likely supported by top-down signals and indicating higher-level processes involved in the brain's adjustment to speech. (2) As phase entrainment to speech can be observed without systematic fluctuations in sound amplitude or spectral content, it does not only reflect a passive steady-state "ringing" of the cochlea, but entails a higher-level process. (3) The role of intelligibility for phase entrainment is debated. Recent results suggest that intelligibility modulates the behavioral consequences of entrainment, rather than directly affecting the strength of entrainment in auditory regions. We conclude that phase entrainment to speech reflects a sophisticated mechanism: several high-level processes interact to optimally align neural oscillations with predicted events of high relevance, even when they are hidden in a continuous stream of background noise.},
author = {Zoefel, Benedikt and VanRullen, Rufin},
doi = {10.3389/fnhum.2015.00651},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zoefel, VanRullen - 2015 - The Role of High-Level Processes for Oscillatory Phase Entrainment to Speech Sound.pdf:pdf},
issn = {1662-5161},
journal = {Frontiers in Human Neuroscience},
keywords = {EEG, oscillation, phase, entrainment, high-level,,amount of,auditory,cope with an enormous,eeg,entrainment,high-level,in virtually every situation,incoming information,intelligibility,of our life,only a fraction of,oscillation,phase,phase entrainment as a,s interpretation,speech,the brain has to,the scene,tool for input gating,which is essential for},
mendeley-groups = {First Exam Citations/Ch4},
number = {December},
pages = {1--12},
pmid = {26696863},
title = {{The Role of High-Level Processes for Oscillatory Phase Entrainment to Speech Sound}},
url = {http://journal.frontiersin.org/Article/10.3389/fnhum.2015.00651/abstract},
volume = {9},
year = {2015}
}
@article{Ding2017,
abstract = {Speech and music have structured rhythms. Here we discuss a major acoustic correlate of spoken and musical rhythms, the slow (0.25-32. Hz) temporal modulations in sound intensity and compare the modulation properties of speech and music. We analyze these modulations using over 25. h of speech and over 39. h of recordings of Western music. We show that the speech modulation spectrum is highly consistent across 9 languages (including languages with typologically different rhythmic characteristics). A different, but similarly consistent modulation spectrum is observed for music, including classical music played by single instruments of different types, symphonic, jazz, and rock. The temporal modulations of speech and music show broad but well-separated peaks around 5 and 2. Hz, respectively. These acoustically dominant time scales may be intrinsic features of speech and music, a possibility which should be investigated using more culturally diverse samples in each domain. Distinct modulation timescales for speech and music could facilitate their perceptual analysis and its neural processing.},
author = {Ding, Nai and Patel, Aniruddh D. and Chen, Lin and Butler, Henry and Luo, Cheng and Poeppel, David},
doi = {10.1016/j.neubiorev.2017.02.011},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding et al. - 2017 - Temporal modulations in speech and music.pdf:pdf},
issn = {01497634},
journal = {Neuroscience {\&} Biobehavioral Reviews},
keywords = {Modulation spectrum,Music,Rhythm,Speech,Temporal modulations},
mendeley-groups = {First Exam Citations/Ch4},
pages = {181--187},
pmid = {28212857},
publisher = {Elsevier Ltd},
title = {{Temporal modulations in speech and music}},
url = {http://dx.doi.org/10.1016/j.neubiorev.2017.02.011 http://linkinghub.elsevier.com/retrieve/pii/S0149763416305668},
volume = {81},
year = {2017}
}
@article{Vanthornhout2017,
author = {Vanthornhout, J. and Decruy, L. and Wouters, J. and Simon, J.Z. and Francart, T.},
doi = {10.1101/246660},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vanthornhout et al. - 2017 - Speech intelligibility predicted from neural entrainment of the speech envelope.pdf:pdf},
journal = {Journal of the Association for Research in Otolaryngology},
mendeley-groups = {First Exam Citations/Ch4,obama paper},
number = {637424},
title = {{Speech intelligibility predicted from neural entrainment of the speech envelope}},
year = {2017}
}
@article{Nourski2009,
abstract = {Speech comprehension relies on temporal cues contained in the speech envelope, and the auditory cortex has been implicated as playing a critical role in encoding this temporal information. We investigated auditory cortical responses to speech stimuli in subjects undergoing invasive electrophysiological monitoring for pharmacologically refractory epilepsy. Recordings were made from multicontact electrodes implanted in Heschl's gyrus (HG). Speech sentences, time compressed from 0.75 to 0.20 of natural speaking rate, elicited average evoked potentials (AEPs) and increases in event-related band power (ERBP) of cortical high-frequency (70-250 Hz) activity. Cortex of posteromedial HG, the presumed core of human auditory cortex, represented the envelope of speech stimuli in the AEP and ERBP. Envelope following in ERBP, but not in AEP, was evident in both language-dominant and -nondominant hemispheres for relatively high degrees of compression where speech was not comprehensible. Compared to posteromedial HG, responses from anterolateral HG-an auditory belt field-exhibited longer latencies, lower amplitudes, and little or no time locking to the speech envelope. The ability of the core auditory cortex to follow the temporal speech envelope over a wide range of speaking rates leads us to conclude that such capacity in itself is not a limiting factor for speech comprehension.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Nourski, K. V. and Reale, R. A. and Oya, H. and Kawasaki, H. and Kovach, C. K. and Chen, H. and Howard, M. A. and Brugge, J. F.},
doi = {10.1523/JNEUROSCI.3065-09.2009},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nourski et al. - 2009 - Temporal Envelope of Time-Compressed Speech Represented in the Human Auditory Cortex.pdf:pdf},
issn = {0270-6474},
journal = {Journal of Neuroscience},
mendeley-groups = {First Exam Citations/Ch4,obama paper},
number = {49},
pages = {15564--15574},
pmid = {20007480},
title = {{Temporal Envelope of Time-Compressed Speech Represented in the Human Auditory Cortex}},
url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.3065-09.2009},
volume = {29},
year = {2009}
}
@article{Jewett1970,
abstract = {Auditory potentials recorded from the vertex of humans by a modified averaging technique have very short latencies and are probably generated by brain stem structures located at a considerable distance from the recording point. The evoked waves, which shOW considerable detail and consistency within and across subjects, may be clinically useful in evaluating subcortical function.},
author = {Jewett, D. L. and Romano, M. N. and Williston, J. S.},
doi = {10.1126/science.167.3924.1517},
issn = {0036-8075},
journal = {Science},
mendeley-groups = {First Exam Citations/Ch4},
number = {3924},
pages = {1517--1518},
pmid = {5415287},
title = {{Human Auditory Evoked Potentials: Possible Brain Stem Components Detected on the Scalp}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.167.3924.1517},
volume = {167},
year = {1970}
}
@article{Young1979,
abstract = {This paper is concerned with the representation of the spectra of synthesized steady-state vowels in the temporal aspects of the discharges of auditory-nerve fibers. The results are based on a study of the responses of large numbers of single auditory-nerve fibers in anesthetized cats. By presenting the same set of stimuli to all the fibers encountered in each cat, we can directly estimate the population response to those stimuli. Period histograms of the responses of each unit to the vowels were constructed. The temporal response of a fiber to each harmonic component of the stimulus is taken to be the amplitude of the corresponding component in the Fourier transform of the unit's period histogram. At low sound levels, the temporal response to each stimulus component is maximal among units with CFs near the frequency of the component (i.e., near its place). Responses to formant components are larger than responses to other stimulus components. As sound level is increased, the responses to the formants, particularly the first formant, increase near their places and spread to adjacent regions, particularly toward higher CFs. Responses to nonformant components, exept for harmonics and intermodulation products of the formants (2F1,2F2,F1 + F2, etc), are suppressed; at the highest sound levels used (approximately 80 dB SPL), temporal responses occur almost exclusively at the first two or three formants and their harmonics and intermodulation products. We describe a simple calculation which combines rate, place, and temporal information to provide a good representation of the vowels' spectra, including a clear indication of at least the first two formant frequencies. This representation is stable with changes in sound level at least up to 80 dB SPL; its stability is in sharp contrast to the behavior of the representation of the vowels' spectra in terms of discharge rate which degenerates at stimulus levels within the conversational range.},
author = {Young, Eric D. and Sachs, Murray B.},
doi = {10.1121/1.383532},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
mendeley-groups = {First Exam Citations/Ch2},
number = {5},
pages = {1381--1403},
pmid = {500976},
title = {{Representation of steady-state vowels in the temporal aspects of the discharge patterns of populations of auditory-nerve fibers}},
url = {http://asa.scitation.org/doi/10.1121/1.383532},
volume = {66},
year = {1979}
}
@article{Jewett1971,
abstract = {Differential recording between closely spaced electrodes is useful for locating a neural generator since such a recording configuration can only detect near fields and is uninfluenced by the far fields of distant generators. Far field recordings (which imply that the generator is at a distance) offer advantages in that the position of the electrode is not critical for obtaining satisfactory recordings and in that potentials from widely spaced generators can be detected at a single electrode.Both of these advantages can be seen in the work presented here, where far field potentials evoked by auditory click stimuli are recorded from the scalps of humans. On the basis of some indirect evidence, it is possible to deduce the location of the generators of some of the waves. It is clear that far fields at least 10 cm. from their brain-stem generators can be recorded from humans and that electrical activity from any brain location within the human skull may be detectable at the scalp, given a satisfactory method of synchronizing the activity with the averager.},
author = {Jewett, Don L. and Williston, John S.},
doi = {10.1093/brain/94.4.681},
issn = {00068950},
journal = {Brain},
mendeley-groups = {First Exam Citations/Ch4},
number = {4},
pages = {681--696},
pmid = {5132966},
title = {{Auditory-evoked far fields averaged from the scalp of humans}},
volume = {94},
year = {1971}
}
@misc{pict,
author = {{Wikimedia Commons}},
mendeley-groups = {First Exam Citations},
publisher = {Wikimedia},
title = {{Spectrogram iua}},
url = {https://commons.wikimedia.org/wiki/File:Spectrogram{\_}-iua-.png},
year = {2005}
}
@article{Noll1967,
abstract = {The cepstrum , defined as the power spectrum of the logarithm of the power spectrum, has a strong peak corresponding to the pitch period of the voiced-speech segment being analyzed. Cepstra were calculated on a digital computer and were automatically plotted ... $\backslash$n},
author = {Noll, A Michael},
doi = {10.1121/1.1910339},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
mendeley-groups = {First Exam Citations},
number = {2},
pages = {293--309},
pmid = {6040805},
title = {{Cepstrum pitch determination}},
url = {http://scitation.aip.org/content/asa/journal/jasa/41/2/10.1121/1.1910339},
volume = {41},
year = {1967}
}
@article{Hickok2007,
abstract = {Despite decades of research, the functional neuroanatomy of speech processing has been difficult to characterize. A major impediment to progress may have been the failure to consider task effects when mapping speech-related processing systems. We outline a dual-stream model of speech processing that remedies this situation. In this model, a ventral stream processes speech signals for comprehension, and a dorsal stream maps acoustic speech signals to frontal lobe articulatory networks. The model assumes that the ventral stream is largely bilaterally organized — although there are important computational differences between the left- and right-hemisphere systems — and that the dorsal stream is strongly left-hemisphere dominant.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Hickok, Gregory and Poeppel, David},
doi = {10.1038/nrn2113},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hickok, Poeppel - 2007 - The cortical organization of speech processing(2).pdf:pdf},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch4},
number = {5},
pages = {393--402},
pmid = {17431404},
title = {{The cortical organization of speech processing}},
url = {http://www.nature.com/doifinder/10.1038/nrn2113},
volume = {8},
year = {2007}
}
@article{Davis1980,
abstract = {Several parametric representations of the acoustic signal were compared with regard to word recognition performance in a syllable-oriented continuous speech recognition system. The vocabulary included many phonetically similar monosyllabic words, therefore the emphasis was on the ability to retain phonetically significant acoustic information in the face of syntactic and duration variations. For each parameter set (based on a mel-frequency cepstrum, a linear frequency cepstrum, a linear prediction cepstrum, a linear prediction spectrum, or a set of reflection coefficients), word templates were generated using an efficient dynamic warping method, and test data were time registered with the templates. A set of ten mel-frequency cepstrum coefficients computed every 6.4 ms resulted in the best performance, namely 96.5 percent and 95.0 percent recognition with each of two speakers. The superior performance of the mel-frequency cepstrum coefficients may be attributed to the fact that they better represent the perceptually relevant aspects of the short-term speech spectrum.},
author = {Davis, S. and Mermelstein, Paul},
doi = {10.1109/TASSP.1980.1163420},
issn = {0096-3518},
journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
mendeley-groups = {First Exam Citations},
number = {4},
pages = {357--366},
pmid = {1163420},
title = {{Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences}},
url = {http://ieeexplore.ieee.org/document/1163420/},
volume = {28},
year = {1980}
}
@article{DeTaillez2018,
author = {de Taillez, Tobias and Kollmeier, Birger and Meyer, Bernd T.},
doi = {10.1111/ejn.13790},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/de Taillez, Kollmeier, Meyer - 2018 - Machine learning for decoding listeners' attention from electroencephalography evoked by continuou.pdf:pdf},
isbn = {0000000221},
issn = {0953816X},
journal = {European Journal of Neuroscience},
keywords = {Auditory,auditory processing,hearing,neural networks,signaling pathways},
mendeley-groups = {First Exam - To Read,First Exam Citations/Ch4,obama paper},
number = {June},
pages = {1--8},
pmid = {29205588},
title = {{Machine learning for decoding listeners' attention from electroencephalography evoked by continuous speech}},
url = {http://doi.wiley.com/10.1111/ejn.13790},
year = {2018}
}
@book{Webster1992,
address = {New York, NY},
doi = {10.1007/978-1-4612-4416-5},
editor = {Webster, Douglas B. and Popper, Arthur N. and Fay, Richard R.},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 1992 - The Mammalian Auditory Pathway Neuroanatomy.pdf:pdf},
isbn = {978-0-387-97800-0},
mendeley-groups = {First Exam Citations/Ch4},
publisher = {Springer New York},
series = {Springer Handbook of Auditory Research},
title = {{The Mammalian Auditory Pathway: Neuroanatomy}},
volume = {1},
year = {1992}
}
@article{Poeppel2008,
abstract = {Speech perception consists of a set of computations that take continuously varying acoustic waveforms as input and generate discrete representations that make contact with the lexical representations stored in long-term memory as output. Because the perceptual objects that are recognized by the speech perception enter into subsequent linguistic computation, the format that is used for lexical representation and processing fundamentally constrains the speech perceptual processes. Consequently, theories of speech perception must, at some level, be tightly linked to theories of lexical representation. Minimally, speech perception must yield representations that smoothly and rapidly interface with stored lexical items. Adopting the perspective of Marr, we argue and provide neurobiological and psychophysical evidence for the following research programme. First, at the implementational level, speech perception is a multi-time resolution process, with perceptual analyses occurring concurrently on at least two time scales (approx. 20-80 ms, approx. 150-300 ms), commensurate with (sub)segmental and syllabic analyses, respectively. Second, at the algorithmic level, we suggest that perception proceeds on the basis of internal forward models, or uses an 'analysis-by-synthesis' approach. Third, at the computational level (in the sense of Marr), the theory of lexical representation that we adopt is principally informed by phonological research and assumes that words are represented in the mental lexicon in terms of sequences of discrete segments composed of distinctive features. One important goal of the research programme is to develop linking hypotheses between putative neurobiological primitives (e.g. temporal primitives) and those primitives derived from linguistic inquiry, to arrive ultimately at a biologically sensible and theoretically satisfying model of representation and computation in speech.},
author = {Poeppel, David and Idsardi, W. J and van Wassenhove, V.},
doi = {10.1098/rstb.2007.2160},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Poeppel, Idsardi, van Wassenhove - 2008 - Speech perception at the interface of neurobiology and linguistics.pdf:pdf},
isbn = {0962-8436 (Print)$\backslash$r0962-8436 (Linking)},
issn = {0962-8436},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
keywords = {analysis-by-synthesis,distinctive features,forward model,multi-time resolution,predictive coding,temporal coding},
mendeley-groups = {First Exam Citations/Ch5},
number = {1493},
pages = {1071--1086},
pmid = {17890189},
title = {{Speech perception at the interface of neurobiology and linguistics}},
url = {http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2007.2160},
volume = {363},
year = {2008}
}
@misc{Galbraith1995,
abstract = {The human brain stem frequency-following response (FFR) registers phase-locked neural activity to cyclical auditory stimuli. We show that the FFR can be elicited by word stimuli, and when speech-evoked FFTs are reproduced as auditory stimuli they are heard as intelligible speech. Stimuli were 10 high- and 10 low-probability words drawn from normative verbal responses of university students. Horizontal and vertical dipole FFRs based on 1000 repetitions of each word were recorded from two different participants. Speech-evoked FFRs were evaluated by 80 listeners. The results showed significant effects of FFR participant, word probability, and whether or not words were presented with category cues. Depending on such subject and experimental variables, FFRs were correctly perceived from 5{\%} to 92{\%} of the time.},
archivePrefix = {arXiv},
arxivId = {1708.02002},
author = {Galbraith, Gary C. and Arbagey, Paul W. and Branski, Renee and Comerci, Nelson and Rector, Pollyanna M.},
booktitle = {NeuroReport},
doi = {10.1097/00001756-199511270-00021},
eprint = {1708.02002},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Galbraith et al. - 1995 - Intelligible speech encoded in the human brain stem frequency-following response.pdf:pdf},
issn = {0959-4965},
mendeley-groups = {First Exam Citations/Ch4},
number = {17},
pages = {2363--2367},
pmid = {8747154},
title = {{Intelligible speech encoded in the human brain stem frequency-following response}},
url = {http://content.wkhealth.com/linkback/openurl?sid=WKPTLP:landingpage{\&}an=00001756-199511270-00021},
volume = {6},
year = {1995}
}
@article{Skoe2010,
abstract = {This tutorial provides a comprehensive overview of the methodological approach to collecting and analyzing auditory brain stem responses to complex sounds (cABRs). cABRs provide a window into how behaviorally relevant sounds such as speech and music are processed in the brain. Because temporal and spectral characteristics of sounds are preserved in this subcortical response, cABRs can be used to assess specific impairments and enhancements in auditory processing. Notably, subcortical auditory function is neither passive nor hardwired but dynamically interacts with higher-level cognitive processes to refine how sounds are transcribed into neural code. This experience-dependent plasticity, which can occur on a number of time scales (e.g., life-long experience with speech or music, short-term auditory training, on-line auditory processing), helps shape sensory perception. Thus, by being an objective and noninvasive means for examining cognitive function and experience-dependent processes in sensory activity, cABRs have considerable utility in the study of populations where auditory function is of interest (e.g., auditory experts such as musicians, and persons with hearing loss, auditory processing, and language disorders). This tutorial is intended for clinicians and researchers seeking to integrate cABRs into their clinical or research programs.},
author = {Skoe, Erika and Kraus, Nina},
doi = {10.1097/AUD.0b013e3181cdb272},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Skoe, Kraus - 2010 - Auditory Brain Stem Response to Complex Sounds A Tutorial.pdf:pdf},
issn = {0196-0202},
journal = {Ear and Hearing},
keywords = {*Evoked Potentials,Acoustic Stimulation/*methods,Auditory,Auditory Pathways/*physiology,Brain Stem,Continuing,Education,Hearing Disorders/*diagnosis/*physiopathology,Humans,Language Disorders/diagnosis/physiopathology,Medical,Music,Neuronal Plasticity/physiology,Phonetics},
mendeley-groups = {First Exam Citations/Ch4},
number = {3},
pages = {302--324},
pmid = {20084007},
title = {{Auditory Brain Stem Response to Complex Sounds: A Tutorial}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20084007 http://content.wkhealth.com/linkback/openurl?sid=WKPTLP:landingpage{\&}an=00003446-201006000-00002},
volume = {31},
year = {2010}
}
@article{Choi2013,
abstract = {OBJECTIVES: It would be clinically valuable if an electrophysiological validation of hearing aid effectiveness in conveying speech information could be performed when a device is first provided to the individual after electroacoustic verification. This study evaluated envelope following responses (EFRs) elicited by English vowels in a steady state context and in natural sentences. It was the purpose of this study to determine whether EFRs could be detected rapidly enough to be clinically useful.$\backslash$n$\backslash$nDESIGN: EFRs were elicited using 5 vowels spanning the English vowel space, /i/, /$\epsilon$/, /{\ae}/, /(Equation is included in full-text article.)/, and /u/. These were presented either as concatenated steady state vowels (total duration 10.04 seconds) or in three 5-word sentences (total duration 11.77 seconds), where each vowel appeared once per sentence. Single-channel electroencephalogram was recorded from vertex (Cz) to the nape of the neck for 190 and 160 repetitions of the steady state vowels and sentences, respectively. The stimuli were presented at 70 dBA SPL. The fundamental frequency (f0) track from the stimuli was used with a Fourier analyzer to estimate the EFRs to each vowel. Noise amplitudes were also calculated at neighboring frequencies. Fifteen normal-hearing subjects who were 20 to 34 years of age participated in the experiment.$\backslash$n$\backslash$nRESULTS: In the analysis of steady state vowels, the mean response amplitude of /i/ was statistically the largest at 173 nV. The other 4 steady state vowels did not differ in mean response amplitude, which varied between 73 and 106 nV. In the analysis of vowels from the 3 sentences, the largest response amplitudes tended to be for /u/. Mean amplitudes for /u/ were 164, 111, and 140 nV for the words "booed," "food," and "Sue," respectively. The vowel /u/ produced statistically larger responses than /i/, /$\epsilon$/, and /(Equation is included in full-text article.)/ when grouped across words, whereas other vowels did not differ. Mean response amplitudes for the other vowel categories in the sentences varied between 82 and 105 nV. All subjects showed significant EFRs in response to the words "Bee's" and "booed," but only 9 subjects showed significant EFRs for "pet," "bed," and "Bob."$\backslash$n$\backslash$nCONCLUSIONS: The authors were readily able to detect significant EFRs elicited by vowels in a steady state context and from 3 natural sentences. These results are promising as an early step in developing a clinical tool for validating that vowel stimuli are at least partially encoded at the level of the auditory brainstem. Future research will require evaluation of the technique with aided listeners, where the natural sentences are expected to be treated as typical speech by hearing aid signal-processing algorithms.},
author = {Choi, Jong Min and Purcell, David W. and Coyne, Julie Anne M and Aiken, Steven J.},
doi = {10.1097/AUD.0b013e31828e4dad},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi et al. - 2013 - Envelope following responses elicited by english sentences.pdf:pdf},
isbn = {1538-4667 (Electronic)$\backslash$r0196-0202 (Linking)},
issn = {01960202},
journal = {Ear and Hearing},
mendeley-groups = {First Exam Citations/Ch4},
number = {5},
pages = {637--650},
pmid = {23575462},
title = {{Envelope following responses elicited by english sentences}},
volume = {34},
year = {2013}
}
@article{Power2012,
abstract = {Distinguishing between speakers and focusing attention on one speaker in multi-speaker environments is extremely important in everyday life. Exactly how the brain accomplishes this feat and, in particular, the precise temporal dynamics of this attentional deployment are as yet unknown. A long history of behavioral research using dichotic listening paradigms has debated whether selective attention to speech operates at an early stage of processing based on the physical characteristics of the stimulus or at a later stage during semantic processing. With its poor temporal resolution fMRI has contributed little to the debate, while EEG-ERP paradigms have been hampered by the need to average the EEG in response to discrete stimuli which are superimposed onto ongoing speech. This presents a number of problems, foremost among which is that early attention effects in the form of endogenously generated potentials can be so temporally broad as to mask later attention effects based on the higher level processing of the speech stream. Here we overcome this issue by utilizing the AESPA (auditory evoked spread spectrum analysis) method which allows us to extract temporally detailed responses to two concurrently presented speech streams in natural cocktail-party-like attentional conditions without the need for superimposed probes. We show attentional effects on exogenous stimulus processing in the 200-220 ms range in the left hemisphere. We discuss these effects within the context of research on auditory scene analysis and in terms of a flexible locus of attention that can be deployed at a particular processing stage depending on the task.},
author = {Power, Alan J. and Foxe, John J. and Forde, Emma Jane and Reilly, Richard B. and Lalor, Edmund C.},
doi = {10.1111/j.1460-9568.2012.08060.x},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Power et al. - 2012 - At what time is the cocktail party A late locus of selective attention to natural speech.pdf:pdf},
isbn = {1662-5145 (Electronic)$\backslash$r1662-5145 (Linking)},
issn = {0953816X},
journal = {European Journal of Neuroscience},
keywords = {AESPA,EEG,Exogenous processing,Multi-speaker environments},
mendeley-groups = {First Exam Citations/Ch4,obama paper},
number = {9},
pages = {1497--1503},
pmid = {22462504},
title = {{At what time is the cocktail party? A late locus of selective attention to natural speech}},
volume = {35},
year = {2012}
}
@article{Fletcher1933,
abstract = {Loudness of sounds is defined as the magnitude of auditory sensations. Mathematical formulae are developed, and tables and graphs are presented to be used in calculating the loudness of continuous complex sounds. A discussion of the standard conditions in which the formulae hold, the types of apparatus used, the position of the observers, the frequencies and intensities of the sounds, etc., is included. (PsycINFO Database Record (c) 2009 APA, all rights reserved)},
archivePrefix = {arXiv},
arxivId = {129.255.231.83},
author = {Fletcher, H and Munson, W a},
doi = {10.1121/1.1915637},
eprint = {129.255.231.83},
issn = {0001-4966},
journal = {Journal of the Acoustical Society of America},
keywords = {Equal Loudness Curves,Loudness,Measurement,Perception,Volume},
mendeley-groups = {First Exam Citations/Ch3},
number = {1924},
pages = {82--108},
title = {{Loudness, its Definition, Measurement and Calculation}},
volume = {5},
year = {1933}
}
@book{Kandel2000,
abstract = {Important features to this edition include a new chapter - Genes and Behavior; a complete updating of development of the nervous system; the genetic basis of...},
author = {Kandel, E R and Schwartz, J H and Jessell, T M},
booktitle = {Neurology},
doi = {10.1036/0838577016},
isbn = {0838577016},
issn = {01956108},
mendeley-groups = {First Exam Citations/Ch3},
pages = {1414},
pmid = {11769631},
title = {{Principles of Neural Science}},
volume = {3},
year = {2000}
}
@article{Reichenbach2016,
abstract = {The auditory-brainstem response (ABR) to short and simple acoustical signals is an important clinical tool used to diagnose the integrity of the brainstem. The ABR is also employed to investigate the auditory brainstem in a multitude of tasks related to hearing, such as processing speech or selectively focusing on one speaker in a noisy environment. Such research measures the response of the brainstem to short speech signals such as vowels or words. Because the voltage signal of the ABR has a tiny amplitude, several hundred to a thousand repetitions of the acoustic signal are needed to obtain a reliable response. The large number of repetitions poses a challenge to assessing cognitive functions due to neural adaptation. Here we show that continuous, non-repetitive speech, lasting several minutes, may be employed to measure the ABR. Because the speech is not repeated during the experiment, the precise temporal form of the ABR cannot be determined. We show, however, that important structural features of the ABR can nevertheless be inferred. In particular, the brainstem responds at the fundamental frequency of the speech signal, and this response is modulated by the envelope of the voiced parts of speech. We accordingly introduce a novel measure that assesses the ABR as modulated by the speech envelope, at the fundamental frequency of speech and at the characteristic latency of the response. This measure has a high signal-to-noise ratio and can hence be employed effectively to measure the ABR to continuous speech. We use this novel measure to show that the auditory brainstem response is weaker to intelligible speech than to unintelligible, time-reversed speech. The methods presented here can be employed for further research on speech processing in the auditory brainstem and can lead to the development of future clinical diagnosis of brainstem function.},
author = {Reichenbach, Chagit S. and Braiman, Chananel and Schiff, Nicholas D. and Hudspeth, A. J. and Reichenbach, Tobias},
doi = {10.3389/fncom.2016.00047},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Reichenbach et al. - 2016 - The Auditory-Brainstem Response to Continuous, Non-repetitive Speech Is Modulated by the Speech Envelope and.pdf:pdf},
isbn = {1662-5188},
issn = {1662-5188},
journal = {Frontiers in Computational Neuroscience},
keywords = {abr,auditory brainstem response,auditory brainstem response (ABR), speech processi,fundamental frequency,speech envelope,speech processing},
mendeley-groups = {First Exam Citations/Ch4,First Exam - To Read},
number = {May},
pages = {1--11},
pmid = {27303286},
title = {{The Auditory-Brainstem Response to Continuous, Non-repetitive Speech Is Modulated by the Speech Envelope and Reflects Speech Processing}},
url = {http://journal.frontiersin.org/Article/10.3389/fncom.2016.00047/abstract},
volume = {10},
year = {2016}
}
@book{Schnupp2011,
abstract = {As I write these lines, a shiver is running down my back. Not that writing usually has that effect on me. But on this occasion, I am allowing myself a little moment of indulgence. As I am writing, I am also listening to one of my favorite pieces of music, the aria “ Vorrei spiegarvi, oh dio! ” composed by Mozart and masterfully performed by the soprano Kathleen Battle. A digital audio player sits in my pocket. It is smaller than a matchbox and outwardly serene; yet inside the little device is immensely busy, extracting 88,200 numerical values every second from computer fi les stored in its digital memory, which it converts into electrical currents. The currents, in turn, generate electric fi elds that incessantly push and tug ever so gently on a pair of delicate membranes in the ear buds of my in-ear headphones. And, voil {\`{a}} , there she is, Kathleen, hypnotizing me with her beautiful voice and dragging me through a brief but intense emotional journey that begins with a timid sadness, grows in intensity to plumb the depths of despair only to resolve into powerful and determined, almost uplifting defi ance.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Schnupp, Jan and Nelken, Israel and King, Andrew},
booktitle = {Auditory Neuroscience},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9780262113182},
issn = {1098-6596},
mendeley-groups = {First Exam Citations/Ch2},
pages = {1--50},
pmid = {25246403},
title = {{Auditory Neuroscience}},
year = {2011}
}
@book{Fastl2007,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fastl, Hugo and Zwicker, Eberhard},
booktitle = {Psychoacoustics: Facts and Models},
doi = {10.1007/978-3-540-68888-4},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fastl, Zwicker - 2007 - Psychoacoustics Facts and models.pdf:pdf},
isbn = {3540231595},
issn = {1098-6596},
mendeley-groups = {First Exam Citations/Ch2},
pages = {1--463},
pmid = {25246403},
title = {{Psychoacoustics: Facts and models}},
year = {2007}
}
@misc{Mannell2008,
author = {Mannell, Robert},
booktitle = {Macquarie University},
mendeley-groups = {First Exam Citations/Ch2},
title = {{Speech Acoustics}},
url = {http://clas.mq.edu.au/speech/acoustics/waveforms/speech{\_}waveforms.html},
year = {2008}
}
@misc{Gutierrez-Osuna2017,
author = {Gutierrez-Osuna, Ricardo},
booktitle = {Texas A{\&}M},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gutierrez-Osuna - 2017 - Organization of Speech Sounds.pdf:pdf},
keywords = {organization of speech sounds},
mendeley-groups = {First Exam Citations/Ch2},
pages = {1--40},
title = {{Organization of Speech Sounds}},
year = {2017}
}
@article{Rabiner2007,
abstract = {chapter 6 LPC pag 73},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rabiner, Lawrence R. and Schafer, Ronald W.},
doi = {10.1561/2000000001},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rabiner, Schafer - 2007 - Introduction to Digital Speech Processing.pdf:pdf},
isbn = {1601980701},
issn = {1932-8346},
journal = {Foundations and Trends{\textregistered} in Signal Processing},
mendeley-groups = {First Exam Citations/Ch2},
number = {1–2},
pages = {1--194},
pmid = {24439530},
title = {{Introduction to Digital Speech Processing}},
url = {http://www.nowpublishers.com/article/Details/SIG-001},
volume = {1},
year = {2007}
}
@article{Umeda1975,
abstract = {This is a summary report of the vowel duration data that have been accumulated over the past several years. The data corpus analyzed to derive temporal controls of vowels consists mainly of three different readings by three different speakers, each about 10 to 20 min in duration. The rules cover the temporal behavior of vowels under many phonological conditions. The conditions include stressed and unstressed positions, prepausal and nonprepausal positions, word-final and non-word-final conditions, and monosyllabic and polysyllabic words. The influence of following consonants is discussed as well. Included also are conditions other than phonological ones, such as the effect of the prominence of words on their vowels, and the speed of reading. The duration rules derived from the data are intended for use in our speech synthesis-by-rule system from printed text.},
author = {Umeda, Noriko},
doi = {10.1121/1.380688},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Umeda - 1975 - Vowel duration in american english.pdf:pdf},
issn = {0001-4966},
journal = {Journal of the Acoustical Society of America},
mendeley-groups = {First Exam Citations/Ch2},
number = {2},
pages = {434--445},
pmid = {1184837},
title = {{Vowel duration in american english}},
url = {http://scitation.aip.org/content/asa/journal/jasa/58/2/10.1121/1.380688},
volume = {58},
year = {1975}
}
@misc{Simons2017,
author = {Simons, Gary F and Fennig, Charles D},
booktitle = {Ethnologue},
mendeley-groups = {First Exam Citations/Ch1},
title = {{How many languages are there in the world?}},
url = {https://www.ethnologue.com/guides/how-many-languages},
year = {2017}
}
@article{Sereno2005,
abstract = {Despite the paucity of direct evidence about the origin of human language, the great intrinsic interest in this question has made it difficult for writers to resist speculating about it (Harnad et al., eds., 1976; Merlin, 1991; Deacon, 1997; Jablonski {\&} Aiello, eds., 1998; King, ed., 1999; Knight et al., eds., 2000). The following attempts to bring a fresh perspective on this old question, using an analogy with the origin of cellular coding systems and applying it to what we know about the evolution of vocal behavior in animals. In other places (Sereno, 1991b), I have argued that DNA and protein based life and language based human thought may have enough in common as the only two naturally occurring examples of a code-using system to make it useful to take an analogical look at one system in order to make predictions about the other. Rather than rehearsing those arguments, I will only visit two jumping off points reached while developing that analogy: the difference between origin and evolution, and the foundational role of an intermediate string of “symbol representation” segments with properties partway between symbol and meaning.},
author = {Sereno, M. I.},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sereno - 2005 - Language origins without the semantic urge.pdf:pdf},
journal = {Cognitive Science Online},
mendeley-groups = {First Exam Citations/Ch2,First Exam Citations/Ch1},
number = {1},
pages = {1 -- 12},
title = {{Language origins without the semantic urge}},
volume = {3},
year = {2005}
}
@book{Ladefoged1996,
abstract = {This book gives a description of all the known ways in which the sounds of the world's languages differ. In doing so, it provides the empirical foundations for linguistic phonetics and phonology. Encapsulating the work of two leading figures in the field, it will be a standard work of reference for researchers in phonetics and linguistics for many years to come. The scope of the book is truly global, with data drawn from nearly 400 languages, many of them investigated at first hand by the authors. A picture of the full range of possible contrasting phonetic categories is created by comparing families of similar sounds across many different languages. Separate chapters deal with place of articulation, stops, nasals, fricatives, laterals, rhotics, clicks, vowels, and segments with multiple articulations. Each chapter is packed with illustrations documenting the articulatory and acoustic characteristics of the sounds discussed, and serving to illustrate the application of modern experimental techniques to descriptive phonetic studies.},
author = {Ladefoged, Peter and Maddieson, Ian},
booktitle = {Blackwell Publishing},
doi = {10.1177/136700699700100110},
isbn = {0631198148},
issn = {1367-0069},
mendeley-groups = {First Exam Citations/Ch2},
pmid = {2071501},
title = {{The Sounds of the World's Languages}},
year = {1996}
}
@article{Ramachandran2001,
abstract = {We investigated grapheme–colour synaesthesia and found that:$\backslash$r$\backslash$n(1) The induced colours led to perceptual grouping and pop-out, (2) a grapheme$\backslash$r$\backslash$nrendered invisible through ‘crowding' or lateral masking induced synaesthetic$\backslash$r$\backslash$ncolours — a form of blindsight — and (3) peripherally presented graphemes did$\backslash$r$\backslash$nnot induce colours even when they were clearly visible. Taken collectively, these$\backslash$r$\backslash$nand other experiments prove conclusively that synaesthesia is a genuine perceptual$\backslash$r$\backslash$nphenomenon, not an effect based on memory associations from childhood or$\backslash$r$\backslash$non vague metaphorical speech. We identify different subtypes of number–colour$\backslash$r$\backslash$nsynaesthesia and propose that they are caused by hyperconnectivity between colour$\backslash$r$\backslash$nand number areas at different stages in processing; lower synaesthetes may$\backslash$r$\backslash$nhave cross-wiring (or cross-activation) within the fusiform gyrus, whereas higher$\backslash$r$\backslash$nsynaesthetes may have cross-activation in the angular gyrus. This hyperconnectivity$\backslash$r$\backslash$nmight be caused by a genetic mutation that causes defective pruning of connections$\backslash$r$\backslash$nbetween brain maps. The mutation may further be expressed selectively$\backslash$r$\backslash$n(due to transcription factors) in the fusiform or angular gyri, and this may explain$\backslash$r$\backslash$nthe existence of different forms of synaesthesia. If expressed very diffusely, there$\backslash$r$\backslash$nmay be extensive cross-wiring between brain regions that represent abstract$\backslash$r$\backslash$nconcepts, which would explain the link between creativity, metaphor and$\backslash$r$\backslash$nsynaesthesia (and the higher incidence of synaesthesia among artists and poets).$\backslash$r$\backslash$nAlso, hyperconnectivity between the sensory cortex and amygdala would explain$\backslash$r$\backslash$nthe heightened aversion synaesthetes experience when seeing numbers printed in$\backslash$r$\backslash$nthe ‘wrong' colour. Lastly, kindling (induced hyperconnectivity in the temporal$\backslash$r$\backslash$nlobes of temporal lobe epilepsy [TLE] patients) may explain the purported higher$\backslash$r$\backslash$nincidence of synaesthesia in these patients. We conclude with a$\backslash$r$\backslash$nsynaesthesia-based theory of the evolution of language. Thus, our experiments on$\backslash$r$\backslash$nsynaesthesia and our theoretical framework attempt to link several seemingly$\backslash$r$\backslash$nunrelated facts about the human mind. Far from being a mere curiosity,$\backslash$r$\backslash$nsynaesthesia may provide a window into perception, thought and language.},
author = {Ramachandran, V.S. and Hubbard, E.H.},
doi = {10.1111/1468-0068.00363},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ramachandran, Hubbard - 2001 - Synaesthesia—A Window Into Perception, Thought and Language.pdf:pdf},
isbn = {1355-8250},
issn = {13558250},
journal = {Journal of Consciousness Studies},
mendeley-groups = {First Exam - To Read,First Exam Citations},
number = {12},
pages = {3--34},
pmid = {1000104869},
title = {{Synaesthesia—A Window Into Perception, Thought and Language}},
volume = {8},
year = {2001}
}
@book{Denes1993,
abstract = {Explains the basic mechanisms involved in spoken communication, merging the field of speech pathology, communications, psychology, engineering, and computer science.},
author = {Denes, P.B. and Pinson, E.N},
doi = {10.1121/1.1971294},
isbn = {0716723441},
issn = {00014966},
mendeley-groups = {First Exam Citations/Ch2,First Exam Citations},
pages = {1--9},
title = {{The Speech Chain}},
year = {1993}
}
@article{Mesgarani2014,
abstract = {During speech perception, linguistic elements such as consonants and vowels are extracted from a complex acoustic speech signal. The superior temporal gyrus (STG) participates in high-order auditory processing of speech, but how it encodes phonetic information is poorly understood. We used high-density direct cortical surface recordings in humans while they listened to natural, continuous speech to reveal the STG representation of the entire English phonetic inventory. At single electrodes, we found response selectivity to distinct phonetic features. Encoding of acoustic properties was mediated by a distributed population response. Phonetic features could be directly related to tuning for spectrotemporal acoustic cues, some of which were encoded in a nonlinear fashion or by integration of multiple cues. These findings demonstrate the acoustic-phonetic representation of speech in human STG.},
author = {Mesgarani, N. and Cheung, C. and Johnson, K. and Chang, E. F.},
doi = {10.1126/science.1245994},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mesgarani et al. - 2014 - Phonetic Feature Encoding in Human Superior Temporal Gyrus.pdf:pdf},
isbn = {0036-8075},
issn = {0036-8075},
journal = {Science},
mendeley-groups = {First Exam Citations/Ch2},
pmid = {24482117},
title = {{Phonetic Feature Encoding in Human Superior Temporal Gyrus}},
year = {2014}
}
@article{Wijayasiri2017,
abstract = {The purpose of this study was to establish whether functional near-infrared spectroscopy (fNIRS), an emerging brain-imaging technique based on optical principles, is suitable for studying the brain activity that underlies effortful listening. In an event-related fNIRS experiment, normally-hearing adults listened to sentences that were either clear or degraded (noise vocoded). These sentences were presented simultaneously with a non-speech distractor, and on each trial participants were instructed to attend either to the speech or to the distractor. The primary region of interest for the fNIRS measurements was the left inferior frontal gyrus (LIFG), a cortical region involved in higher-order language processing. The fNIRS results confirmed findings previously reported in the functional magnetic resonance imaging (fMRI) literature. Firstly, the LIFG exhibited an elevated response to degraded versus clear speech, but only when attention was directed towards the speech. This attention-dependent increase in frontal brain activation may be a neural marker for effortful listening. Secondly, during attentive listening to degraded speech, the haemodynamic response peaked significantly later in the LIFG than in superior temporal cortex, possibly reflecting the engagement of working memory to help reconstruct the meaning of degraded sentences. The homologous region in the right hemisphere may play an equivalent role to the LIFG in some left-handed individuals. In conclusion, fNIRS holds promise as a flexible tool to examine the neural signature of effortful listening.},
author = {Wijayasiri, Pramudi and Hartley, Douglas E.H. and Wiggins, Ian M.},
doi = {10.1016/j.heares.2017.05.010},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wijayasiri, Hartley, Wiggins - 2017 - Brain activity underlying the recovery of meaning from degraded speech A functional near-infrared.pdf:pdf},
issn = {18785891},
journal = {Hearing Research},
keywords = {Auditory cortex,Functional near-infrared spectroscopy,Inferior frontal gyrus,Listening effort,Neuroimaging,Noise vocoding,Speech comprehension,fNIRS},
mendeley-groups = {First Exam Citations/Ch4},
title = {{Brain activity underlying the recovery of meaning from degraded speech: A functional near-infrared spectroscopy (fNIRS) study}},
year = {2017}
}
@article{Defenderfer2017,
abstract = {A B S T R A C T Functional near infrared spectroscopy (fNIRS) is a safe, non-invasive, relatively quiet imaging technique that is tolerant of movement artifact making it uniquely ideal for the assessment of hearing mechanisms. Previous research demonstrates the capacity for fNIRS to detect cortical changes to varying speech intelligibility, re-vealing a positive relationship between cortical activation amplitude and speech perception score. In the present study, we use an event-related design to investigate the hemodynamic response in the temporal lobe across different listening conditions. We presented participants with a speech recognition task using sentences in quiet, sentences in noise, and vocoded sentences. Hemodynamic responses were examined across conditions and then compared when speech perception was accurate compared to when speech perception was inaccurate in the context of noisy speech. Repeated measures, two-way ANOVAs revealed that the speech in noise condition (−2.8 dB signal-to-noise ratio/SNR) demonstrated significantly greater activation than the easier listening conditions on multiple channels bilaterally. Further analyses comparing correct recognition trials to incorrect recognition trials (during the presentation phase of the trial) revealed that activation was significantly greater during correct trials. Lastly, during the repetition phase of the trial, where participants correctly repeated the sentence, the hemodynamic response demonstrated significantly higher deoxyhemoglobin than oxyhemoglobin, indicating a difference between the effects of perception and production on the cortical response. Using fNIRS, the present study adds meaningful evidence to the body of knowledge that describes the brain/behavior re-lationship related to speech perception.},
author = {Defenderfer, Jessica and Kerr-German, Anastasia and Hedrick, Mark and Buss, Aaron T},
doi = {10.1016/j.neuropsychologia.2017.09.004},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Defenderfer et al. - 2017 - Investigating the role of temporal lobe activation in speech perception accuracy with normal hearing adults.pdf:pdf},
keywords = {Event-related design,Hearing science,Speech perception,fNIRS},
mendeley-groups = {First Exam Citations/Ch4},
title = {{Investigating the role of temporal lobe activation in speech perception accuracy with normal hearing adults: An event-related fNIRS study}},
year = {2017}
}
@article{Tuller2017,
abstract = {The present paper explores the dynamics of speech production and perception in the context of syllabification and categorization. The selective review includes empirical work and dynamical models that account for changes in the perception and production of syllable structure as transitions between attractors in a dynamical system and that highlight the role of instabilities as a mechanism for regulating flexibility and change. Different conceptual approaches to changes in perceptual categorization are reviewed, including a nonlinear dynamic model, a related Bayesian approach, and a hybrid approach. Of particular importance are recent models that incorporate cognitive factors (such as attention, expectation, and memory) and that change slowly or quickly relative to the changing acoustic input. These dynamical models allow phenomena such as self-organization, emergence, and other hallmarks of complex adaptive systems and may also suggest a mechanism linking speech production and perception, providing an alternative description to the internal models often invoked.},
author = {Tuller, Betty and Lancia, Leonardo},
doi = {10.1016/j.wocn.2017.02.001},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tuller, Lancia - 2017 - Speech dynamics Converging evidence from syllabification and categorization.pdf:pdf},
issn = {00954470},
journal = {Journal of Phonetics},
keywords = {Categorization,Dynamical systems,Models of speech perception,Models of speech production,Syllabification},
mendeley-groups = {First Exam Citations/Ch2},
title = {{Speech dynamics: Converging evidence from syllabification and categorization}},
year = {2017}
}
@article{Best2007,
author = {Best, Catherine T. and Tyler, Michael D},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Best, Tyler - 2007 - Best, C. T., {\&} Tyler, M. D. (2007). Nonnative and second-language speech perception Commonalities and complement(2).pdf:pdf},
mendeley-groups = {First Exam Citations/Ch2},
title = {{Nonnative and second-language speech perception: Commonalities and complementarities. Language experience in second language speech learning}},
volume = {10389},
year = {2007}
}
@article{Kuhl2004,
abstract = {Infants learn language with remarkable speed, but how they do it remains a mystery. New data show that infants use computational strategies to detect the statistical and prosodic patterns in language input, and that this leads to the discovery of phonemes and words. Social interaction with another human being affects speech learning in a way that resembles communicative learning in songbirds. The brain's commitment to the statistical and prosodic patterns that are experienced early in life might help to explain the long-standing puzzle of why infants are better language learners than adults. Successful learning by infants, as well as constraints on that learning, are changing theories of language acquisition.},
author = {Kuhl, Patricia K.},
doi = {10.1038/nrn1533},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuhl - 2004 - Early language acquisition cracking the speech code(2).pdf:pdf},
isbn = {1471-0048},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
mendeley-groups = {First Exam Citations/Ch3,First Exam Citations/Ch2,First Exam Citations},
number = {11},
pages = {831--843},
pmid = {15496861},
title = {{Early language acquisition: cracking the speech code}},
url = {http://www.nature.com/doifinder/10.1038/nrn1533},
volume = {5},
year = {2004}
}
@article{Byrne2000,
abstract = {We describe procedures and experimental results using speech from diverse source languages to build an ASR system for a single target language. This work is intended to improve ASR in languages for which large amounts of training data are not available. We have developed both knowledge-based and automatic methods to map phonetic units from the source languages to the target language. We employed HMM adaptation techniques and discriminative model combination to combine acoustic models from the individual source languages for recognition of speech in the target language. Experiments are described in which Czech Broadcast News is transcribed using acoustic models trained from small amounts of Czech read speech augmented by English, Spanish, Russian, and Mandarin acoustic models},
author = {Byrne, William and Beyerlein, Peter and Huerta, Juan M and Khudanpur, Sanjeev and Marthi, Bhaskara and Morgan, John and Peterek, Nino and Picone, Joe and Vergyri, Dimitra and Wang, W},
doi = {10.1109/ICASSP.2000.859138},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Byrne et al. - 2000 - Towards language independent acoustic modeling(2).pdf:pdf},
isbn = {0-7803-6293-4},
issn = {1520-6149},
journal = {Acoustics, Speech, and Signal Processing, 2000. ICASSP'00. Proceedings. 2000 IEEE International Conference on},
mendeley-groups = {First Exam Citations/Ch2},
number = {1},
pages = {II1029----II1032},
title = {{Towards language independent acoustic modeling}},
volume = {2},
year = {2000}
}
@article{Strange2007,
abstract = {Cross-language perception studies report influences of speech style and consonantal context on perceived similarity and discrimination of non-native vowels by inexperienced and experienced listeners. Detailed acoustic comparisons of distributions of vowels produced by native speakers of North German (NG), Parisian French (PF) and New York English (AE) in citation (di)syllables and in sentences (surrounded by labial and alveolar stops) are reported here. Results of within- and cross-language discriminant analyses reveal striking dissimilarities across languages in the spectral/temporal variation of coarticulated vowels. As expected, vocalic duration was most important in differentiating NG vowels; it did not contribute to PF vowel classification. Spectrally, NG long vowels showed little coarticulatory change, but back/low short vowels were fronted/raised in alveolar context. PF vowels showed greater coarticulatory effects overall; back and front rounded vowels were fronted, low and mid-low vowels were raised in both sentence contexts. AE mid to high back vowels were extremely fronted in alveolar contexts, with little change in mid-low and low long vowels. Cross-language discriminant analyses revealed varying patterns of spectral (dis)similarity across speech styles and consonantal contexts that could, in part, account for AE listeners' perception of German and French front rounded vowels, and "similar" mid-high to mid-low vowels.},
author = {Strange, Winifred and Weber, Andrea and Levy, Erika S. and Shafiro, Valeriy and Hisagi, Miwako and Nishi, Kanae},
doi = {10.1121/1.2749716},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Strange et al. - 2007 - Acoustic variability within and across German, French, and American English vowels Phonetic context effects.pdf:pdf},
isbn = {0001-4966},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
mendeley-groups = {First Exam Citations/Ch2},
number = {2},
pages = {1111--1129},
pmid = {17672658},
title = {{Acoustic variability within and across German, French, and American English vowels: Phonetic context effects}},
url = {http://asa.scitation.org/doi/10.1121/1.2749716},
volume = {122},
year = {2007}
}
@article{Graves2013,
abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7{\%} on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
archivePrefix = {arXiv},
arxivId = {arXiv:1303.5778v1},
author = {Graves, A and Mohamed, A.-R. and Hinton, G},
doi = {10.1109/ICASSP.2013.6638947},
eprint = {arXiv:1303.5778v1},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Graves, Mohamed, Hinton - 2013 - Speech recognition with deep recurrent neural networks(2).pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {1520-6149},
journal = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
mendeley-groups = {First Exam Citations/Ch5},
number = {6},
pages = {6645--6649},
pmid = {27295638},
title = {{Speech recognition with deep recurrent neural networks}},
url = {files/543/Graves{\_}et{\_}al-2013-Speech{\_}recognition{\_}with{\_}deep{\_}recurrent{\_}neural{\_}networks.pdf},
year = {2013}
}
@article{Horiguchi2003,
abstract = {The authors describe the neuropsychological development of a 10-year-old boy with Noonan syndrome. The subject's IQ showed normal intelligence, although there was a discrepancy between verbal and performance IQs. Visual perception was delayed, with clumsiness and inattention affecting his performance. Both visual perception and hyperactivity improved with the subject's general development, but his inattention seemed to increase after the age of 9. The behavioral features and cognitive profile of our case resembled those of attention-deficit/hyperactivity disorder. We recommend that clinicians should evaluate cautiously the specific profile of cognitive development in Noonan syndrome with particular focus on controlling the patient's inattention.},
author = {Horiguchi, Toshihiro and Takeshita, Kazuhide},
doi = {10.1016/S0},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Horiguchi, Takeshita - 2003 - Neuropsychological developmental change in a case with Noonan syndrome longitudinal assessment.pdf:pdf},
isbn = {1514934809},
issn = {0387-7604},
journal = {Brain {\&} development},
keywords = {algometry,experimental pain,gender differences,pressure pain threshold},
mendeley-groups = {First Exam Citations/Ch3},
number = {4},
pages = {291--293},
pmid = {12767464},
title = {{Neuropsychological developmental change in a case with Noonan syndrome: longitudinal assessment.}},
volume = {25},
year = {2003}
}
@misc{Best1995,
author = {Best, Catherine T},
booktitle = {Speech Perception and Linguistic Experience: Issues in Cross-Language Research},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Best - 1995 - A Direct Realist View of Cross-Language Speech Perception(2).pdf:pdf},
isbn = {9780912752365},
issn = {091275236X},
mendeley-groups = {First Exam Citations/Ch4},
pages = {171--204},
title = {{A Direct Realist View of Cross-Language Speech Perception}},
year = {1995}
}
@article{Moses2016,
abstract = {OBJECTIVE The superior temporal gyrus (STG) and neighboring brain regions play a key role in human language processing. Previous studies have attempted to reconstruct speech information from brain activity in the STG, but few of them incorporate the probabilistic framework and engineering methodology used in modern speech recognition systems. In this work, we describe the initial efforts toward the design of a neural speech recognition (NSR) system that performs continuous phoneme recognition on English stimuli with arbitrary vocabulary sizes using the high gamma band power of local field potentials in the STG and neighboring cortical areas obtained via electrocorticography. APPROACH The system implements a Viterbi decoder that incorporates phoneme likelihood estimates from a linear discriminant analysis model and transition probabilities from an n-gram phonemic language model. Grid searches were used in an attempt to determine optimal parameterizations of the feature vectors and Viterbi decoder. MAIN RESULTS The performance of the system was significantly improved by using spatiotemporal representations of the neural activity (as opposed to purely spatial representations) and by including language modeling and Viterbi decoding in the NSR system. SIGNIFICANCE These results emphasize the importance of modeling the temporal dynamics of neural responses when analyzing their variations with respect to varying stimuli and demonstrate that speech recognition techniques can be successfully leveraged when decoding speech from neural signals. Guided by the results detailed in this work, further development of the NSR system could have applications in the fields of automatic speech recognition and neural prosthetics.},
author = {Moses, David A and Mesgarani, Nima and Leonard, Matthew K and Chang, Edward F},
doi = {10.1088/1741-2560/13/5/056004},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moses et al. - 2016 - Neural speech recognition continuous phoneme decoding using spatiotemporal representations of human cortical activ.pdf:pdf},
isbn = {1741-2552},
issn = {1741-2560},
journal = {Journal of Neural Engineering},
keywords = {43.71.An,43.71.Qr,43.71.Sy,43.71Es,43.72.Ne,87.19.L,87.85.D,87.85.E,87.85.Wc,electrocorticography,high gamma,human auditory cortex,neural speech recognition,speech perception,superior temporal gyrus},
mendeley-groups = {First Exam Citations/Ch3},
number = {5},
pages = {056004},
pmid = {27484713},
publisher = {IOP Publishing},
title = {{Neural speech recognition: continuous phoneme decoding using spatiotemporal representations of human cortical activity}},
url = {http://stacks.iop.org/1741-2552/13/i=5/a=056004?key=crossref.ac3fcc4576d40ed0daf5b1e6181cd472},
volume = {13},
year = {2016}
}
@article{Dmochowski2017,
abstract = {In neuroscience, stimulus-response relationships have traditionally been analyzed using either encoding or decoding models. Here we propose a hybrid approach that decomposes neural activity into multiple components, each representing a portion of the stimulus. The technique is implemented via canonical correlation analysis (CCA) by temporally filtering the stimulus (encoding) and spatially filtering the neural responses (decoding) such that the resulting components are maximally correlated. In contrast to existing methods, this approach recovers multiple correlated stimulus-response pairs, and thus affords a richer, multidimensional analysis of neural representations. We first validated the technique's ability to recover multiple stimulus-driven components using electroencephalographic (EEG) data simulated with a finite element model of the head. We then applied the technique to real EEG responses to auditory and audiovisual narratives experienced identically across subjects, as well as uniquely experienced video game play. During narratives, both auditory and visual stimulus-response correlations (SRC) were modulated by attention and tracked inter-subject correlations. During video game play, SRC varied with game difficulty and the presence of a dual task. Interestingly, the strongest component extracted for visual and auditory features of film clips had nearly identical spatial distributions, suggesting that the predominant encephalographic response to naturalistic stimuli is supramodal. The diversity of these findings demonstrates the utility of measuring multidimensional SRC via hybrid encoding-decoding.},
author = {Dmochowski, Jacek P. and Ki, Jason J. and DeGuzman, Paul and Sajda, Paul and Parra, Lucas C.},
doi = {10.1016/j.neuroimage.2017.05.037},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dmochowski et al. - 2017 - Extracting multidimensional stimulus-response correlations using hybrid encoding-decoding of neural activi(2).pdf:pdf},
issn = {10959572},
journal = {NeuroImage},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch5,obama paper},
number = {May},
pages = {1--13},
publisher = {Elsevier},
title = {{Extracting multidimensional stimulus-response correlations using hybrid encoding-decoding of neural activity}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2017.05.037},
year = {2017}
}
@article{Brodbeck2017,
author = {Brodbeck, Christian and Presacco, Alessandro and Simon, Jonathan Z},
doi = {10.1101/182881},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brodbeck, Presacco, Simon - 2017 - Neural source dynamics of brain responses to continuous stimuli speech processing from acoustics to.pdf:pdf},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch3,First Exam Citations/Ch4},
pages = {1--27},
title = {{Neural source dynamics of brain responses to continuous stimuli : speech processing from acoustics to comprehension}},
year = {2017}
}
@article{Kosem2017,
author = {K{\"{o}}sem, Anne and Bosker, Hans Rutger and Takashima, Atsuko and Meyer, Antje and Jensen, Ole and Hagoort, Peter},
doi = {10.1101/175000},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/K{\"{o}}sem et al. - 2017 - Neural entrainment determines the words we hear.pdf:pdf},
isbn = {2012203566},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch3,First Exam Citations/Ch4,obama paper},
title = {{Neural entrainment determines the words we hear}},
year = {2017}
}
@article{Horton2014,
abstract = {Objective. Recent studies have shown that auditory cortex better encodes the envelope of attended speech than that of unattended speech during multi-speaker ('cocktail party') situations. We investigated whether these differences were sufficiently robust within single-trial electroencephalographic (EEG) data to accurately determine where subjects attended. Additionally, we compared this measure to other established EEG markers of attention. Approach. High-resolution EEG was recorded while subjects engaged in a two-speaker 'cocktail party' task. Cortical responses to speech envelopes were extracted by cross-correlating the envelopes with each EEG channel. We also measured steady-state responses (elicited via high-frequency amplitude modulation of the speech) and alpha-band power, both of which have been sensitive to attention in previous studies. Using linear classifiers, we then examined how well each of these features could be used to predict the subjects' side of attention at various epoch lengths. Main results. We found that the attended speaker could be determined reliably from the envelope responses calculated from short periods of EEG, with accuracy improving as a function of sample length. Furthermore, envelope responses were far better indicators of attention than changes in either alpha power or steady-state responses. Significance. These results suggest that envelope-related signals recorded in EEG data can be used to form robust auditory BCI's that do not require artificial manipulation (e.g., amplitude modulation) of stimuli to function.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Horton, Cort and Srinivasan, Ramesh and D'Zmura, Michael},
doi = {10.1088/1741-2560/11/4/046015},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Horton, Srinivasan, D'Zmura - 2014 - Envelope responses in single-trial EEG indicate attended speaker in a ‘cocktail party'.pdf:pdf},
isbn = {2156623929},
issn = {1741-2560},
journal = {Journal of Neural Engineering},
keywords = {alpha lateralization,brain,computer interfaces,selective attention,speech envelopes,steady-state responses},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch4,obama paper},
number = {4},
pages = {046015},
pmid = {24963838},
publisher = {IOP Publishing},
title = {{Envelope responses in single-trial EEG indicate attended speaker in a ‘cocktail party'}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24963838 http://stacks.iop.org/1741-2552/11/i=4/a=046015?key=crossref.e543a3a482c9db8ce42ab850fec18319},
volume = {11},
year = {2014}
}
@article{Hyafil2015a,
author = {Hyafil, Alexandre and Fontolan, Lorenzo and Kabdebon, Claire and Gutkin, Boris and Giraud, Anne-Lise},
doi = {10.7554/eLife.06213},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hyafil et al. - 2015 - Speech encoding by coupled cortical theta and gamma oscillations.pdf:pdf},
issn = {2050-084X},
journal = {eLife},
mendeley-groups = {First Exam - To Read,First Exam Citations/to read,obama paper},
month = {may},
title = {{Speech encoding by coupled cortical theta and gamma oscillations}},
url = {http://elifesciences.org/lookup/doi/10.7554/eLife.06213},
volume = {4},
year = {2015}
}
@article{Kayser2015,
abstract = {UNLABELLED: The entrainment of slow rhythmic auditory cortical activity to the temporal regularities in speech is considered to be a central mechanism underlying auditory perception. Previous work has shown that entrainment is reduced when the quality of the acoustic input is degraded, but has also linked rhythmic activity at similar time scales to the encoding of temporal expectations. To understand these bottom-up and top-down contributions to rhythmic entrainment, we manipulated the temporal predictive structure of speech by parametrically altering the distribution of pauses between syllables or words, thereby rendering the local speech rate irregular while preserving intelligibility and the envelope fluctuations of the acoustic signal. Recording EEG activity in human participants, we found that this manipulation did not alter neural processes reflecting the encoding of individual sound transients, such as evoked potentials. However, the manipulation significantly reduced the fidelity of auditory delta (but not theta) band entrainment to the speech envelope. It also reduced left frontal alpha power and this alpha reduction was predictive of the reduced delta entrainment across participants. Our results show that rhythmic auditory entrainment in delta and theta bands reflect functionally distinct processes. Furthermore, they reveal that delta entrainment is under top-down control and likely reflects prefrontal processes that are sensitive to acoustical regularities rather than the bottom-up encoding of acoustic features.$\backslash$n$\backslash$nSIGNIFICANCE STATEMENT: The entrainment of rhythmic auditory cortical activity to the speech envelope is considered to be critical for hearing. Previous work has proposed divergent views in which entrainment reflects either early evoked responses related to sound encoding or high-level processes related to expectation or cognitive selection. Using a manipulation of speech rate, we dissociated auditory entrainment at different time scales. Specifically, our results suggest that delta entrainment is controlled by frontal alpha mechanisms and thus support the notion that rhythmic auditory cortical entrainment is shaped by top-down mechanisms.},
author = {Kayser, Stephanie J and Ince, Robin A A and Gross, Joachim and Kayser, Christoph},
doi = {10.1523/JNEUROSCI.2243-15.2015},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kayser et al. - 2015 - Irregular Speech Rate Dissociates Auditory Cortical Entrainment, Evoked Responses, and Frontal Alpha.pdf:pdf},
isbn = {0270-6474},
issn = {1529-2401},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
keywords = {auditory cortex,auditory cortical activity to,considered to be critical,delta band,encoding or,for hearing,information theory,previous,reflects either early evoked,responses related to sound,rhythmic entrainment,significance statement,speech,the entrainment of rhythmic,the speech envelope is,views in which entrainment,work has proposed divergent},
mendeley-groups = {First Exam - To Read,First Exam Citations/Ch4},
number = {44},
pages = {14691--701},
pmid = {26538641},
title = {{Irregular Speech Rate Dissociates Auditory Cortical Entrainment, Evoked Responses, and Frontal Alpha.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4635123{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {35},
year = {2015}
}
@article{Giraud2012,
abstract = {Neuronal oscillations are ubiquitous in the brain and may contribute to cognition in several ways: for example, by segregating information and organizing spike timing. Recent data show that delta, theta and gamma oscillations are specifically engaged by the multi-timescale, quasi-rhythmic properties of speech and can track its dynamics. We argue that they are foundational in speech and language processing, 'packaging' incoming information into units of the appropriate temporal granularity. Such stimulus-brain alignment arguably results from auditory and motor tuning throughout the evolution of speech and language and constitutes a natural model system allowing auditory research to make a unique contribution to the issue of how neural oscillatory activity affects human cognition.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Giraud, Anne-Lise and Poeppel, David},
doi = {10.1038/nn.3063},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Giraud, Poeppel - 2012 - Cortical oscillations and speech processing emerging computational principles and operations.pdf:pdf},
issn = {1546-1726},
journal = {Nature neuroscience},
keywords = {Acoustic Stimulation,Acoustic Stimulation: methods,Action Potentials,Action Potentials: physiology,Auditory Cortex,Auditory Cortex: physiology,Brain Mapping,Brain Mapping: methods,Brain Waves,Brain Waves: physiology,Computational Biology,Computational Biology: methods,Cortical Synchronization,Cortical Synchronization: physiology,Humans,Nerve Net,Nerve Net: physiology,Speech,Speech Perception,Speech Perception: physiology,Speech: physiology},
mendeley-groups = {First Exam - To Read,First Exam Citations/Ch4},
number = {4},
pages = {511--7},
pmid = {22426255},
publisher = {Nature Publishing Group},
title = {{Cortical oscillations and speech processing: emerging computational principles and operations.}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84859217287{\&}partnerID=tZOtx3y1},
volume = {15},
year = {2012}
}
@article{Morillon2015,
abstract = {Neuronal oscillations are comprised of rhythmic fluctuations of excitability that are synchronized in ensembles of neurons and thus function as temporal filters that dynamically organize sensory processing. When perception relies on anticipatory mechanisms, ongoing oscillations also provide a neurophysiological substrate for temporal prediction. In this article, we review evidence for this account with a focus on auditory perception. We argue that such "oscillatory temporal predictions" can selectively amplify neuronal sensitivity to inputs that occur in a predicted, task-relevant rhythm and optimize temporal selection. We elaborate this argument for a prototypic example, speech processing, where information is present at multiple time scales, with delta, theta, and low-gamma oscillations being specifically and simultaneously engaged, enabling multiplexing. We then consider the origin of temporal predictions, specifically the idea that the motor system is involved in the generation of such prior information. Finally, we place temporal predictions in the general context of internal models, discussing how they interact with feature-based or spatial predictions. We propose that complementary predictions interact synergistically according to a dominance hierarchy, shaping perception in the form of a multidimensional filter mechanism.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Morillon, Benjamin and Schroeder, Charles E.},
doi = {10.1111/nyas.12629},
eprint = {15334406},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Morillon, Schroeder - 2015 - Neuronal oscillations as a mechanistic substrate of auditory temporal prediction.pdf:pdf},
issn = {17496632},
journal = {Annals of the New York Academy of Sciences},
keywords = {Active sensing,Expectation,Neurophysiology,Perception,Sensorimotor},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch3},
number = {1},
pages = {26--31},
pmid = {25773613},
title = {{Neuronal oscillations as a mechanistic substrate of auditory temporal prediction}},
volume = {1337},
year = {2015}
}
@article{Ghitza2011,
abstract = {The premise of this study is that current models of speech perception, which are driven by acoustic features alone, are incomplete, and that the role of decoding time during memory access must be incorporated to account for the patterns of observed recognition phenomena. It is postulated that decoding time is governed by a cascade of neuronal oscillators, which guide template-matching operations at a hierarchy of temporal scales. Cascaded cortical oscillations in the theta, beta, and gamma frequency bands are argued to be crucial for speech intelligibility. Intelligibility is high so long as these oscillations remain phase locked to the auditory input rhythm. A model (Tempo) is presented which is capable of emulating recent psychophysical data on the intelligibility of speech sentences as a function of "packaging" rate (Ghitza and Greenberg, 2009). The data show that intelligibility of speech that is time-compressed by a factor of 3 (i.e., a high syllabic rate) is poor (above 50{\%} word error rate), but is substantially restored when the information stream is re-packaged by the insertion of silent gaps in between successive compressed-signal intervals - a counterintuitive finding, difficult to explain using classical models of speech perception, but emerging naturally from the Tempo architecture.},
author = {Ghitza, Oded},
doi = {10.3389/fpsyg.2011.00130},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghitza - 2011 - Linking speech perception and neurophysiology Speech decoding guided by cascaded oscillators locked to the input rhythm.pdf:pdf},
isbn = {1664-1078 (Electronic)},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Brain rhythms,Cascaded cortical oscillations,Decoding,Decoding time,Locking,Memory access,Parsing,Phase,Speech perception},
mendeley-groups = {First Exam - To Read,First Exam Citations/Ch4,First Exam Citations/to read},
number = {JUN},
pages = {1--13},
pmid = {21743809},
title = {{Linking speech perception and neurophysiology: Speech decoding guided by cascaded oscillators locked to the input rhythm}},
volume = {2},
year = {2011}
}
@inproceedings{Biesmans2015,
abstract = {Recent research has shown that it is possible to detect which of two simultaneous speakers a person is attending to, using brain recordings and the temporal envelope of the separate speech signals. However, a wide range of possible methods for extracting this speech envelope exists. This paper assesses the effect of different envelope extraction methods with varying degrees of auditory modelling on the performance of auditory attention detection (AAD), and more specifically on the detection accuracy. It is found that subband envelope extraction with proper power-law compression yields best performance, and that the use of several more detailed auditory models does not yield a further improvement in performance.},
author = {Biesmans, Wouter and Vanthornhout, Jonas and Wouters, Jan and Moonen, Marc and Francart, Tom and Bertrand, Alexander},
booktitle = {Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
doi = {10.1109/EMBC.2015.7319552},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Biesmans et al. - 2015 - Comparison of speech envelope extraction methods for EEG-based auditory attention detection in a cocktail party.pdf:pdf},
isbn = {9781424492718},
issn = {1557170X},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch5},
pages = {5155--5158},
pmid = {27244743},
title = {{Comparison of speech envelope extraction methods for EEG-based auditory attention detection in a cocktail party scenario}},
volume = {2015-Novem},
year = {2015}
}
@article{Dmochowski2016,
abstract = {In neuroscience, stimulus-response relationships have traditionally been analyzed using either encoding or decoding models. Here we combined both techniques by decomposing neural activity into multiple components, each representing a portion of the stimulus. We tested this hybrid approach on encephalographic responses to auditory and audiovisual narratives identically experienced across subjects, as well as uniquely experienced video game play. The highest stimulus-response correlations (SRC) were detected for dynamic visual features. During narratives both auditory and visual SRC were modulated by attention and tracked correlations between subjects. During video game play, SRC was modulated by task difficulty and attentional state. Importantly, the strongest component extracted for visual and auditory features had nearly identical spatial distributions, suggesting that the predominant encephalographic response to naturalistic stimuli is supramodal. The variety of novel findings demonstrates the utility of measuring multidimensional stimulus-response correlations.},
author = {Dmochowski, Jacek P and Ki, Jason and Deguzman, Paul and Sajda, Paul and Parra, Lucas C},
doi = {10.1101/077230},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dmochowski et al. - 2016 - Multidimensional stimulus-response correlation reveals supramodal neural responses to naturalistic stimuli.pdf:pdf},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch5,First Exam Citations/Ch4},
pages = {1--18},
title = {{Multidimensional stimulus-response correlation reveals supramodal neural responses to naturalistic stimuli}},
year = {2016}
}
@article{DiLiberto2016,
abstract = {The International Symposium on Hearing is a prestigious, triennial gathering where world-class scientists present and discuss the most recent advances in the field of human and animal hearing research. The 2015 edition will particularly focus on integrative approaches linking physiological, psychophysical and cognitive aspects of normal and impaired hearing. Like previous editions, the proceedings will contain about 50 chapters ranging from basic to applied research, and of interest to neuroscientists, psychologists, audiologists, engineers, otolaryngologists, and artificial intelligence researchers.​},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Di Liberto}, Giovanni M. and Lalor, Edmund C.},
doi = {10.1007/978-3-319-25474-6_35},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Di Liberto, Lalor - 2016 - Isolating neural indices of continuous speech processing at the phonetic level.pdf:pdf},
isbn = {978-3-319-25472-2},
issn = {22148019},
journal = {Advances in Experimental Medicine and Biology},
keywords = {EEG,Hierarchical,Intelligibility,Natural speech,Noise vocoding,Priming},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch3},
pages = {337--345},
pmid = {25246403},
title = {{Isolating neural indices of continuous speech processing at the phonetic level}},
volume = {894},
year = {2016}
}
@article{Crosse2015,
abstract = {UNLABELLED: Congruent audiovisual speech enhances our ability to comprehend a speaker, even in noise-free conditions. When incongruent auditory and visual information is presented concurrently, it can hinder a listener's perception and even cause him or her to perceive information that was not presented in either modality. Efforts to investigate the neural basis of these effects have often focused on the special case of discrete audiovisual syllables that are spatially and temporally congruent, with less work done on the case of natural, continuous speech. Recent electrophysiological studies have demonstrated that cortical response measures to continuous auditory speech can be easily obtained using multivariate analysis methods. Here, we apply such methods to the case of audiovisual speech and, importantly, present a novel framework for indexing multisensory integration in the context of continuous speech. Specifically, we examine how the temporal and contextual congruency of ongoing audiovisual speech affects the cortical encoding of the speech envelope in humans using electroencephalography. We demonstrate that the cortical representation of the speech envelope is enhanced by the presentation of congruent audiovisual speech in noise-free conditions. Furthermore, we show that this is likely attributable to the contribution of neural generators that are not particularly active during unimodal stimulation and that it is most prominent at the temporal scale corresponding to syllabic rate (2-6 Hz). Finally, our data suggest that neural entrainment to the speech envelope is inhibited when the auditory and visual streams are incongruent both temporally and contextually.$\backslash$n$\backslash$nSIGNIFICANCE STATEMENT: Seeing a speaker's face as he or she talks can greatly help in understanding what the speaker is saying. This is because the speaker's facial movements relay information about what the speaker is saying, but also, importantly, when the speaker is saying it. Studying how the brain uses this timing relationship to combine information from continuous auditory and visual speech has traditionally been methodologically difficult. Here we introduce a new approach for doing this using relatively inexpensive and noninvasive scalp recordings. Specifically, we show that the brain's representation of auditory speech is enhanced when the accompanying visual speech signal shares the same timing. Furthermore, we show that this enhancement is most pronounced at a time scale that corresponds to mean syllable length.},
author = {Crosse, M. J. and Butler, J. S. and Lalor, E. C.},
doi = {10.1523/JNEUROSCI.1829-15.2015},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Crosse, Butler, Lalor - 2015 - Congruent Visual Speech Enhances Cortical Entrainment to Continuous Auditory Speech in Noise-Free Conditi.pdf:pdf},
isbn = {0270-6474},
issn = {0270-6474},
journal = {Journal of Neuroscience},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch4,obama paper},
number = {42},
pages = {14195--14204},
pmid = {26490860},
title = {{Congruent Visual Speech Enhances Cortical Entrainment to Continuous Auditory Speech in Noise-Free Conditions}},
url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.1829-15.2015},
volume = {35},
year = {2015}
}
@article{Doelling2014,
abstract = {A growing body of research suggests that intrinsic neuronal slow ({\textless}. 10. Hz) oscillations in auditory cortex appear to track incoming speech and other spectro-temporally complex auditory signals. Within this framework, several recent studies have identified critical-band temporal envelopes as the specific acoustic feature being reflected by the phase of these oscillations. However, how this alignment between speech acoustics and neural oscillations might underpin intelligibility is unclear. Here we test the hypothesis that the 'sharpness' of temporal fluctuations in the critical band envelope acts as a temporal cue to speech syllabic rate, driving delta-theta rhythms to track the stimulus and facilitate intelligibility. We interpret our findings as evidence that sharp events in the stimulus cause cortical rhythms to re-align and parse the stimulus into syllable-sized chunks for further decoding. Using magnetoencephalographic recordings, we show that by removing temporal fluctuations that occur at the syllabic rate, envelope-tracking activity is reduced. By artificially reinstating these temporal fluctuations, envelope-tracking activity is regained. These changes in tracking correlate with intelligibility of the stimulus. Together, the results suggest that the sharpness of fluctuations in the stimulus, as reflected in the cochlear output, drive oscillatory activity to track and entrain to the stimulus, at its syllabic rate. This process likely facilitates parsing of the stimulus into meaningful chunks appropriate for subsequent decoding, enhancing perception and intelligibility. {\textcopyright} 2013 Elsevier Inc.},
author = {Doelling, Keith B. and Arnal, Luc H. and Ghitza, Oded and Poeppel, David},
doi = {10.1016/j.neuroimage.2013.06.035},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doelling et al. - 2014 - Acoustic landmarks drive delta-theta oscillations to enable speech comprehension by facilitating perceptual par.pdf:pdf},
issn = {10959572},
journal = {NeuroImage},
keywords = {Acoustic edge,Auditory cortex,MEG,Neural oscillation,Perceptual parsing,Speech},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch3,obama paper},
pages = {761--768},
pmid = {23791839},
title = {{Acoustic landmarks drive delta-theta oscillations to enable speech comprehension by facilitating perceptual parsing}},
volume = {85},
year = {2014}
}
@article{OSullivan2013,
abstract = {Traditionally, the use of electroencephalography (EEG) to study the neural processing of natural stimuli in humans has been hampered by the need to repeatedly present discrete stimuli. Progress has been made recently by the realization that cortical population activity tracks the amplitude envelope of speech stimuli. This has led to studies using linear regression methods which allow the presentation of continuous speech. One such method, known as stimulus reconstruction, has so far only been utilized in multi-electrode cortical surface recordings and magnetoencephalography (MEG). Here, in two studies, we show that such an approach is also possible with EEG, despite the poorer signal-to-noise ratio of the data. In the first study, we show that it is possible to decode attention in a naturalistic cocktail party scenario on a single trial (≈60 s) basis. In the second, we show that the representation of the envelope of auditory speech in the cortex is more robust when accompanied by visual speech. The sensitivity of this inexpensive, widely-accessible technology for the online monitoring of natural stimuli has implications for the design of future studies of the cocktail party problem and for the implementation of EEG-based brain-computer interfaces.},
author = {O'Sullivan, James A. and Crosse, Michael J. and Power, Alan J. and Lalor, Edmund C.},
doi = {10.1109/EMBC.2013.6610122},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Sullivan et al. - 2013 - The effects of attention and visual input on the representation of natural speech in EEG.pdf:pdf},
isbn = {9781457702167},
issn = {1557170X},
journal = {Conference proceedings : ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual Conference},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch4},
pages = {2800--2803},
pmid = {24110309},
title = {{The effects of attention and visual input on the representation of natural speech in EEG}},
volume = {2013},
year = {2013}
}
@article{OSullivan2015,
abstract = {How humans solve the cocktail party problem remains unknown. However, progress has been made recently thanks to the realization that cortical activity tracks the amplitude envelope of speech. This has led to the development of regression methods for studying the neurophysiology of continuous speech. One such method, known as stimulus-reconstruction, has been successfully utilized with cortical surface recordings and magnetoencephalography (MEG). However, the former is invasive and gives a relatively restricted view of processing along the auditory hierarchy, whereas the latter is expensive and rare. Thus it would be extremely useful for research in many populations if stimulus-reconstruction was effective using electroencephalography (EEG), a widely available and inexpensive technology. Here we show that single-trial (≈60 s) unaveraged EEG data can be decoded to determine attentional selection in a naturalistic multispeaker environment. Furthermore, we show a significant correlation between our EEG-based measure of attention and performance on a high-level attention task. In addition, by attempting to decode attention at individual latencies, we identify neural processing at ∼200 ms as being critical for solving the cocktail party problem. These findings open up new avenues for studying the ongoing dynamics of cognition using EEG and for developing effective and natural brain-computer interfaces.},
author = {O'Sullivan, James A. and Power, Alan J. and Mesgarani, Nima and Rajaram, Siddharth and Foxe, John J. and Shinn-Cunningham, Barbara G. and Slaney, Malcolm and Shamma, Shihab A. and Lalor, Edmund C.},
doi = {10.1093/cercor/bht355},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Sullivan et al. - 2015 - Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG.pdf:pdf},
issn = {14602199},
journal = {Cerebral Cortex},
keywords = {BCI,EEG,attention,cocktail party,speech,stimulus-reconstruction},
mendeley-groups = {Alice+Pieman,First Exam Citations,First Exam Citations/Ch4,obama paper},
number = {7},
pages = {1697--1706},
pmid = {24429136},
title = {{Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG}},
volume = {25},
year = {2015}
}
@article{Lalor2010,
abstract = {The human auditory system has evolved to efficiently process individual streams of speech. However, obtaining temporally detailed responses to distinct continuous natural speech streams has hitherto been impracticable using standard neurophysiological techniques. Here a method is described which provides for the estimation of a temporally precise electrophysiological response to uninterrupted natural speech. We have termed this response AESPA (Auditory Evoked Spread Spectrum Analysis) and it represents an estimate of the impulse response of the auditory system. It is obtained by assuming that the recorded electrophysiological function represents a convolution of the amplitude envelope of a continuous speech stream with the to-be-estimated impulse response. We present examples of these responses using both scalp and intracranially recorded human EEG, which were obtained while subjects listened to a binaurally presented recording of a male speaker reading naturally from a classic work of fiction. This method expands the arsenal of stimulation types that can now be effectively used to derive auditory evoked responses and allows for the use of considerably more ecologically valid stimulation parameters. Some implications for future research efforts are presented.},
author = {Lalor, Edmund C. and Foxe, John J.},
doi = {10.1111/j.1460-9568.2009.07055.x},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lalor, Foxe - 2010 - Neural responses to uninterrupted natural speech can be extracted with precise temporal resolution.pdf:pdf},
isbn = {1460-9568 (Electronic)$\backslash$n0953-816X (Linking)},
issn = {0953816X},
journal = {European Journal of Neuroscience},
keywords = {Auditory evoked potential,EEG,Impulse response,Speech},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch4,obama paper},
number = {1},
pages = {189--193},
pmid = {20092565},
title = {{Neural responses to uninterrupted natural speech can be extracted with precise temporal resolution}},
volume = {31},
year = {2010}
}
@article{Mesgarani2012,
abstract = {Humans possess a remarkable ability to attend to a single speaker's voice in a multi-talker background. How the auditory system manages to extract intelligible speech under such acoustically complex and adverse listening conditions is not known, and, indeed, it is not clear how attended speech is internally represented. Here, using multi-electrode surface recordings from the cortex of subjects engaged in a listening task with two simultaneous speakers, we demonstrate that population responses in non-primary human auditory cortex encode critical features of attended speech: speech spectrograms reconstructed based on cortical responses to the mixture of speakers reveal the salient spectral and temporal features of the attended speaker, as if subjects were listening to that speaker alone. A simple classifier trained solely on examples of single speakers can decode both attended words and speaker identity. We find that task performance is well predicted by a rapid increase in attention-modulated neural selectivity across both single-electrode and population-level cortical responses. These findings demonstrate that the cortical representation of speech does not merely reflect the external acoustic environment, but instead gives rise to the perceptual aspects relevant for the listener's intended goal.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Mesgarani, Nima and Chang, Edward F.},
doi = {10.1038/nature11020},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mesgarani, Chang - 2012 - Selective cortical representation of attended speaker in multi-talker speech perception.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch3,obama paper},
number = {7397},
pages = {233--236},
pmid = {22522927},
title = {{Selective cortical representation of attended speaker in multi-talker speech perception}},
url = {http://www.nature.com/doifinder/10.1038/nature11020},
volume = {485},
year = {2012}
}
@article{Luo2010,
abstract = {Integrating information across sensory domains to construct a unified representation of multi-sensory signals is a fundamental characteristic of perception in ecological contexts. One provocative hypothesis deriving from neurophysiology suggests that there exists early and direct cross-modal phase modulation. We provide evidence, based on magnetoencephalography (MEG) recordings from participants viewing audiovisual movies, that low-frequency neuronal information lies at the basis of the synergistic coordination of information across auditory and visual streams. In particular, the phase of the 2-7 Hz delta and theta band responses carries robust (in single trials) and usable information (for parsing the temporal structure) about stimulus dynamics in both sensory modalities concurrently. These experiments are the first to show in humans that a particular cortical mechanism, delta-theta phase modulation across early sensory areas, plays an important "active" role in continuously tracking naturalistic audio-visual streams, carrying dynamic multi-sensory information, and reflecting cross-sensory interaction in real time.},
author = {Luo, Huan and Liu, Zuxiang and Poeppel, David},
doi = {10.1371/journal.pbio.1000445},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo, Liu, Poeppel - 2010 - Auditory cortex tracks both auditory and visual stimulus dynamics using low-frequency neuronal phase modulati.pdf:pdf},
isbn = {1545-7885 (Electronic)$\backslash$n1544-9173 (Linking)},
issn = {15449173},
journal = {PLoS Biology},
mendeley-groups = {Alice+Pieman,First Exam Citations,First Exam Citations/Ch4},
number = {8},
pages = {25--26},
pmid = {20711473},
title = {{Auditory cortex tracks both auditory and visual stimulus dynamics using low-frequency neuronal phase modulation}},
volume = {8},
year = {2010}
}
@article{Luo2007,
abstract = {How natural speech is represented in the auditory cortex constitutes a major challenge for cognitive neuroscience. Although many single-unit and neuroimaging studies have yielded valuable insights about the processing of speech and matched complex sounds, the mechanisms underlying the analysis of speech dynamics in human auditory cortex remain largely unknown. Here, we show that the phase pattern of theta band (4-8 Hz) responses recorded from human auditory cortex with magnetoencephalography (MEG) reliably tracks and discriminates spoken sentences and that this discrimination ability is correlated with speech intelligibility. The findings suggest that an ???200 ms temporal window (period of theta oscillation) segments the incoming speech signal, resetting and sliding to track speech dynamics. This hypothesized mechanism for cortical speech analysis is based on the stimulus-induced modulation of inherent cortical rhythms and provides further evidence implicating the syllable as a computational primitive for the representation of spoken language. ?? 2007 Elsevier Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Luo, Huan and Poeppel, David},
doi = {10.1016/j.neuron.2007.06.004},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo, Poeppel - 2007 - Phase Patterns of Neuronal Responses Reliably Discriminate Speech in Human Auditory Cortex.pdf:pdf},
isbn = {2122633255},
issn = {08966273},
journal = {Neuron},
keywords = {SYSNEURO},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch4,obama paper},
number = {6},
pages = {1001--1010},
pmid = {17582338},
title = {{Phase Patterns of Neuronal Responses Reliably Discriminate Speech in Human Auditory Cortex}},
volume = {54},
year = {2007}
}
@article{Howard2010,
abstract = {Speech stimuli give rise to neural activity in the listener that can be observed as waveforms using magnetoencephalography. Although waveforms vary greatly from trial to trial due to activity unrelated to the stimulus, it has been demonstrated that spoken sentences can be discriminated based on theta-band (3-7 Hz) phase patterns in single-trial response waveforms. Furthermore, manipulations of the speech signal envelope and fine structure that reduced intelligibility were found to produce correlated reductions in discrimination performance, suggesting a relationship between theta-band phase patterns and speech comprehension. This study investigates the nature of this relationship, hypothesizing that theta-band phase patterns primarily reflect cortical processing of low-frequency ({\textless}40 Hz) modulations present in the acoustic signal and required for intelligibility, rather than processing exclusively related to comprehension (e.g., lexical, syntactic, semantic). Using stimuli that are quite similar to normal spoken sentences in terms of low-frequency modulation characteristics but are unintelligible (i.e., their time-inverted counterparts), we find that discrimination performance based on theta-band phase patterns is equal for both types of stimuli. Consistent with earlier findings, we also observe that whereas theta-band phase patterns differ across stimuli, power patterns do not. We use a simulation model of the single-trial response to spoken sentence stimuli to demonstrate that phase-locked responses to low-frequency modulations of the acoustic signal can account not only for the phase but also for the power results. The simulation offers insight into the interpretation of the empirical results with respect to phase-resetting and power-enhancement models of the evoked response.},
author = {Howard, Mary F and Poeppel, David},
doi = {10.1152/jn.00251.2010},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Howard, Poeppel - 2010 - Discrimination of speech stimuli based on neuronal response phase patterns depends on acoustics but not compreh.pdf:pdf},
isbn = {1522-1598 (Electronic)$\backslash$r0022-3077 (Linking)},
issn = {0022-3077},
journal = {Journal of neurophysiology},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch3,obama paper},
pages = {2500--2511},
pmid = {20484530},
title = {{Discrimination of speech stimuli based on neuronal response phase patterns depends on acoustics but not comprehension.}},
volume = {104},
year = {2010}
}
@article{Horton2013,
abstract = {People are highly skilled at attending to one speaker in the presence of competitors, but the neural mechanisms supporting this remain unclear. Recent studies have argued that the auditory system enhances the gain of a speech stream relative to competitors by entraining (or "phase-locking") to the rhythmic structure in its acoustic envelope, thus ensuring that syllables arrive during periods of high neuronal excitability. We hypothesized that such a mechanism could also suppress a competing speech stream by ensuring that syllables arrive during periods of low neuronal excitability. To test this, we analyzed high-density EEG recorded from human adults while they attended to one of two competing, naturalistic speech streams. By calculating the cross-correlation between the EEG channels and the speech envelopes, we found evidence of entrainment to the attended speech's acoustic envelope as well as weaker yet significant entrainment to the unattended speech's envelope. An independent component analysis (ICA) decomposition of the data revealed sources in the posterior temporal cortices that displayed robust correlations to both the attended and unattended envelopes. Critically, in these components the signs of the correlations when attended were opposite those when unattended, consistent with the hypothesized entrainment-based suppressive mechanism.},
author = {Horton, C. and D'Zmura, M. and Srinivasan, R.},
doi = {10.1152/jn.01026.2012},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Horton, D'Zmura, Srinivasan - 2013 - Suppression of competing speech through entrainment of cortical oscillations.pdf:pdf},
isbn = {1522-1598 (Electronic)$\backslash$r0022-3077 (Linking)},
issn = {0022-3077},
journal = {Journal of Neurophysiology},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch4},
number = {12},
pages = {3082--3093},
pmid = {23515789},
title = {{Suppression of competing speech through entrainment of cortical oscillations}},
url = {http://jn.physiology.org/cgi/doi/10.1152/jn.01026.2012},
volume = {109},
year = {2013}
}
@article{Ding2014,
abstract = {Speech recognition is robust to background noise. One underlying neural mechanism is that the auditory system segregates speech from the listening background and encodes it reliably. Such robust internal representation has been demonstrated in auditory cortex by neural activity entrained to the temporal envelope of speech. A paradox, however, then arises, as the spectro-temporal fine structure rather than the temporal envelope is known to be the major cue to segregate target speech from background noise. Does the reliable cortical entrainment in fact reflect a robust internal "synthesis" of the attended speech stream rather than direct tracking of the acoustic envelope? Here, we test this hypothesis by degrading the spectro-temporal fine structure while preserving the temporal envelope using vocoders. Magnetoencephalography (MEG) recordings reveal that cortical entrainment to vocoded speech is severely degraded by background noise, in contrast to the robust entrainment to natural speech. Furthermore, cortical entrainment in the delta-band (1-4. Hz) predicts the speech recognition score at the level of individual listeners. These results demonstrate that reliable cortical entrainment to speech relies on the spectro-temporal fine structure, and suggest that cortical entrainment to the speech envelope is not merely a representation of the speech envelope but a coherent representation of multiscale spectro-temporal features that are synchronized to the syllabic and phrasal rhythms of speech. ?? 2013 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Ding, Nai and Chatterjee, Monita and Simon, Jonathan Z.},
doi = {10.1016/j.neuroimage.2013.10.054},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding, Chatterjee, Simon - 2014 - Robust cortical entrainment to the speech envelope relies on the spectro-temporal fine structure.pdf:pdf},
issn = {10538119},
journal = {NeuroImage},
keywords = {Auditory cortex,Auditory scene analysis,Envelope entrainment,MEG},
mendeley-groups = {Alice+Pieman,First Exam Citations,First Exam Citations/Ch4,obama paper},
pages = {41--46},
pmid = {24188816},
title = {{Robust cortical entrainment to the speech envelope relies on the spectro-temporal fine structure}},
volume = {88},
year = {2014}
}
@article{ZionGolumbic2013,
abstract = {The ability to focus on and understand one talker in a noisy social environment is a critical social-cognitive capacity, whose underlying neuronal mechanisms are unclear. We investigated the manner in which speech streams are represented in brain activity and the way that selective attention governs the brain's representation of speech using a "Cocktail Party"paradigm, coupled with direct recordings from the cortical surface in surgical epilepsy patients. We find that brain activity dynamically tracks speech streams using both low-frequency phase and high-frequency amplitude fluctuations and that optimal encoding likely combines the two. In and near low-level auditory cortices, attention "modulates"the representation by enhancing cortical tracking of attended speech streams, but ignored speech remains represented. In higher-order regions, the representation appears to become more "selective,"in that there is no detectable tracking of ignored speech. This selectivity itself seems to sharpen as a sentence unfolds. {\textcopyright} 2013 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {{Zion Golumbic}, Elana M. and Ding, Nai and Bickel, Stephan and Lakatos, Peter and Schevon, Catherine A. and McKhann, Guy M. and Goodman, Robert R. and Emerson, Ronald and Mehta, Ashesh D. and Simon, Jonathan Z. and Poeppel, David and Schroeder, Charles E.},
doi = {10.1016/j.neuron.2012.12.037},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zion Golumbic et al. - 2013 - Mechanisms underlying selective neuronal tracking of attended speech at a cocktail party.pdf:pdf},
issn = {08966273},
journal = {Neuron},
mendeley-groups = {Alice+Pieman,First Exam Citations,First Exam Citations/Ch4,obama paper},
number = {5},
pages = {980--991},
pmid = {23473326},
title = {{Mechanisms underlying selective neuronal tracking of attended speech at a "cocktail party"}},
volume = {77},
year = {2013}
}
@article{Ding2013,
abstract = {Natural sensory inputs, such as speech and music, are often rhythmic. Recent studies have consistently demonstrated that these rhythmic stimuli cause the phase of oscillatory, i.e. rhythmic, neural activity, recorded as local field potential (LFP), electroencephalography (EEG) or magnetoencephalography (MEG), to synchronize with the stimulus. This phase synchronization, when not accompanied by any increase of response power, has been hypothesized to be the result of phase resetting of ongoing, spontaneous, neural oscillations measurable by LFP, EEG, or MEG. In this article, however, we argue that this same phenomenon can be easily explained without any phase resetting, and where the stimulus-synchronized activity is generated independently of background neural oscillations. It is demonstrated with a simple (but general) stochastic model that, purely due to statistical properties, phase synchronization, as measured by 'inter-trial phase coherence', is much more sensitive to stimulus-synchronized neural activity than is power. These results question the usefulness of analyzing the power and phase of stimulus-synchronized activity as separate and complementary measures; particularly in the case of attempting to demonstrate whether stimulus-synchronized neural activity is generated by phase resetting of ongoing neural oscillations.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Ding, Nai and Simon, Jonathan Z.},
doi = {10.1007/s10827-012-0424-6},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding, Simon - 2013 - Power and phase properties of oscillatory neural responses in the presence of background activity.pdf:pdf},
issn = {09295313},
journal = {Journal of Computational Neuroscience},
keywords = {Entrainment,Neural oscillations,Phase coherence,Phase resetting},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch4},
number = {2},
pages = {337--343},
pmid = {23007172},
title = {{Power and phase properties of oscillatory neural responses in the presence of background activity}},
volume = {34},
year = {2013}
}
@article{Ding2012,
abstract = {The cortical representation of the acoustic features of continuous speech is the foundation of speech perception. In this study, noninvasive mag- netoencephalography (MEG) recordings are obtained from human subjects actively listening to spoken narratives, in both simple and cocktail party-like auditory scenes. By modeling how acoustic fea- tures of speech are encoded in ongoing MEG activity as a spectro- temporal response function, we demonstrate that the slow temporal modulations of speech in a broad spectral region are represented bilaterally in auditory cortex by a phase-locked temporal code. For speech presented monaurally to either ear, this phase-locked response is always more faithful in the right hemisphere, but with a shorter latency in the hemisphere contralateral to the stimulated ear. When different spoken narratives are presented to each ear simultaneously (dichotic listening), the resulting cortical neural activity precisely encodes the acoustic features of both of the spoken narratives, but slightly weakened and delayed compared with the monaural response. Critically, the early sensory response to the attended speech is con- siderably stronger than that to the unattended speech, demonstrating top-down attentional gain control. This attentional gain is substantial even during the subjects' very first exposure to the speech mixture and therefore largely independent of knowledge of the speech content. Together, these findings characterize how the spectrotemporal fea- tures of speech are encoded in human auditory cortex and establish a single-trial-based paradigm to study the neural basis underlying the cocktail party phenomenon.},
author = {Ding, N. and Simon, J. Z.},
doi = {10.1152/jn.00297.2011},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding, Simon - 2012 - Neural coding of continuous speech in auditory cortex during monaural and dichotic listening.pdf:pdf},
issn = {0022-3077},
journal = {Journal of Neurophysiology},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch4,obama paper},
number = {1},
pages = {78--89},
pmid = {21975452},
title = {{Neural coding of continuous speech in auditory cortex during monaural and dichotic listening}},
url = {http://jn.physiology.org/cgi/doi/10.1152/jn.00297.2011},
volume = {107},
year = {2012}
}
@article{DiLiberto2015,
abstract = {The human ability to understand speech is underpinned by a hierarchical auditory system whose successive stages process increasingly complex attributes of the acoustic input. It has been suggested that to produce categorical speech perception, this system must elicit consistent neural responses to speech tokens (e.g., phonemes) despite variations in their acoustics. Here, using electroencephalography (EEG), we provide evidence for this categorical phoneme-level speech processing by showing that the relationship between continuous speech and neural activity is best described when that speech is represented using both low-level spectrotemporal information and categorical labeling of phonetic features. Furthermore, the mapping between phonemes and EEG becomes more discriminative for phonetic features at longer latencies, in line with what one might expect from a hierarchical system. Importantly, these effects are not seen for time-reversed speech. These findings may form the basis for future research on natural language processing in specific cohorts of interest and for broader insights into how brains transform acoustic input into meaning.},
author = {{Di Liberto}, Giovanni M. and O'Sullivan, James A. and Lalor, Edmund C.},
doi = {10.1016/j.cub.2015.08.030},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Di Liberto, O'Sullivan, Lalor - 2015 - Low-frequency cortical entrainment to speech reflects phoneme-level processing.pdf:pdf},
isbn = {1879-0445 (Electronic)$\backslash$r0960-9822 (Linking)},
issn = {09609822},
journal = {Current Biology},
mendeley-groups = {First Exam Citations,First Exam Citations/Ch3,First Exam Citations/to read},
number = {19},
pages = {2457--2465},
pmid = {26412129},
publisher = {Elsevier Ltd},
title = {{Low-frequency cortical entrainment to speech reflects phoneme-level processing}},
url = {http://dx.doi.org/10.1016/j.cub.2015.08.030},
volume = {25},
year = {2015}
}
@article{Ding2014a,
abstract = {Auditory cortical activity is entrained to the temporal envelope of speech, which corresponds to the syllabic rhythm of speech. Such entrained cortical activity can be measured from subjects naturally listening to sentences or spoken passages, providing a reliable neural marker of online speech processing. A central question still remains to be answered about whether cortical entrained activity is more closely related to speech perception or non-speech-specific auditory encoding. Here, we review a few hypotheses about the functional roles of cortical entrainment to speech, e.g., encoding acoustic features, parsing syllabic boundaries, and selecting sensory information in complex listening environments. It is likely that speech entrainment is not a homogeneous response and these hypotheses apply separately for speech entrainment generated from different neural sources. The relationship between entrained activity and speech intelligibility is also discussed. A tentative conclusion is that theta-band entrainment (4-8 Hz) encodes speech features critical for intelligibility while delta-band entrainment (1-4 Hz) is related to the perceived, non-speech-specific acoustic rhythm. To further understand the functional properties of speech entrainment, a splitter's approach will be needed to investigate (1) not just the temporal envelope but what specific acoustic features are encoded and (2) not just speech intelligibility but what specific psycholinguistic processes are encoded by entrained cortical activity. Similarly, the anatomical and spectro-temporal details of entrained activity need to be taken into account when investigating its functional properties.},
author = {Ding, Nai and Simon, Jonathan Z.},
doi = {10.3389/fnhum.2014.00311},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding, Simon - 2014 - Cortical entrainment to continuous speech functional roles and interpretations.pdf:pdf},
issn = {1662-5161},
journal = {Frontiers in Human Neuroscience},
mendeley-groups = {Alice+Pieman,First Exam Citations,First Exam Citations/Ch4,obama paper},
pmid = {24904354},
title = {{Cortical entrainment to continuous speech: functional roles and interpretations}},
url = {http://journal.frontiersin.org/article/10.3389/fnhum.2014.00311/abstract},
volume = {8},
year = {2014}
}
@article{Huth2016,
abstract = {Previous neuroimaging studies have identified a group of regions that seem to represent information about the meaning of language. These regions, collectively known as the semantic system, respond more to words than non-words 1 , more to semantic tasks than phonological tasks 1 , and more to natural speech than temporally scrambled speech 2 . Studies that have investigated specific types of representation in the semantic system have found areas selective for concrete or abstract words 3–5 , action verbs 6 , social narratives 7 or other semantic features. Others have found areas selective for specific semantic domains— groups of related concepts such as living things, tools, food or shelter 8–13 . However, all previous studies tested only a handful of stimulus conditions, so no study has yet produced a comprehensive survey of how semantic information is represented across the entire semantic system. We addressed this problem by using a data-driven approach 14 to model brain responses elicited by naturally spoken narrative stories that contain many different semantic domains 15 . Seven subjects listened to more than two hours of stories from The Moth Radio Hour 2 while whole-brain blood-oxygen-level-dependent (BOLD) responses were recorded by fMRI. We then used voxel-wise modelling, a highly effec-tive approach for modelling responses to complex natural stimuli 14–17 , to estimate the semantic selectivity of each voxel (Fig. 1a). Voxel-wise model estimation and validation In voxel-wise modelling, features of interest are first extracted from the stimuli and then regression is used to determine how each feature modulates BOLD responses in each voxel. We used a word embedding space to identify semantic features of each word in the stories 12,15,18–20 . The embedding space was constructed by computing the normalized co-occurrence between each word and a set of 985 common English words (such as 'above' , 'worry' and 'mother') across a large corpus of English text. Words related to the same semantic domain tend to occur in similar contexts, and so have similar co-occurrence values. For example, the words 'month' and 'week' are very similar (the corre-lation between the two is 0.74), while the words 'month' and 'tall' are not (correlation − 0.22). Next we used regularized linear regression to estimate how the 985 semantic features influenced BOLD responses in every cortical voxel and in each individual subject (Fig. 1a). To account for responses caused by low-level properties of the stimulus such as word rate and phonemic content, additional regressors were included during voxel-wise model estimation and then discarded before further analysis. We also included additional regressors to account for physiological and emotional factors, but these had no effect on the estimated semantic models (Supplementary Data 3). One advantage of voxel-wise modelling over conventional neuro-imaging approaches is that the fit models can be validated by predicting BOLD responses to new natural stimuli that were not used during model estimation. This makes it possible to compute effect size by finding the fraction of response variance explained by the models. We tested how well the voxel-wise models predicted BOLD responses elicited by a new 10-min Moth story (Fig. 1b) that had not been used for model estimation. We found good prediction performance for voxels located throughout the semantic system, including in the lateral tem-poral cortex (LTC) and ventral temporal cortex (VTC), lateral parietal cortex (LPC) and medial parietal cortex (MPC), and medial prefrontal cortex, superior prefrontal cortex (SPFC) and inferior prefrontal cortex (IPFC) (Fig. 1c and Extended Data Fig. 1). This suggests that much of the semantic system is domain selective.},
author = {Huth, Alexander G and {De Heer}, Wendy A and Griffiths, Thomas L and Theunissen, Fr{\'{e}}d{\'{e}}ric E and Gallant, Jack L},
doi = {10.1038/nature17637},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huth et al. - 2016 - Natural speech reveals the semantic maps that tile human cerebral cortex Mapping semantic representation across cor.pdf:pdf},
journal = {Nature},
mendeley-groups = {First Exam Citations},
title = {{Natural speech reveals the semantic maps that tile human cerebral cortex Mapping semantic representation across cortex}},
volume = {532},
year = {2016}
}
@article{Ghitza2013a,
abstract = {A recent opinion article (Neural oscillations in speech: do not be enslaved by the envelope. Obleser et al., 2012) questions the validity of a class of speech perception models inspired by the possible role of neuronal oscillations in decoding speech (e.g., Ghitza, 2011; Giraud and Poeppel, 2012). The authors criticize, in particular, what they see as an over-emphasis of the role of temporal speech envelope information, and an over-emphasis of entrainment to the input rhythm while neglecting the role of top-down processes in modulating the entrainment of neuronal oscillations. Here we respond to these arguments, referring to the phenomenological model of Ghitza (2011), taken as a representative of the criticized approach. There is a remarkable correspondence between the time scales of phonemic, syllabic, and phrasal (psycho)-linguistic units, on the one hand, and the periods of the gamma, beta, theta, and delta oscillations, on the other. This correspondence has inspired recent hypotheses on the potential role of neuronal oscillations in speech perception (e.g., Poeppel, 2003; Ahissar and Ahissar, 2005; Ghitza and Greenberg, 2009; Ghitza, 2011; Giraud and Poeppel, 2012; Peelle and Davis, 2012). In particular, in an attempt to account for counterintuitive behavioral findings on the intelligi-bility of time-compressed speech as a function of " repackaging " rate (Ghitza and Greenberg, 2009), a cortical computation princi-ple was proposed according to which the speech decoding process is controlled by a time-varying, hierarchical window structure synchronized with the input (Ghitza, 2011). The window struc-ture was assumed to be realized by a neuronal mechanism, with cascaded oscillations at the core, capable of tracking the input pseudo-rhythm embedded in the critical-band envelopes of the auditory stream. In the model, the theta oscillator is the " mas-ter " and the other oscillators entrain to theta. The key property that enabled an explanation of the behavioral data is the capa-bility of the window structure to stay synchronized with the input; performance is high so long as the oscillators are phase-locked to the input rhythm (and within their intrinsic frequency range), and drops once the oscillators are out of their preferred temporal regime (e.g., exceed their boundaries). Giraud and Poeppel (2012) described a neurophysiological model which par-allels Ghitza's phenomenological model, and discussed new neu-roimaging evidence illustrating the operations and computations implicated in this oscillatory framework. In a recent opinion article, Obleser et al. (2012) criticize the proposed model. Addressing Giraud and Poeppel (2012) they write: " . . . while we enjoy the 'perspective' Giraud and Poeppel (2012) are offering, it seems to oversimplify the avail-able evidence . . . " in the following three respects: (1) lack of precision in defining the range of the neuronal oscillations and lack of specificity about the relationship between them (in par-ticular, the boundaries between delta and theta or theta and alpha), hence the overlook of important functional differentia-tions between these oscillations, (2) over-emphasis of the role of temporal speech-envelope information in speech perception, and (3) over-emphasis of entrainment to the input pseudo-rhythm while neglecting the role of top-down processes in modulating the entrainment of neuronal oscillations. It should be noted, at the outset, that we were aiming to offer a model for some critical computations in parsing and decod-ing speech, not a programmatic one-size-fits-all solution for all of speech comprehension. Nevertheless, Obleser et al. raise some important follow-up questions. For the sake of argument, items (1) and (3) can be grouped into one category, namely the poten-tial implication of the omission of alpha-theta and delta-theta interactions on the validity of the cortical computation principle at the core of our model. In the following we briefly address these arguments by referring to the phenomenological model proposed by Ghitza (2011).},
author = {Ghitza, Oded and Giraud, Anne-Lise and Poeppel, David},
doi = {10.3389/fnhum.2012.00340},
file = {:C$\backslash$:/Users/ivan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghitza et al. - 2012 - Neuronal oscillations and speech perception critical-band temporal envelopes are the essence.pdf:pdf},
issn = {1662-5161},
journal = {Frontiers in Human Neuroscience},
keywords = {cascaded neuronal oscillations,critical-band envelopes,hierarchical window structure,intelligibility,syllabic parsing},
mendeley-groups = {First Exam Citations/Ch4},
title = {{Neuronal oscillations and speech perception: critical-band temporal envelopes are the essence}},
url = {http://journal.frontiersin.org/article/10.3389/fnhum.2012.00340/abstract},
volume = {6},
year = {2013}
}
@misc{Purves2001,
abstract = {Whether judged in molecular, cellular, systemic, behavioral, or cognitive terms, the human nervous system is a stupendous piece of biological machinery. Given its accomplishments—all the artifacts of human culture, for instance—there is good reason for wanting to understand how the brain and the rest of the nervous system works. The debilitating and costly effects of neurological and psychiatric disease add a further sense of urgency to this quest. The aim of this book is to highlight the intellectual challenges and excitement—as well as the uncertainties—of what many see as the last great frontier of biological science. The information presented should serve as a starting point for undergraduates, medical students, graduate students in the neurosciences, and others who want to understand how the human nervous system operates. Like any other great challenge, neuroscience should be, and is, full of debate, dissension, and considerable fun. All these ingredients have gone into the construction of this book; we hope they will be conveyed in equal measure to readers at all levels.},
archivePrefix = {arXiv},
arxivId = {NBK10799},
author = {Purves, Dale and Augustine, George and Fitzpatrick, David and Katz, Lawrence and LaMantia, Anthony-Samuel and McNamara, James and Williams, Mark},
booktitle = {Sunderland (MA): Sinauer Associates; 2001.},
doi = {978-0878937257},
eprint = {NBK10799},
isbn = {0-87893-742-0},
issn = {1471003X},
mendeley-groups = {First Exam Citations},
pages = {109--12},
title = {{Neuroscience. 2nd edition}},
url = {https://www.ncbi.nlm.nih.gov/books/NBK10799/},
year = {2001}
}
@article{Polster1998,
abstract = {This article examines four disorders of auditory processing that can result from selective brain damage (cortical deafness, pure word deafness, auditory agnosia and phonagnosia) in an effort to derive a plausible functional and neuroanatomical model of audition. The article begins by identifying three possible reasons why models of auditory processing have been slower to emerge than models of visual processing: neuroanatomical differences between the visual and auditory systems, terminological confusions relating to auditory processing disorders, and technical factors that have made auditory stimuli more difficult to study than visual stimuli. The four auditory disorders are then reviewed and current theories of auditory processing considered. Taken together, these disorders suggest a modular architecture analogous to models of visual processing that have been derived from studying neurological patients. Ideas for future research to test modular theory more fully are presented.},
author = {Polster, Michael R. and Rose, Sally B.},
doi = {10.1016/S0010-9452(08)70736-6},
file = {:home/ivan/Downloads/ee74fbcb3d004cc36a15e3cc57a8309a7219.pdf:pdf},
issn = {00109452},
journal = {Cortex},
keywords = {Auditory processing,Modularity,Review},
mendeley-groups = {First Exam Citations},
number = {1},
pages = {47--65},
pmid = {9533993},
title = {{Disorders of auditory processing: Evidence for modularity in audition}},
volume = {34},
year = {1998}
}
@article{Cavinato2012,
abstract = {The phenomenon of blindsight has been largely studied and refers to residual abilities of blind patients without an acknowledged visual awareness. Similarly, "deaf hearing" might represent a further example of dissociation between detection and perception of sounds. Here we report the rare case of a patient with a persistent and complete cortical deafness caused by damage to the bilateral temporo-parietal lobes who occasionally showed unexpected reactions to environmental sounds despite she denied hearing. We applied for the first time electrophysiological techniques to better understand auditory processing and perceptual awareness of the patient. While auditory brainstem responses were within normal limits, no middle- and long-latency waveforms could be identified. However, event-related potentials showed conflicting results. While the Mismatch Negativity could not be evoked, robust P3-like waveforms were surprisingly found in the latency range of 600-700 ms. The generation of P3-like potentials, despite extensive destruction of the auditory cortex, might imply the integrity of independent circuits necessary to process auditory stimuli even in the absence of consciousness of sound. Our results support the reverse hierarchy theory that asserts that the higher levels of the hierarchy are immediately available for perception, while low-level information requires more specific conditions. The accurate characterization in terms of anatomy and neurophysiology of the auditory lesions might facilitate understanding of the neural substrates involved in deaf-hearing.},
author = {Cavinato, Marianna and Rigon, Jessica and Volpato, Chiara and Semenza, Carlo and Piccione, Francesco},
doi = {10.1371/journal.pone.0029909},
file = {:home/ivan/Downloads/journal.pone.0029909.PDF:PDF},
issn = {19326203},
journal = {PLoS ONE},
mendeley-groups = {First Exam Citations},
number = {1},
pages = {1--6},
pmid = {22272260},
title = {{Preservation of Auditory P300-Like Potentials in Cortical Deafness}},
volume = {7},
year = {2012}
}
@article{Dronkers2007,
abstract = {In 1861, the French surgeon, Pierre Paul Broca, described two patients who had lost the ability to speak after injury to the posterior inferior frontal gyrus of the brain. Since that time, an infinite number of clinical and functional imaging studies have relied on this brain-behaviour relationship as their anchor for the localization of speech functions. Clinical studies of Broca's aphasia often assume that the deficits in these patients are due entirely to dysfunction in Broca's area, thereby attributing all aspects of the disorder to this one brain region. Moreover, functional imaging studies often rely on activation in Broca's area as verification that tasks have successfully tapped speech centres. Despite these strong assumptions, the range of locations ascribed to Broca's area varies broadly across studies. In addition, recent findings with language-impaired patients have suggested that other regions also play a role in speech production, some of which are medial to the area originally described by Broca on the lateral surface of the brain. Given the historical significance of Broca's original patients and the increasing reliance on Broca's area as a major speech centre, we thought it important to re-inspect these brains to determine the precise location of their lesions as well as other possible areas of damage. Here we describe the results of high resolution magnetic resonance imaging of the preserved brains of Broca's two historic patients. We found that both patients' lesions extended significantly into medial regions of the brain, in addition to the surface lesions observed by Broca. Results also indicate inconsistencies between the area originally identified by Broca and what is now called Broca's area, a finding with significant ramifications for both lesion and functional neuroimaging studies of this well-known brain area.},
author = {Dronkers, N. F. and Plaisant, O. and Iba-Zizen, M. T. and Cabanis, E. A.},
doi = {10.1093/brain/awm042},
file = {:home/ivan/Downloads/awm042.pdf:pdf},
issn = {14602156},
journal = {Brain},
keywords = {Aphasia,Broca,History,Language,Magnetic resonance imaging},
mendeley-groups = {First Exam Citations},
number = {5},
pages = {1432--1441},
pmid = {17405763},
title = {{Paul Broca's historic cases: High resolution MR imaging of the brains of Leborgne and Lelong}},
volume = {130},
year = {2007}
}
@article{Rodd2005,
abstract = {A number of regions of the temporal and frontal lobes are known to be important for spoken language comprehension, yet we do not have a clear understanding of their functional role(s). In particular, there is considerable disagreement about which brain regions are involved in the semantic aspects of comprehension. Two functional magnetic resonance studies use the phenomenon of semantic ambiguity to identify regions within the fronto-temporal language network that subserve the semantic aspects of spoken language comprehension. Volunteers heard sentences containing ambiguous words (e.g. the shell was fired towards the tank') and well-matched low-ambiguity sentences (e.g. her secrets were written in her diary'). Although these sentences have similar acoustic, phonological, syntactic and prosodic properties (and were rated as being equally natural), the high-ambiguity sentences require additional processing by those brain regions involved in activating and selecting contextually appropriate word meanings. The ambiguity in these sentences goes largely unnoticed, and yet high-ambiguity sentences produced increased signal in left posterior inferior temporal cortex and inferior frontal gyri bilaterally. Given the ubiquity of semantic ambiguity, we conclude that these brain regions form an important part of the network that is involved in computing the meaning of spoken sentences.},
author = {Rodd, Jennifer M. and Davis, Matthew H. and Johnsrude, Ingrid S.},
doi = {10.1093/cercor/bhi009},
file = {:home/ivan/Downloads/bhi009.pdf:pdf},
issn = {10473211},
journal = {Cerebral Cortex},
keywords = {Frontal cortex,Semantics,Sentences,Spoken language,Temporal lobe},
mendeley-groups = {First Exam Citations},
number = {8},
pages = {1261--1269},
pmid = {15635062},
title = {{The neural mechanisms of speech comprehension: fMRI studies of semantic ambiguity}},
volume = {15},
year = {2005}
}
@article{Brookshire2017,
abstract = {Despite immense variability across languages, people can learn to understand any human language, spoken or signed. What neural mechanisms allow people to comprehend language across sensory modalities? When people listen to speech, electrophys-iological oscillations in auditory cortex entrain to slow ({\textless}8 Hz) fluctuations in the acoustic envelope. Entrainment to the speech envelope may reflect mechanisms specialized for auditory percep-tion. Alternatively, flexible entrainment may be a general-purpose cortical mechanism that optimizes sensitivity to rhythmic infor-mation regardless of modality. Here, we test these proposals by examining cortical coherence to visual information in sign lan-guage. First, we develop a metric to quantify visual change over time. We find quasiperiodic fluctuations in sign language, charac-terized by lower frequencies than fluctuations in speech. Next, we test for entrainment of neural oscillations to visual change in sign language, using electroencephalography (EEG) in fluent speakers of American Sign Language (ASL) as they watch videos in ASL. We find significant cortical entrainment to visual oscillations in sign language {\textless}5 Hz, peaking at ∼1 Hz. Coherence to sign is strongest over occipital and parietal cortex, in contrast to speech, where coherence is strongest over the auditory cortex. Nonsigners also show coherence to sign language, but entrainment at frontal sites is reduced relative to fluent signers. These results demonstrate that flexible cortical entrainment to language does not depend on neural processes that are specific to auditory speech perception. Low-frequency oscillatory entrainment may reflect a general cor-tical mechanism that maximizes sensitivity to informational peaks in time-varying signals. sign language | cortical entrainment | oscillations | EEG},
author = {Brookshire, Geoffrey and Lu, Jenny and Nusbaum, Howard C. and Goldin-Meadow, Susan and Casasanto, Daniel},
doi = {10.1073/pnas.1620350114},
file = {:home/ivan/Downloads/6352.full.pdf:pdf},
isbn = {1620350114},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
mendeley-groups = {First Exam Citations},
number = {24},
pages = {6352--6357},
pmid = {28559320},
title = {{Visual cortex entrains to sign language}},
url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1620350114},
volume = {114},
year = {2017}
}
@article{DiLiberto2018,
abstract = {Speech perception may be underpinned by a hierarchical cortical system, which attempts to match “external” incoming sensory inputs with “internal” top-down predictions. Prior knowledge modulates internal predictions of an upcoming stimulus and exerts its effects in temporal and inferior frontal cortex. Here, we used source-space magnetoencephalography (MEG) to study the spatiotemporal dynamics underpinning the integration of prior knowledge in the speech processing network. Prior knowledge was manipulated to i) increase the perceived intelligibility of speech sentences, and ii) dissociate the perceptual effects of changes in speech intelligibility from acoustical differences in speech stimuli. Cortical entrainment to the speech temporal envelope, which accounts for neural activity specifically related to sensory information, was affected by prior knowledge: This effect emerged early (∼50 ms) in left inferior frontal gyrus (IFG) and then (∼100 ms) in Heschl's gyrus (HG), and was sustained until latencies of ∼250 ms. Directed transfer function (DTF) measures were used for estimating direct Granger causal relations between locations of interest. In line with the cortical entrainment result, this analysis indicated that prior knowledge enhanced top-down connections from left IFG to all the left temporal areas of interest – namely HG, superior temporal sulcus (STS), and middle temporal gyrus (MTG). In addition, intelligible speech increased top-down information flow between left STS and left HG, and increased bottom-up flow in higher-order temporal cortex, specifically between STS and MTG. These results are compatible with theories that explain this mechanism as a result of both ascending and descending cortical interactions, such as predictive coding. Altogether, this study provides a detailed view of how, where and when prior knowledge influences continuous speech perception.},
author = {{Di Liberto}, Giovanni M. and Lalor, Edmund C. and Millman, Rebecca E.},
doi = {10.1016/j.neuroimage.2017.10.066},
file = {:home/ivan/Downloads/1-s2.0-S1053811917309023-main.pdf:pdf},
issn = {10959572},
journal = {NeuroImage},
keywords = {Beamforming,Entrainment,Magnetoencephalography,Speech intelligibility},
mendeley-groups = {First Exam Citations},
number = {November 2017},
pages = {247--258},
pmid = {29102808},
publisher = {Elsevier Ltd},
title = {{Causal cortical dynamics of a predictive enhancement of speech intelligibility}},
url = {https://doi.org/10.1016/j.neuroimage.2017.10.066},
volume = {166},
year = {2018}
}
@article{DiLiberto2017,
abstract = {Speech is central to human life. As such, any delay or impairment in receptive speech processing can have a profoundly negative impact on the social and professional life of a person. Thus, being able to assess the integrity of speech processing in different populations is an important goal. Current standardized assessment is mostly based on psychometric measures that do not capture the full extent of a person's speech processing abilities and that are difficult to administer in some subjects groups. A potential alternative to these tests would be to derive “direct”, objective measures of speech processing from cortical activity. One such approach was recently introduced and showed that it is possible to use electroencephalography (EEG) to index cortical processing at the level of phonemes from responses to continuous natural speech. However, a large amount of data was required for such analyses. This limits the usefulness of this approach for assessing speech processing in particular cohorts for whom data collection is difficult. Here, we used EEG data from 10 subjects to assess whether measures reflecting phoneme-level processing could be reliably obtained using only 10 min of recording time from each subject. This was done successfully using a generic modeling approach wherein the data from a training group composed of 9 subjects were combined to derive robust predictions of the EEG signal for new subjects. This allowed the derivation of indices of cortical activity at the level of phonemes and the disambiguation of responses to specific phonetic features (e.g., stop, plosive, and nasal consonants) with limited data. This objective approach has the potential to complement psychometric measures of speech processing in a wide variety of subjects.},
author = {{Di Liberto}, Giovanni M. and Lalor, Edmund C.},
doi = {10.1016/j.heares.2017.02.015},
file = {:home/ivan/Downloads/1-s2.0-S0378595516304701-main.pdf:pdf},
isbn = {0378-5955},
issn = {18785891},
journal = {Hearing Research},
keywords = {Categorical perception,Clinical research,Continuous speech,EEG,Language impairment,Neuromarker},
mendeley-groups = {First Exam Citations},
pages = {70--77},
pmid = {28246030},
publisher = {Elsevier B.V},
title = {{Indexing cortical entrainment to natural speech at the phonemic level: Methodological considerations for applied research}},
url = {http://dx.doi.org/10.1016/j.heares.2017.02.015},
volume = {348},
year = {2017}
}
@article{DeCheveigne2018,
abstract = {The relation between a stimulus and the evoked brain response can shed light on perceptual processes within the brain. Signals derived from this relation can also be harnessed to control external devices for Brain Computer Interface (BCI) applications. While the classic event-related potential (ERP) is appropriate for isolated stimuli, more sophisticated “decoding” strategies are needed to address continuous stimuli such as speech, music or environmental sounds. Here we describe an approach based on Canonical Correlation Analysis (CCA) that finds the optimal transform to apply to both the stimulus and the response to reveal correlations between the two. Compared to prior methods based on forward or backward models for stimulus-response mapping, CCA finds significantly higher correlation scores, thus providing increased sensitivity to relatively small effects, and supports classifier schemes that yield higher classification scores. CCA strips the brain response of variance unrelated to the stimulus, and the stimulus representation of variance that does not affect the response, and thus improves observations of the relation between stimulus and response.},
author = {de Cheveign{\'{e}}, Alain and Wong, Daniel E. and {Di Liberto}, Giovanni M. and Hjortkj{\ae}r, Jens and Slaney, Malcolm and Lalor, Edmund},
doi = {10.1016/j.neuroimage.2018.01.033},
file = {:home/ivan/Downloads/1-s2.0-S1053811918300338-main.pdf:pdf},
isbn = {3314432267},
issn = {10959572},
journal = {NeuroImage},
keywords = {CCA,Canonical correlation,EEG,ICA,LFP,MEG,Modulation filter,PCA,Reverse correlation,Speech,TRF},
mendeley-groups = {First Exam Citations},
number = {January},
pages = {206--216},
pmid = {29378317},
title = {{Decoding the auditory brain with canonical component analysis}},
volume = {172},
year = {2018}
}
@article{Zatorre1992,
author = {Zatorre, Robert J and Evans, Alan C and Meyer, Ernst and Gjedde, Albert},
journal = {Science},
mendeley-groups = {First Exam Citations},
number = {5058},
pages = {846--849},
pmid = {1589767},
title = {{Lateralization of phonetic and pitch discrimination in speech processing}},
volume = {256},
year = {1992}
}
@article{Ojanen2005,
abstract = {We investigated cerebral processing of audiovisual speech stimuli in humans using functional magnetic resonance imaging (fMRI). Ten healthy volunteers were scanned with a 'clustered volume acquisition' paradigm at 3 T during observation of phonetically matching (e.g., visual and acoustic /y/) and conflicting (e.g., visual /a/ and acoustic /y/) audiovisual vowels. Both stimuli activated the sensory-specific auditory and visual cortices, along with the superior temporal, inferior frontal (Broca's area), premotor, and visual-parietal regions bilaterally. Phonetically conflicting vowels, contrasted with matching ones, specifically increased activity in Broca's area. Activity during phonetically matching stimuli, contrasted with conflicting ones, was not enhanced in any brain region. We suggest that the increased activity in Broca's area reflects processing of conflicting visual and acoustic phonetic inputs in partly disparate neuron populations. On the other hand, matching acoustic and visual inputs would converge on the same neurons. {\textcopyright} 2004 Elsevier Inc. All rights reserved.},
author = {Ojanen, Ville and M{\"{o}}tt{\"{o}}nen, Riikka and Pekkola, Johanna and J{\"{a}}{\"{a}}skel{\"{a}}inen, Iiro P. and Joensuu, Raimo and Autti, Taina and Sams, Mikko},
doi = {10.1016/j.neuroimage.2004.12.001},
file = {:home/ivan/Downloads/1-s2.0-S1053811904007360-main.pdf:pdf},
issn = {10538119},
journal = {NeuroImage},
keywords = {Audiovisual vowel,Brain,Broca's area},
mendeley-groups = {First Exam Citations},
number = {2},
pages = {333--338},
pmid = {15784412},
title = {{Processing of audiovisual speech in Broca's area}},
volume = {25},
year = {2005}
}
@article{Watkins2006,
abstract = {Studies in both human and nonhuman primates indicate that motor and premotor cortical regions participate in auditory and visual perception of actions. Previous studies, using transcranial magnetic stimulation (TMS), showed that perceiving visual and auditory speech increased the excitability of the orofacial motor system during speech perception. Such studies, however, cannot tell us which brain regions mediate this effect. In this study, we used the technique of combining positron emission tomography with TMS to identify the brain regions that modulate the excitability of the motor system during speech perception. Our results show that during auditory speech perception, there is increased excitability of motor system underlying speech production and that this increase is significantly correlated with activity in the posterior part of the left inferior frontal gyrus (Broca's area). We propose that this area "primes" the motor system in response to heard speech even when no speech output is required and, as such, operates at the interface of perception and action.},
author = {Watkins, Kate and Paus, Tom{\'{a}}s},
doi = {10.1162/0898929041502616},
file = {:home/ivan/Downloads/watkins2004.pdf:pdf},
issn = {0898-929X},
journal = {Journal of cognitive neuroscience},
keywords = {Adult,Brain Mapping,Cerebrovascular Circulation,Cerebrovascular Circulation: physiology,Cerebrovascular Circulation: radiation effects,Electric Stimulation,Electric Stimulation: methods,Electromyography,Electromyography: methods,Evoked Potentials, Motor,Evoked Potentials, Motor: physiology,Evoked Potentials, Motor: radiation effects,Eye,Female,Frontal Lobe,Frontal Lobe: physiology,Humans,Lip,Magnetic Resonance Imaging,Magnetic Resonance Imaging: methods,Magnetics,Male,Motor Cortex,Motor Cortex: physiology,Motor Cortex: radiation effects,Photic Stimulation,Photic Stimulation: methods,Regional Blood Flow,Regional Blood Flow: physiology,Regional Blood Flow: radiation effects,Speech,Speech Perception,Speech Perception: physiology,Tomography, Emission-Computed,Tomography, Emission-Computed: methods,Verbal Behavior,Verbal Behavior: physiology},
mendeley-groups = {First Exam Citations},
number = {6},
pages = {978--87},
pmid = {15298785},
title = {{Modulation of motor excitability during speech perception: the role of Broca's area.}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/0898929041502616{\#}.Vuqll8ewF-c},
volume = {16},
year = {2006}
}
@article{Zurif1980,
author = {Zurif, Edgar B},
file = {:home/ivan/Downloads/10.230727849826.pdf:pdf},
journal = {American Scientist},
mendeley-groups = {First Exam Citations},
number = {3},
pages = {305--311},
title = {{Language Mechanisms: A Neuropsychological Perspective}},
volume = {68},
year = {1980}
}
@article{Hickok2013,
abstract = {Dual stream models of cortical organization have proven useful in understanding both language and visual-related systems and indeed have been a recurrent theme in neural models stretching back more than a century (Wernicke, 1874/1977). Thus, the general concept underlying the model - that the brain must interface sensory information with two different systems: conceptual and motor - not only is intuitively appealing but has a proven track record. In the language domain, the dual stream model provides an explanation of classical language disorders (Hickok and Poeppel, 2004; Hickok et al., 2011) and provides a framework for integrating and unifying research across psycholinguistic, neurolinguistic, and neurophysiological traditions. Recent work has shown that still further integration with motor control models is possible (Hickok et al., 2011). All of this suggests that the dual stream framework is on the right track as a model of language organization and provides a rich context for guiding future research. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
author = {Hickok, Gregory},
doi = {10.1016/B978-0-7020-5310-8.00003-X},
file = {:home/ivan/Downloads/1-s2.0-S1571064509000220-main.pdf:pdf},
isbn = {9780702053108},
issn = {15674231},
journal = {Handbook of Clinical Neurophysiology},
keywords = {aphasia,fmri,language,mirror neurons,production aphasia,sensory-motor integration,speech,speech perception,speech production,working memory},
mendeley-groups = {First Exam Citations},
number = {3},
pages = {61--70},
pmid = {20161054},
publisher = {Elsevier B.V.},
title = {{The functional neuroanatomy of language}},
url = {http://dx.doi.org/10.1016/j.plrev.2009.06.001},
volume = {10},
year = {2013}
}
@article{Leonard1998,
abstract = {Interpersonal communication via the auditory modality is fundamental to normal human development. One of the prominent anatomical specializations supporting this communication is the transverse gyrus of Heschl on the superior surface of the temporal lobe. This gyrus frequently appears duplicated, either by a sulcus indenting the crown of an initially single gyrus (common stem), or by a complete posterior duplication. The frequency of these duplications has been reported to be elevated in populations with learning disabilities and genetic anomalies. The significance of this observation is unclear, however, due to conflicting reports of the base rate of duplication and the location of relevant sulcal landmarks. In this study we report the variation in frequency and location of the sulcal boundaries of Heschl's gyrus in volumetric magnetic resonance imaging scans of 105 normal controls aged 5-65. The major results were as follows: (i) duplications were unstable--the frequency of duplication ranged from 20 to 60{\%} depending on distance from the midline; (ii) common stem duplications were more frequent than posterior duplications, particularly in the right hemisphere. Intra- and interindividual instability in sulcal landmarks pose serious obstacles to the attempt to map behavioral function onto the brain. Novel methods for dealing with structural variation are needed to facilitate the development of valid mapping techniques.},
author = {Leonard, Christiana M. and Puranik, Cynthia and Kuldau, John M. and Lombardino, Linda J.},
doi = {10.1093/cercor/8.5.397},
file = {:home/ivan/Desktop/reviewForExam/397.full.pdf:pdf},
isbn = {1047-3211 (Print)$\backslash$r1047-3211 (Linking)},
issn = {10473211},
journal = {Cerebral Cortex},
mendeley-groups = {First Exam Citations},
number = {5},
pages = {397--406},
pmid = {9722083},
title = {{Normal variation in the frequency and location of human auditory cortex landmarks. Heschl's gyrus: Where is it?}},
volume = {8},
year = {1998}
}
@article{Marie2015,
abstract = {This study describes the gyrification patterns and surface areas of Heschl's gyrus (HG) in 430 healthy volunteers mapped with magnetic resonance imaging. Among the 232 right-handers, we found a large occurrence of duplication (64 {\%}), especially on the right (49 vs. 37 {\%} on the left). Partial duplication was twice more frequent on the left than complete duplication. On the opposite, in the right hemisphere, complete duplication was 10 {\%} more frequent than partial duplication. The most frequent inter-hemispheric gyrification patterns were bilateral single HG (36 {\%}) and left single-right duplication (27 {\%}). The least common patterns were left duplication-right single (22 {\%}) and bilateral duplication (15 {\%}). Duplication was associated with decreased anterior HG surface area on the corresponding side, independently of the type of duplication, and increased total HG surface area (including the second gyrus). Inter-hemispheric gyrification patterns strongly influenced both anterior and total HG surface area asymmetries, leftward asymmetry of the anterior HG surface was observed in all patterns except double left HG, and total HG surface asymmetry favored the side of duplication. Compared to right-handers, the 198 left-handers exhibited lower occurrence of duplication, and larger right anterior HG surface and total HG surface areas. Left-handers' HG surface asymmetries were thus significantly different from those of right-handers, with a loss of leftward asymmetry of their anterior HG surface, and with significant rightward asymmetry of their total HG surface. In summary, gyrification patterns have a strong impact on HG surface and asymmetry. The observed reduced lateralization of HG duplications and anterior HG asymmetry in left-handers highlights HG inter-hemispheric gyrification patterns as a potential candidate marker of speech lateralization.},
author = {Marie, D. and Jobard, G. and Crivello, F. and Perchey, G. and Petit, L. and Mellet, E. and Joliot, M. and Zago, L. and Mazoyer, B. and Tzourio-Mazoyer, N.},
doi = {10.1007/s00429-013-0680-x},
file = {:home/ivan/Desktop/reviewForExam/429{\_}2013{\_}Article{\_}680.pdf:pdf},
isbn = {1863-2661 (Electronic)$\backslash$r1863-2653 (Linking)},
issn = {18632661},
journal = {Brain Structure and Function},
keywords = {Anatomy,Handedness,Hemispheric specialization,Heschl's gyrus,MRI,Speech},
mendeley-groups = {First Exam Citations},
number = {2},
pages = {729--743},
pmid = {24310352},
title = {{Descriptive anatomy of Heschl's gyri in 430 healthy volunteers, including 198 left-handers}},
volume = {220},
year = {2015}
}
