\documentclass[titlepage]{article}
\usepackage[utf8]{inputenc}
\usepackage{lineno, color, graphicx, verbatim, blindtext}
% \linenumbers
% \modulolinenumbers[2]
\usepackage[
backend=biber,
style=numeric
]{biblatex}
\addbibresource{firstExamRefs.bib}
% \addbibresource{secondaryRefs.bib}
\usepackage{setspace}
\usepackage{tipa}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
% \usepackage{gfsdidot}
\usepackage{venturis}
% \renewcommand{\familydefault}{\sfdefault}

\usepackage{graphicx}
\graphicspath{ {images/} }


\title{Oscillatory Mechanics in the Perception of Spoken Language}
\author{Ivan Iotzov}
\date{September 2018}

\begin{document}

% \fontfamily{cmss}\selectfont
% \fontfamily{lmr}\selectfont
% \fontfamily{phv}\selectfont

\maketitle

%\doublespacing

\section{Introduction} \label{intro}

  In recent years there has been great progress made in the domain
  of decoding human speech processing, and this review is intended
  to serve as an overview of some of that work, specifically in the 
  domain of how neural oscillations, both innate and exogenously 
  generated, factor into speech processing. In this introduction I will review
  some basic properties of speech signals as well as a brief overview of the 
  various endogenous oscillations that can be measured in the brain. The first 
  chapter will go into more detail on endogenous oscillations in the brain and 
  their theorized role in speech processing. The second chapter will present 
  differing theories that are more grounded in exogenous, stimulus-driven 
  processes than those in the first. Lastly, in the conclusion, I will attempt
  to integrate these perspectives and point to interesting avenues for future
  study.

  First, it is important to delineate the scope of this review. I 
  will be writing specifically about spoken language processing, 
  both in contexts of speech in noise and clean auditory stimuli.
  I will not be discussing speech production or the perception of 
  written language, but only auditory speech processing. Additionally, 
  I will confine myself to the `lower' levels of speech processing,
  i.e. the various statistics of the speech signals itself, to the 
  exclusion of `higher' level factors such as lexical and semantic 
  properties.

\section{Cortical Auditory Processing} \label{corticalAuditoryProcessing}

  In the above sections, the emphasis has been on the properties of the
  acoustic stimulus itself, its important features, and the mechanisms by
  which it is detected by the human auditory system. This chapter is dedicated
  to the transformation and integration of that acoustic information after it
  has been detected by the auditory system. Once an acoustic stimulus has
  been transformed into neural impulses by the machinery of the inner and outer
  ear, it proceeds through the brainstem and into the cortex. Roughly,
  information travels from the cochlea to the cochlear nuclei of the brainstem,
  the superior olive, the inferior colliculus, the medial geniculate nucleus of
  the thalamus, and finally to the primary auditory cortex
  \cite{Hickok2007,Webster1992}. This outline of the auditory pathway is
  a gross oversimplification but useful for this review as it shows that 
  there is some processing of incoming speech signals being done before the
  neural impulse reaches the brain. Brainstem and subcortical innervation
  sites are more numerous than described above, and there are also significant
  efferent connections from the higher auditory processing areas all the way
  down to the cochlear neurons themselves \cite{Kandel2000,Webster1992}. This
  overview is meant to provide a rough sketch to demonstrate the hierarchical
  and interconnected nature of the auditory processing pathways as well as to
  give a general view of how information proceeds from the ear to the cortex.
  Here, we will mainly concern ourselves with the processing of neural signals
  once they reach the primary auditory cortex and other related areas.


  This pathway shares some features of the visual processing pathway. It is
  hierarchical, but not strictly so, and it contains significant
  back-projections and divergent pathways \cite{Webster1992,Hickok2007}.
  Hickok and Poeppel, two prominent voices in the field, maintain that there
  are actually two parallel cortical pathways for acoustic information much
  like the parallel processing pathways found in the visual system
  \cite{Hickok2007,Hickok2004,Hickok2000}. The ventral auditory processing
  stream is thought to underlie the conversion from acoustic signals into
  lexical and semantic representations. The analogy could be drawn to the
  ventral visual stream, which is thought to mainly be responsible for visual
  object recognition. In a similar way, the ventral auditory stream underlies
  the ability to recognize acoustic `objects' and connect those to phonological
  or semantic meanings \cite{Parker2005,Rauschecker2009}. The dorsal stream of
  the auditory pathway has a less well-understood function, but it is thought
  to be involved in sensorimotor integration in much the same way as the dorsal
  visual stream is. In particular, Hickok and Poeppel theorize that this
  network is crucial in the development of speech as it integrates the sounds
  that are perceived and facilitates the motor learning task of learning to
  speak.

  A competing theory put forward by Angela Friederici \cite{Friederici2011} is
  that there are actually two ventral pathways and two parallel dorsal pathways.
  In this theory, there is one ventral pathway from roughly Brodmann's area 45
  to the temporal cortex and another from the frontal operculum (FOP) to the
  uncinate fascile (UF). These two ventral pathways are hypothesized to mainly
  be responsible for language processing and the processing of adjacent elements
  in an audio signal. The two dorsal pathways are thought to connect from the
  temporal cortex to the premotor cortex, as well as from the temporal cortex
  to Brodmann's area 44. The first pathway is thought to mainly support
  auditory motor functions similar to those proposed by Hickok and Poeppel,
  while the second is thought to be involved in more high-level language
  processing functions. Specifically, the second pathway is thought to provide
  a complement to the ventral streams in that it connects information that
  is not adjacent in the auditory stream and allows for grammatical analysis
  and connections.

  \subsection{Primary Auditory Cortex} \label{primaryAuditoryCortex}

    \begin{figure}
      \centering
      \includegraphics[scale=0.25]{primaryAuditoryAnatomy}
      \caption{This figure shows the gross anatomy of the left hemisphere
      of the human brain. The lobes, Brodmann's areas, and other areas
      of interest have been highlighted. \cite{Friederici2011}}
      \label{primaryAuditoryAnatomy}
    \end{figure}

    The primary auditory cortex (A1) is the main target of auditory information
    from the sensory neurons in the cochlea and is the backbone of auditory
    perception. It is located on the superior temporal gyrus
    (see Fig. \ref{primaryAuditoryAnatomy}) and can be divided into a primary
    area and various belt or peripheral areas \cite{Purves2001}. A1 is organized
    tonotopically, meaning that there is a separation by frequency of the
    incoming auditory signals, and that neighboring neurons respond to
    neighboring frequencies \cite{Lauter1985}. Organization of A1 is analogous
    to the topographical mapping that can be found in V1, and mirrors the
    tonotopic organization of the basilar membrane, discussed above.
    Therefore, A1 can be said to
    have a frequency `axis', groupings of neurons that react selectively to the
    frequency content of incoming auditory signals. Additionally, A1 contains an
    orthogonal `axis' that processes information related to the binaural aspects
    of incoming auditory signals and may serve localization or source
    identification purposes \cite{Purves2001}.


    Bilateral destruction of A1 results in cortical deafness, resulting in a
    total loss of hearing faculties in those affected. In order to manifest,
    this disorder requires total bilateral destruction of A1. Due to this, as
    well as the relatively high degree of redundancy in subcortical auditory
    structures, the disorder is quite rare \cite{Polster1998}. Additionally,
    subcortical processing of auditory signals may preserve some auditory
    capacities even in patients with this condition \cite{Cavinato2012}. More
    commonly, following damage to the cortical auditory areas are the conditions
    of auditory agnosia, pure word deafness, and phonoagnosia.

    Auditory agnosia
    is a condition in which patients are incapable of identifying sounds such
    as coughing or whistling, but show no evidence of impaired auditory speech
    comprehension. Pure word deafness is essentially the opposite of auditory
    agnosia, where patients are incapable of comprehending speech while
    maintaining their ability to speak, read, and identify sounds.
    Interestingly, some patients with pure word deafness retain their ability
    to extract information about a speaker (such as age, sex, etc.) based on
    their speech \cite{Polster1998}. Phonoagnosia is a condition analogous to
    prosopagnosia (inability to recognize familiar faces), where patients
    lack the ability to recognize familiar voices.


    These conditions provide insights into the different functions that are
    performed by the auditory processing system and demonstrate that these
    functions can be dissociated without disrupting the system completely. This
    observation points to the fact that the human auditory system is attempting
    to extract multiple types of information from an incoming auditory signal
    and that the methods for extracting this information involve separate
    cortical processing mechanisms.


\section{Entrainment} \label{entrainment}

  \subsection{Brainstem Entrainment} \label{brainstemEntrainment}

    The Auditory Brainstem Response (ABR) is a well-known neural response to
    auditory stimuli that was discovered more than 40 years ago
    \cite{Jewett1971,Jewett1970} and has seen widespread use in clinical
    settings for determining auditory thresholds or diagnosing neuropathologies
    \cite{Skoe2010} and is especially popular for hearing screening in infants.
    Measurement of this response is one of the most common clinical applications
    of auditory evoked potentials and the typical waveform is well
    characterized.

    The test typically consists of presenting a subject with a series of click
    stimuli while recording neural activity through surface electrodes placed on
    the scalp. The averaged activity is then examined for the characteristic
    response that develops over a window of \textasciitilde 10ms and consists
    of a series of waveform peaks labeled \textit{I-VII}
    \cite{Sininger1993,Bhattacharyya2017}. This activity is a stimulus-driven
    response that is generated in the brainstem and the auditory nerve and as
    such cannot be equated with the conscious perception of a stimulus, but
    does seem to be highly predictive of a subject's level of hearing loss
    \cite{Sininger1993}.

    More recently, there has been work involving the measurement of the ABR in
    response to complex stimuli, as opposed to the simple clicks or tones that
    were used previously. For example, Galbraith et al. (1995)
    \cite{Galbraith1995} found that intelligible speech could be recovered from
    the ABR that is recorded while the subject is presented with speech stimuli.
    Further studies have been conducted looking at a diverse range of stimuli
    including words, phrases, and music, and this work is ongoing. ABR still 
    remains a reliable indicator of healthy auditory processing precisely 
    because it is a more low level response that is easily disrupted by problems
    at the level of the ear or afferent auditory fibers.

    However, due to the low-level nature of the ABR, it is not a very useful 
    response for studying the higher-level auditory responses, such as speech 
    processing. The fact that it responds mainly to changes that occur at the 
    level of the ear limits its usefulness in studying cortical processing of 
    auditory input.

  \subsection{Cortical Entrainment} \label{corticalEntrainment}

    In addition to entrainment occurring in the brainstem, it is also possible
    to measure cortical entrainment in response to acoustic stimuli with methods
    such as electroencephalography and magnetoencephalography. This type of
    entrainment typically has the best correspondence to the temporal
    modulation of the speech signal
    \cite{Ding2014a,Ding2014,Nourski2009,Horton2014}, including both amplitude
    envelope modulation and frequency modulation. This cortical entrainment
    seems to be linked to a number of behavioral factors, such as attention
    \cite{Dmochowski2016,ZionGolumbic2013} as well as engagement with the
    stimulus \cite{Dmochowski2017}. This entrainment is clearly an exogenous
    stimulus-driven process, which is illustrated by the consistent responses
    elicited across subjects by the same audio or visual stimuli
    \cite{Cohen2017,Petroni2017}.

    Interestingly, speech remains intelligible despite a large amount of
    information in the speech signal being destroyed. Speech information is
    resistant to both degradation of temporal as well as spectral information
    \cite{Silipo1999,Drullman1994}. Barring total elimination of temporal and
    spectral information, it is only when both are sufficiently degraded that
    speech becomes unintelligible \cite{Elliott2009}. Related to this, the
    phenomenon of speaker gender identification is also reliant on similar
    information and can be modulated by changing the information present in the
    temporal and the spectral domain. Elliott and Theunissen \cite{Elliott2009}
    found that speaker gender identification rate can be significantly
    degraded by removing spectral modulations between 3 and 7 cycles/Hz and that
    this specifically reduced the subjects' ability to correctly identify
    female speakers. They postulated that this is because the female vocal
    spectrum has more power in this specific range and this is what
    contributes to `sounding like' a female speaker. Clearly, there is more
    information than just that necessary for comprehension that is embedded
    in the temporal and spectral modulations of speech and it is important to
    parse out what is necessary for the comprehension of speech and what
    information is supplemental, but not necessary to the bare comprehension
    of speech.

  \subsection{Innate Brain Rhythms} \label{innateBrainRhythms}

    There exist a number of neural rhythms that are innately present in the
    human brain. Roughly speaking, these are divided into $\alpha$ (8 - 12 Hz),
    $\beta$ (12 - 30 Hz), $\theta$ (4 - 8 Hz), $\delta$ (1 - 4 Hz), $\gamma$
    (30 - 80 Hz), and high $\gamma$ (80 - 150 Hz)
    \cite{Muresan2008,Rangaswamy2002}. These are roughly divided by the
    frequency of the oscillations and their suspected behavioral correlates.
    There is clearly overlap between these categorizations and it is difficult
    to make very clear distinctions between the various classes of neural
    oscillations. Despite this, there are some clear behavioral correlates in
    each of these frequency bands, and there have been attempts to localize the
    neural circuits responsible for their generation \cite{Michel1992}.

    Some of these rhythms are particularly relevant for the study of speech
    processing as they appear to have the same frequency as the typical speech
    rhythm and their power in EEG signals increases as a function of
    intelligibility. Particularly important to speech processing appear to be
    the $\delta$, $\gamma$, and $\theta$ rhythms \cite{Ghitza2009,Meyer2018}.
    In the case of the $\theta$ rhythm, Oded Ghitza has been a particularly
    strong advocate for the idea that the $\theta$ rhythm is crucial to parsing
    the speech signal into syllabic chunks which are the fundamental unit of
    speech processing. He refers to this fundamental unit as the
    `theta-syllable' \cite{Ghitza2013a} and claims that it is central to the
    theory of speech comprehension as a faculty facilitated by cortical
    entrainment to speech signals. In this view, the theta-syllable is defined
    as `a theta-cycle long speech segment located between two successive vocalic
    nuclei'. A vocalic nucleus is the critical section of a syllable (often a
    vowel) that can be preceded or followed by other marginal sounds (often
    consonants). For example, the word \textit{window} can be divided into the
    syllables \textipa{/"wIn/} and \textipa{/doU/}. In the \textipa{/"wIn/}
    syllable, the \textipa{/I/} sound functions as the nucleus while the
    \textipa{/w/} and \textipa{/n/} sounds are on the margins of that nucleus.

    In this interpretation, the theta rhythm is essentially the `master' rhythm
    that is responsible for the main chunking of the incoming speech stimulus
    and leads to the efficient parsing of syllables. This is crucial to
    understanding speech comprehension because of one of the most important
    mysteries in this area is how the brain chunks information into syllables
    that can then be parsed and transformed into higher-order representations.
    Without the ability to transform incoming acoustic information into smaller
    `chunks', the brain would have no ability to separate out syllabic
    information and parse what the speech information in a given chunk of time
    is. Instead, it would appear more as an undifferentiated mass.

  \subsection{Phase Entrainment}

    It has been demonstrated that cortical neurons entrain their phase to the
    oscillations present in speech stimuli, but the causal mechanism of this
    entrainment is still being debated in the field. Some claim that it is
    caused by entrainment to low-level features of speech sounds, basically a
    version of an auditory steady-state potential (such as those found in the
    brainstem discussed in section \ref{brainstemEntrainment}). Other
    researchers maintain that this phase entrainment is a product of more
    high-level speech sound features and reflects a processing of information
    carried in the speech signal by the brain.

    Zoefel and VanRullen \cite{Zoefel2016} address this question by presenting
    subjects with mixed speech/noise stimuli that retain the patterns of
    higher-level features, but do not have the fluctuations in spectral content
    and sound amplitude that some claim is the basis for the phase entrainment
    response. They found that neural phase entrainment occurs despite the
    missing low-level content, indicating that it is the higher-level features
    that drive this response. Additionally, they find that reversing their
    speech/noise mixture stimuli does not eliminate the phase entrainment
    response. This finding seems to indicate that the entrainment is in
    response to higher-level features, but only those that are acoustic in
    nature and not linguistic features, as those are absent in time-reversed
    speech.

    What, exactly, the functional role of this entrainment to the speech
    envelope is remains a topic of debate in the field, but there has recently
    been strong evidence to support the hypothesis that cortical entrainment to
    the speech envelope facilitates comprehension of the speech signal
    \cite{Ding2014a,Ding2012,OSullivan2015}.


    This hypothesis is supported by evidence correlating behavioral measures of
    comprehension of a speech segment to the level of neural envelope-following
    response that is found. These experiments, though, simply show that neural
    entrainment and speech comprehension co-occur. On their own, they fail to
    make the causal connection between neural entrainment and comprehension.

    These envelope-following responses found in response to acoustic stimuli
    differ from the

  \subsection{Envelope Following and Brain Rhythms}

    The fact that cortical neurons track the amplitude envelope of speech
    signals is now well-known and is considered an important part of the speech
    processing pathway. But, the interaction between natural cortical rhythms
    (discussed in Section \ref{innateBrainRhythms}) and the envelope following
    response is still not completely understood, and its functional role remains
     a subject of debate. There is abundant evidence that broad-spectrum EEG
     signals can be used to decode which speaker is being attended to by a
     subject in a cocktail party scenario \cite{Horton2014,DeTaillez2018}, but
     these studies only offer a partial picture of the role of cortical
     envelope-following responses in speech comprehension.

    For instance, Zion Golumbic et al. \cite{ZionGolumbic2013} were able to
    localize the envelope tracking response both in terms of cortical location
    as well as which frequency band was recruited for entrainment using
    electrocorticography (ECoG) in a cocktail party scenario with two speakers.
    They found found that there is a significant ability to predict which
    speaker is being attended in two different frequency bands that they term
    `low-frequency' (1-7 Hz) and `high gamma' (70-150 Hz). Further, they found
    that cortical locations in which these two frequency bands possessed
    significant predictive power differed. Both showed a very strong response
    in the superior temporal gyrus, which is to be expected given that is the
    location of the early cortical auditory areas, but the high gamma band
    showed a more widespread distribution around the brain, while the
    low-frequency band was more concentrated in frontal and temporal areas.

    These findings point to the conclusion that the envelope following response
    is not monolithic, and that there are multiple functional roles filled by
    the envelope following response.

    One of the most popular theories currently is that speech signals are
    essentially a quasi-rhythmic input which certain innate cortical rhythms are
    recruited to track. This tracking then enables both the exclusion of
    irrelevant speech signals \cite{Horton2014,OSullivan2015} as well as the
    syllabic parsing that is thought to be accomplished by the innate theta
    oscillations in the cortex \cite{Doelling2014,Ghitza2013b}.

    This brings up a large controversy in this field of study, which is the
    question of whether neural entrainment occurs because of the recruitment of
    natural neural rhythms that are simply used for alignment and parsing of the
    incoming speech signal, or whether these oscillations are actually
    generated by the speech signal itself and do not rely on oscillatory
    activity that is always ongoing in the cortex. This distinction is
    important because it colors how the increase in stimulus-aligned activity
    is interpreted. In the case of innate, ongoing oscillations being recruited
    to speech comprehension purposes, the activity that is elicited by
    presentation of a stimulus is used in `chunking' the incoming speech signal
    into segments that can be processed for lexical information and then
    combined to give some type of semantic meaning to a sentence. On the other
    hand, if the increase in stimulus-aligned activity is due to the
    emergence of a signal in the brain that tracks the amplitude envelope of
    the incoming speech signal, then it would seem that the analysis being
    performed in the brain is not of the `chunking' type, but involves
    synchronizing to the rate of the incoming speech signal and using
    information other than the relation of the incoming speech signal to
    innate oscillatory activity to process and comprehend speech.

  \subsection{Effects of Visual Information on Entrainment}

    Though this review is focused on the processing of auditory information, it
    is worth taking time to discuss the contributions of other modalities to
    the auditory comprehension process and how those other modalities might
    influence the neural signals that we can measure. Auditory information,
    especially in the case of speech, is rarely present without any visual
    input. When we are speaking with another individual, that person's facial
    expressions, lip movements, and body language all influence the our
    understanding of their speech. Especially in the case of speech that is
    difficult to hear or ambiguous, visual cues provide significant information
    that guides the comprehension of the auditory signal.

    For instance, it is well documented that, especially under noisy
    conditions, being able to see a speaker's face contributes significantly to
    the ability to understand that speaker \cite{Sumby1954,Erber1969}.

  \subsection{Comprehension Effects of Entrainment}

    In recent years, there has been a growing body of literature that supports
    the hypothesis that cortical entrainment to the temporal modulation of
    speech is the critical process that enables speech comprehension
    \cite{Meyer2018,Morillon2015,ZionGolumbic2013,Doelling2014}. This
    entrainment is found in both frequency (FM) and amplitude (AM) modulation,
    though the two are not independent \cite{Ding2009}. Ding \& Simon (2009)
    \cite{Ding2009} find that for a signal that is both frequency
    ($f_{FM}=40Hz$) and amplitude ($f_{AM}\leq 15Hz$) modulated, there is an
    auditory steady-state response at both $f_{FM}$ as well as at $f_{AM}$ but
    that the response at $f_{FM}$ is amplitude and frequency modulated with
    fundamental frequency $f_{AM}$. This points to the fact that when we speak
    about entrainment of signals in the brain there are multiple different
    `sites' of entrainment and that neural oscillations can be entrained to
    multiple features of a stimulus.


\newpage
% \nocite{*}
\printbibliography

\end{document}
